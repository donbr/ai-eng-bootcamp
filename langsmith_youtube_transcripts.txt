<videoId>4rupAXVraEA</videoId>
Hi Harrison from LangChain here and today I want to talk about a series of features that we're releasing as part of LangSmith. So if you haven't used LangSmith already it's our LLM SystemsOps platform for logging, monitoring, debugging, testing, evaluation of your LLM apps. And we're releasing a series of new features specifically around production monitoring and automations. So, LangSmith is really handy for getting a sense of what's going on in your application. And that doesn't just mean what's going on during development time or in offline mode. It also means what's going on when your application is live and in production. And so we've added a bunch of new features to help you understand what's going on and then take action based on that. So as a quick overview, I'm going to talk about the features here. And then we're going to have detailed videos on all of these features. And then also some more end to end use case videos on how to combine and use these features for very real situations. So as an overview of what we'll be covering, first we're going to talk about filtering. So as you get more and more logs in your production service, you need to be able to filter into subsets that you want to explore really easily. And we've added a really good filtering experience. We're then going to talk about some of the monitor in charge that we've added and all the things that you can do with them. So this provides a really great way to get an aggregate view of what is going on in your application. After that, we're going to talk about threads. So one of the main UXs for LLM applications is chat. And so being able to group different turns of the conversation together into a sensible view and have a kind of like a great bird's eye view of that thread or that conversation is really important. And so we've added a view specific, not at the trace level, but at the thread level. And finally, we'll dive into automations. And so automations are basically ways of taking filtered subsets of data automatically and doing things with it. So one thing that you can do with it is you can send it to a data set. Another thing you can do with it is you can send it to a data set. Another thing you can do with it is you can send it to an annotation queue and I'll explain what an annotation queue is. And then the third thing you can do with this and this is a really big new feature we've added is online evaluation. So with online evaluation you can define a prompt to run over some subset of traces and leave some feedback and automatically evaluate your runs that you see coming in to the production traffic. So I'm going to cover all these things and then I'll also cover a few use cases and so what you can do with these and really like real world problems that you can solve with this concept of production monitoring and automations. with this concept of production monitoring and automations. All of these features are part of LangSmith, which is our SaaS platform for doing logging, monitoring, testing, and evaluation. You can use LangSmith whether you are using LangChain or not, so it is completely independent. In order to sign up for an account, you can go to smith.langchain.com and you can sign up for an account, you can go to smith.langchain.com and you can sign up for an account for free. You will probably want to do that before we continue with the rest of this video. Once you've done that, come back and jump into a future guide. Let's get started!

<videoId>fzNSFuqtF_M</videoId>
In this video, we're going to talk about filtering. As you log more and more data to LangSmith from your application, it becomes really important to be able to filter that and dive into different subsets of the data that you want to look at. We've built out a lot of functionality around that, and with that you can do some pretty advanced stuff. And so that's what we're going to talk about in this video. There's also documentation for this, and I'll link to this in the description below. So in order to take advantage of filtering, you can go over to LightingSmith, you can go to a project of your choice, and for this one I'm going to go to ChatLangChain. And so ChatLangChain is an application that we expose that is chat over our documentation for LightingChain. So you can see up here that I have this filter button and if I click into it I can see that there's one filter applied by default and this is isRoot equals true. So what this allows is this allows me to see all top level traces. So remember if I click into a trace there's actually multiple runs that are going on that's happening inside this trace. So when I set isRoot equals to true, this only shows me the top level view of that. If I remove this filter, then I start to see all the individual runs show up below. In addition to controlling filters from up here, I can also choose a few filters from the sidebar here. So here we have a bunch of different filter shortcuts for commonly used filters. So filters on feedback, feedback source, different types of metadata, the name, the run type, the status, any tags, things like that. And so if you wanna go to a quick kind of like filter, this shortcut place is a good place to go. Inside this filter place, I'll add back this like isRead equals true to just look at the top level traces for a little bit. And so there's a bunch of things that I can filter on. I can do a full text search. So I can search for traces that mention memory. So when I apply this, I can now click in and one of in here somewhere is basically memory. So here we can see that memory is specified. So I can do a full text search over all the traces that I get. What else can I add? I can search based on input and output. I can search based on name. And so name is really useful when I want to filter sub runs. And I'll do that in a second. But first, I want to go through the is root equals true still. I can filter based on error or not. So let me filter it for where errors happen. And I can click in. And I can see what exactly went wrong in the error. And so here, it looks like it was canceled. So if I have a bunch of errors occurring, I can filter to runs that had errors and then basically dive into them and see what's going on. So this is a good way to monitor for what bad things are happening. Speaking of bad things happening, another thing that I can filter on is based on feedback. So here in ChatLinkchain, we have a thumbs up, thumbs down that's registered as user score. And so I can filter for where user score is zero. And these are all the data points where I got negative feedback for my application. So I can click into any one and I can see data points that an end user rated as bad. And so that's really helpful for diving into places where the feedback was negative. Other things I can do are I can search for latency. And so this is for like runs that might be longer than 10 seconds. I can go like that. I get a list of all the long runs. I can filter based on tag. I can filter based on metadata. So one of the things that we log in Chat Lang chain is the LLM that we use. And so here I can choose where and we attach this as a metadata tag. And so I can filter for metadata where the LLM is fireworks, for example. And all of these runs use fireworks under the hood. Another thing that I can do here is I can magically create a filter with AI. So I can specify something like runs longer than 10 seconds. And then this will convert it into a filter and add it up above. And this looks like correct, so I'm going to give it some thumbs up, some feedback. So if I don't want to kind of like go diving around in here for different and figure out like all the different things I need to choose manually, I can specify in natural language here and it'll run there. If I remove the is root equals true filter, another neat thing that I can do is basically search based on name. So if we go if we remember from the top level trace there's a bunch of sub runs and maybe I want to look at all the specific sub runs. So let's look at the specific sub runs for retrieve documents. So now I have all the retrieve document steps and if I click into here there's actually some nested things here right so this shows up as part of a larger trace. But basically I can do things like filter on sub components of a chain, and then I can apply the same exact filters as before. So if I want to look up where retrieve documents has a latency of more than one second, I can click into this here. And so here, you know, we can see that we, yeah, everything within here is longer than, and this is just this specific kind of like subset here. There's also a concept of advanced filters. And so let me maybe, let's keep these filters and let me click here. And two, three things pop up. Trace filters, tree filters, and a raw query. So trace filters apply to the root run of the trace. So remember, I removed the isRoot equals true, and so I have all the sub runs here. But one really common thing to do is basically look for sub runs where the parent run has some attribute. Specifically, in this case, what I wanna do is look for sub runs of retrieve documents that take longer than one second where the parent had a negative feedback, let's say. So I can click here into trace filters, and I have all the same options as before. And so I can go feedback, user score is zero. And now, if I look at a run, so I have, this is the retrieved documents where it took longer than a second because I still had that filter applied. And if I go up to the top level run and I look at feedback, it had a user score of zero. So basically what I can do is I can filter the sub runs based on the trace attributes. And again, this is really important when you're collecting things at the top level. And this could be feedback from the user. This could also be other metadata that you assign automatically at the top level, but you want to filter for sub runs. I can also do the opposite. So I can also go back to here. I can say isRoot equals true. And if you notice here, I can't add trace filter because when isRoot equals true, then any filter here is the same as a trace filter. But one thing I can do is a tree filter. And so this will basically filter these runs if any run that happens below matches some criteria. So I can add something like this and so let's do retrieve docs. Let's add another filter for latency one second. And so now here if I look at this for example I can see that this filters for where retrieve docs is greater than one second but it gives me all of the top level traces., I can see that this filters for where retrieve docs is greater than one second, but it gives me all of the top level traces. So I can also do something like feedback on the top level, user score is one, and this is very similar to the query that I did before but in the reverse order. So I'm filtering for top level traces where the feedback, where the user score is one, and where there's some sub run that where retrieve docs has greater than one latency. And so if I click into here, I can see that I have the user score equals one, and the retrieve docs is greater than one second. The last thing I want to show is just copying and pasting these filters. The last thing I want to show is just copying and pasting these filters. So here if I copy this filter, I get basically a string representation of this exact filter that I have here. What I can then do is I can paste this either into this raw query bar right here or into the SDK that we have and this will basically apply this filter. And so you can see that it's a query language, and equals is root true. I can also modify this here if I want, and then when I click it, it applies it. Note that it adds it as filters. So if you have previous filters, you'll need to basically remove them. If you want the filter that you post to be the only filter present. That's pretty much it for filtering. Filtering is really, really powerful to let you dive into subsets of the data based on attributes of the trace, attributes of the parent run, attributes of child runs, and we've added a lot of functionality around that. So I hope you enjoy this feature to be able to explore your data more handily.

<videoId>OXAkjTqLV4c</videoId>
In this video, I want to talk about monitoring. So it's great to look at traces individually, but oftentimes you want to look at aggregate statistics to get a better overall sense of what's happening inside your application. We've added a monitoring tab to enable exactly this. So I'm going to walk through this in LangSmith, but I'm also going to highlight this documentation that we have here, and I'll link to it in the description below. So in LangSmith, you can go into a project that you have. I'm going to go into Chat LangChain and these are runs that make up the chatbot over our documentation. I'm then going to click on this monitoring tab up here and I'll see that I get these monitors of various statistics over time and these are statistics that we find to be important when developing an LLM application. So here this is just a basic count of traces over time and of LLM calls over time. So here there's, you know, they correlate pretty heavily, but it's possible that I could, these could not be kind of like as closely correlated. I can also see the trace success rates over time. So I see that had a slight dip here, but other than that, it's pretty good. I can also see latency over time. I can see tokens per second. I can see feedback over time. And so I track user score and we also track vagueness as well. And so these are two different feedback measures that we can see over time. Tokens we track as well as cost. And so we've really emphasized this for LLM specific features, the above mentioned, as well as things like streaming. So how often streaming is used, time to first token, and things like that. So this is great for getting an overview of what's happening in your application. You can also zoom in and out. So I can zoom out to a 30 day time period and see this a little bit more over time. I can also zoom in more closely. And so here's what's happened in the last hour. Another thing that I can do is filter based on metadata or rather group based on metadata. I can do both. So when I click on metadata, if I click LLM, I can basically then have these results grouped over time. And so what's going on under the hood is that in Chat Lang chain, we actually rotate between five different LLM providers, Anthropic, Coher, Fireworks, Google, and OpenAI. And so I can track the various statistics within these groups. So this is really helpful for basically changing parameters of your application and then tracking it and seeing how it performs. So some interesting things that I can see are basically trace success rates. So here, when we had this dip before, I can actually see that it was Coheer that was erroring a little bit and it caused some of the errors. I can also track latency over time. So I can see right here that OpenAI and Fireworks seem to be the quickest ones to respond. I can also track feedback. So here it's a bit more mixed. We don't get a lot of feedback on Chat Link Chain unfortunately, but you can track feedback over time and you can basically see how different parameters are performing. And so this is really cool for tracking different aspects of your application. And here we show off tracking different LLMs, but it can be other features as well. You can have different prompts that you rotate through and as long as you attach them as feedback, you can filter and group based on them and get these monitoring charts. Another thing that you can do is filter into these subsets really easily. So here I can see that I had this dip in success, and so what I can do is this corresponds to an increase in error. So if I go here, I can see that we had this spike here from Coherent to have a pretty high error rate. And so if I click on this data point, what this is going to do is this is going to actually give me all individual traces that are in this data point on the screen. So here I can see that I have all the ones with failure. And so here, if we check into chat go here, I can see that I got a timeout error. And so it's really useful for diving in to different subsets of the data and really focusing in on what's going wrong there. That's pretty much it for monitoring. Hopefully this shows you how you can use the monitoring dashboard to really track the performance of your application over time, as well as group different runs and track those different subsets over time, which can allow for some really interesting comparisons in an online setting. And I think that's important because oftentimes you can do a bunch of tests and we have a great kind of like testing framework as well, but that's mostly offline and it often changes when you put things in production. People use it in ways that you don't expect and so being able to track that performance in an online manner is super important and so these are some of the features that we've added to enable exactly that.

<videoId>n8WHuupE_i0</videoId>
In this video, I want to talk about threads and tracking threads in LangSmith. So LangSmith captures traces and each trace can have a subset of runs. But these traces are oftentimes different kind of like calls and invocations. So one way to think about this is when you're having a conversation with a chatbot, each time the chatbot responds, that's one trace. There might be many different things happening inside it. It might be doing a retrieval step and then responding, but that's one trace. But when you have a conversation, you send a message as the chatbot sends a message. You send a message as the chatbot sends a message. And each of those messages are separate traces. But when debugging and trying to understand what's going on, it can be useful to tie those traces together. So you have a full view of the conversation that's happening. And so this is where threads come in. And basically what you can do is you can attach a thread ID to traces and then you can view and group traces by those thread IDs. So I'm going to walk through a quick example of doing exactly that. So we have an example here where we're not using Lang chain at all, this is just raw open AI and where we're tracking this thread ID. And so there's three different ways that you can specify a thread ID. It can either be called session ID, thread ID or conversation ID. And so here you'll see that we're putting session ID as an extra metadata argument. So if I copy this code snippet, I'm going to bring it over to my Jupyter notebook I have. I'm going to run this and let's take a closer look at what exactly is going on. So I'm using OpenAI. I'm importing this traceable decorator. This has to do with tracing. This is a really easy way to basically trace your runs. And then I'm defining the session ID. And so this is a unique identifier that I'm going to be passing through in the metadata. I have this function here, which is a nice little thing that basically just calls OpenAI. And then I have my conversation that I'm simulating. So I have the messages that I send in first. Hi, I'm Bob. I send this into the assistant. And I pass in this LangSmithExtra, this metadata key. And this is important because this metadata key isn't known ahead of time. And so if it was known ahead of time, we could have specified it as part of the traceable. Because it's not known ahead of time, because it's this unique session ID, and it's going to be different for each session, I'm going to pass it in with this LangSmithExtra. Then I'm going to get back the response. I'm adding it into my list of messages, and then I'm asking another one, what's my name? And then I'm calling the assistant again. And so if we go to LangSmith, these are getting logged to my default project. And so if I go to Langsmith, these are getting logged to my default project. And so if I go to threads, I can see that I have a new thread here, I can click into it. And here I have basically this very chatbot conversation-like view where I can see the conversation as it kind of unfolds. From here I can also open the individual traces so if I want to see the kind of like full trace here I can open it up and it's not that interesting because I just have a single call here but the point is that you can view the conversations together but then you can easily drill into individual traces if you want to and so it provides kind of like the best of both worlds. I'm going to show this again with lane chain this time. So here, let me copy this code snippet. I'm gonna go into the notebook. And we can see here that I am basically, I'm creating a really simple chain. That's just a prompt plus a model. I'm creating my list of messages. And then I'm creating this run config. And so this is just a dictionary with this metadata key. I'm specifying conversation ID this time. So you can see how I'm alternating the different keys that I could specify. And then when I call the chain, when I use.invoke, I'm passing in this. This is just my input. But then I'm passing in config equals config. And remember, I defined my config up here. And it's just this metadata with the conversation ID. I then add to my messages and then I call it again. And so let me run this. Now I can go back here. I can go back here. I can see that I have a new conversation thread here. And when I click on this, this is what happened when I called it with link chain. So hopefully this is a good example of how you can easily attach this thread ID to any traces that you have, and then you can view them in a very user-friendly way, again, whether you're using link chain or not. That's it for this video.

<videoId>ak2AIiX0P_A</videoId>
In this video, I'm going to talk about automations. So, so far, we've covered a bunch of functionality for how to allow you to dive in manually and pinpoint data points and view data points and debug things and all that is great. And looking at your data is so important and you absolutely should do that. But oftentimes, you maybe want to set up some automations to do things over the vast number of runs that you're getting in production automatically. And that's exactly what we've built. And so this is going to rely pretty heavily on filtering. So if you haven't watched the filtering section, I would highly recommend watching that first. Because basically the way that automations work, and here I'm going to switch over to Ling Smith, but the way that automations work are that you can go in to a project. You can apply some filter and then you can set up a rule to run over a random subset of those data points. So let's break that down. So here I'm in the Chat Lang chain project. This is looks over all runs in our chat link chain documentation bot. I can create a filter. So again, this is where the filtering section comes in handy. But let me create a really simple one that is something like all run, all top level runs where feedback, where user score is positive. And so these are all runs that got a thumbs up from the users. And so I can look at these manually. I can inspect them. That's great. But I can also add a rule. So I'm going to click on this and it's going to pop up a little sidebar. I can give a name to this rule. So let me give thumbs up as a name. I can also adjust the sampling rate. So by default, it's set to one, but I can drag it to be anything between zero and one. This is going to represent the percentage of the runs that meet this filter that I'm going to apply the action to. So let's talk about actions. So there's three actions that we have. Send to annotation queue, send data set, and then online evaluation. I'm going to deep dive into all three of these later on, but at a high level sending to annotation queue allows you to send data points to an annotation queue where humans can go in and review them in a really easy way. Sending to a dataset allows you to send runs to a dataset and you can use this dataset for testing and evaluation, you can use it for few-shot examples in production, you can use it to fine for testing and evaluation. You can use it for a few-shot examples in production. You can use it to fine-tune models. It's basically creating a collection of data points. And then online evaluation is going to be a very deep dive section later on. And what that is, is it's going to run an LLM over this subset of runs and then apply some feedback automatically to that based on the LLM scores. So this is how you can set it up from the project. You can also view and edit all these rules in your settings page. So if you go to settings and then if you go to rules, I can see that I've created a bunch of these rules already. I can edit them and delete them from here as well. So I can go in and edit, I can see what we've created, I can edit it if I want to, or I can just delete the rule. And so you can manage all these automations from this page. That's an overview of this automation feature. The real power comes in with what exactly you're doing with the data points. And so I'm going to talk about the annotation queue and online evaluation in deep dives in the next two videos, because I think those are really important to understand. And then I'm going to talk about a few use cases that you can do. So end to end kind of like framing of problems that you can do with these automations. Let's go to the next one.

<videoId>3Ws5wOS9eko</videoId>
One of the actions that we saw that we could set up automations to take is to send runs to an annotation queue. But what exactly is an annotation queue? That's what we're going to cover in this video. So annotation queues are a user-friendly way to quickly cycle through and annotate data. And so let's see exactly what that means. So if I go to LangSmith and I go to this little sidebar on the left, I can see that I have on the second item, it's this annotation queues tab. So I can click on that and I can see a list of all the annotation queues that I've created. If I click into one, I'm then presented with a kind of like user-friendly way to annotate these runs. So let's break down what I have going on here. Here I have the inputs and the outputs of the run. And so this right here is actually the annotation queue we have set up for the magic filter thing. So if you remember, if you go all the way back to the filtering video, we have a way to automatically kind of like create filters from natural language and that's what's going on here. So here I can see that I have inputs, I have outputs, and then there's a bunch of stuff I can do. So let me break that down. The first thing I can do is view the run. So if I click view run, I open up a sidebar and I get this very familiar looking debugger tracer mode where I can see everything that's going on. I can see what project it came from. And if I want to, I can navigate back to that project. Up in the top, I have a sense of how many runs I have left to annotate. And so we'll walk through going through the annotation queue, but this provides an overview of how many runs are actually present. Over here is where I can leave feedback. So I can leave feedback tags by clicking on a tag here. I can add a new one or choose from any of the ones that I've set up. I can also add a note. So here it looks like the user input was actually already a query, so maybe I'll say something like already a query and leave a note just for anyone in the future to kind of like come and look at. And all of these are saved with this run. So if I query this run programmatically or anything like that, they're all attached to it. So that's leaving feedback. And now let's talk about cycling through these data points. So there's a few actions that I can take for this data point. First up, I can add it to a data set. So there's a few actions that I can take for this data point. First up, I can add it to a data set. So if I click add to data set, I can then choose the data set that I want to add it to. I can also edit it before I do that. So these blocks here are editable. So if I add do something like runs with name text to playbook or something like that. I can then click add to data set and it will add the edited version. So this is really useful when you send and I'll talk about automation and some of the automations you can do but you can send runs with negative feedback here and then you can correct the output to something that it should be and then you can send that corrected output to a data set. Other things that I can do I can move this data point to the end and so that just moves it to the end of the queue so it will move it back here so if I click this you'll notice that the number of things in the queue doesn't change, but I go to the next data point. Alternatively, I could click done, and this will mark this run as done, and you'll see that this number will actually go down. I can also cycle through the runs here. So here it looks like basically the same thing was sent a bunch. This was probably because we were testing something out, but here I can get to some real runs. And so I can see that I can navigate with these buttons forward and backward. And so if I want to just jump through and see what's going on, this is a user-friendly way to do that. So that's an overview of annotation queues. And remember, these can be used in automations. So if I go back to my project and I set up an automation, I can click here in action send to annotation queue and I can select an annotation queue to send this subset of runs to or I can create a new queue. That's it for annotation queues. Thanks for watching.

<videoId>4NbV44E-hCU</videoId>
The last feature that I want to cover as part of this production monitoring and automation series is online evaluation. And this is a really cool and much requested feature that we've heard. And so I'm really excited to dive into this. The basic idea of online evaluation is applying prompt plus an LLM to assign feedback automatically to data points in production. And so I'm going to show how to do that. This is the documentation here, and I'll have a link for this in the description below. But to start, I'm going to jump back over to this familiar project that is ChatLangChain. And so these are all the runs we have coming in. And so one of the automations that we've set up is we want to tag all runs that have negative feedback with a vagueness tag. And so basically the reason for that is we want to look at all runs with negative feedback and basically determine whether it's got a negative feedback because the question was vague or because the response was wrong. And so we could look at all data points by hand but instead we're gonna have an LLM do that and assign this tag and that's going to give us a good indication. So the first part of that is setting up a filter and so this is covered in the filtering video as well as the automations bit, but basically I'm going to set up a filter for where feedback user score is zero. So I set up this filter. Now I'm going to add this automation. I'm going to add something like vagueness as the name. The sampling rate I'm going to set to one. So I want to run this over all data points and that's because Chat Lang chain has a manageable amount of data points with negative feedback. And then I'm going to select this online evaluation component. So when I select this, I get this little button here called Create Evaluator. And so I then open up this tab here and I can see a few things. First, I can see the secrets and API keys associated with this online evaluator. So remember, this is using a language model, and so we need to specify API keys for this language model. So if I click in here, I can see that I can specify my OpenAI key right here. I can then choose the model. So here it's using GPT 3.5 Turbo. I can change it to any of the provided ones if I want. I can also change the temperature, should I choose. The main interesting part comes in when you specify the prompt. So when I go here, I can click set in line prompt and I can get this template that pops up. And there's a few things to note here. First is the template. So the template has two input variables and it doesn't need to use these, but it should use only these two. Because basically what's happening is we're gonna fetch the data from the run, and we're going to pass it into this prompt template, format that into a message, and then we're gonna pass that to the language model. So here I can see that input and output are the two prompt variables, and those are exactly what I should be using. And these represent the inputs and outputs respectively of the run. And so if the input and outputs are nested, if they're a dictionary with multiple keys or something, that'll be rendered as a dictionary as well. So keep that in mind when you're designing this prompt. So here if I want, I can go in and I can change the prompt. And then the other really important thing that I can do is I can actually attach the schema to this. And so this is important to understand. The schema has a series of arguments. Each argument will end up being a metadata key that we attach to the run when it's finished. So here, I want to specify vagueness. So I'm going to change the name of this from correctness to vagueness. And this name, this name of this key, this is what will show up on my run as feedback. Then I'm also going to change the description. So like, is the user input vague or not. I can mark this as required or not. So if I wanna like optionally kind of like let the LLM leave feedback, I can unclick this and then I can choose the type. So Boolean, string, numbers, nulls, integers, objects, arrays, I can choose all of that. And this will be the type of the value that's left as feedback. I can also add other things. So if I want to do vagueness and correctness in the same thing, I definitely could. And these can be different types as well. And then when that's all finished, I can hit save and it will save this rule or save this online evaluator as a rule that gets run over this part of, over these sampled data points. I can see that I've set this up here. And so I've already done this. And I can see that I get the vagueness tags coming in. And so if I filter in to a particular subset of runs, so here I can see, so this is a run. If I look at feedback, I can see that it had a user score of zero, and I can see that it also had this vagueness component of one, and this is actually left by the online evaluator. I've also set up another evaluator that randomly samples data points and tags them according to different categories. So this was labeled as conceptual. So hopefully this shows a few interesting ways that you can use these online evaluators to automatically tag, look at, classify, provide more insights into the various inputs and outputs that I have coming into my system.

<videoId>WODgxh_wGTY</videoId>
In this video, I want to walk through a few of the common workflows that we see around for how to use automation best. And so they're on this use cases page in the documentation here. And these are quick, high level overviews of those workflows. So let's jump into the first one, which is basically just sending bad data points into an annotation queue. And so the idea here is that you generally wanna do this so that you as a human can look at things by eye. So the way that I would do that is I'd go into my project, I'll use the Chat Lang chain project here, and I'd set up a filter to capture all runs that have bad feedback. And so I'd add a filter based on feedback. Here I know that user score is the one that I'm collecting from the user and I'll filter to ones where it's zero. Once I've set up that filter I'll then go into the rule. I'll say something like bad feedback. Sampling rate is generally good to have at B1 and this is because I want to look at all data points although if is because I want to look at all data points, although if I don't want to look at all data points, I could set it to 0.5 or something like that. I'll then click on annotation queue. Maybe I'll create a new queue that's like bad chat lane chain runs. And then I'll send things there. Then when things go into that annotation queue, I can look at them. Honestly, the main value to start is just like seeing what types of data points were getting wrong. And then from there, I can leave more precise feedback. I can correct them and add them to a data set or something like that. The next use case I wanna talk about is sending data points with positive feedback to a data set. So this is useful because we can assume that these data points are good, they're good representative examples and that's great. We can use that for as an example to test against to make sure that when we make prompt changes in the future we still do well. We can use that as an example to use in a few shot examples or fine tuning to improve the application. So here, I'll go in, I'll change, it's basically the same as before, but I'm just flipping the user score to one. And so now this is positive. I'll add a rule. And this is like positive shadow C. Generally we'll take all of them. Again, you can change it if you want and then I'll send it to a data set. And so maybe I'll create a new data set and I'll have it be something like good chat LC examples. And then I'll select key value because I know that the inputs and outputs of these are usually dictionaries. And I will create it and save it. And boom, now I'm going to start building up this data set over time of examples with positive feedback that I can use in a variety of ways. The next one is sending child's runs whose trace got positive feedback to a data set. Okay, so what does this mean? So this means if I go in here, let me remove this filter. If I go in here, let me click on one, I can see that there's a lot of runs here going on under the hood. And so what oftentimes happens is that I want to, like this high level thing, it's great to have examples of that, but when it comes times to optimize what I really need, and by optimizing mean like use few shot examples or update the prompts, I really need few shot examples at one of these levels. Cause like this high level input and output, it doesn't help me at this step where I'm kind of like generating an input and output. And so here we can see the condensed question. We can see that we get as input questions and then this chat history stuff, and then we get out this response. And so this is the inputs and outputs that I need in order to optimize this kind of like node of my chain or of my graph. And so this is really what I wanna be building up examples for if my chain is more than one step. And if I wanna be optimizing the individual nodes. Again, there's value in getting the end to end data set, but there's also a lot of value in getting data sets for the specific nodes. Again, there's value in getting the end-to-end data set, but there's also a lot of value in getting data sets for the specific nodes. So let's see how to do that. So here, I'm going to remove this because I no longer want the top runs. I want the child runs. And so I want, I'm going to filter by name. I find that usually the easiest way to do it. I'm going to filter by name. I find that usually the easiest way to do it. I can see that it's condense question. I'm filtered there. I get all the condensed questions. Now what I want is I want only the runs where the parent trace got positive feedback. And I'm basically assuming that if it got positive feedback at the end-to-end thing, then the step here is correct. And, you know, that's not 100% a good assumption to make, but I think it's a pretty solid assumption to make. Go to Advanced Filters, we can add a trace filter, and then we add the feedback. User score is one here. And so now I have this filter set up where I'm looking at condensed questions where it got positive feedback on the end to end thing. And just to make that even more clear, let me add, actually this automatically applies to the top level trace, so that's fine. Now from here, I can do the same thing as before. I can do the same thing as before. I can do like, condense question data set. I can send this to a data set and create a new data set. And boom, there we go. So now I'm gonna start building up a data set of examples for this particular node in the graph. And so that's really powerful because again, those are what I need to kind of like optimize that call to a language model. It doesn't do me a ton of good to have a call end to end, that call at the node is what I want to optimize. The final one is just sending random data points to an annotation queue. And so this is good for just like making sure that you're seeing a variety of data points. So, you know, it's great to look at that data points, but you'd wanna keep on looking at, you know, you could just not be getting user feedback. That's very common. We don't get a lot of user feedback on ChatLangChain. And so being able to just send random data points and then look at those random data points is pretty valuable. And so what I can do is I'll just go back in. I'll add some rule, random data points. I'm gonna set a lower sampling rate here because I don't wanna look at all data points. I'm going to set a lower sampling rate here because you know, I don't want to look at all data points I want to look at random subset. I'll say like Yeah, sure. Why not? Why not 5% send it to an annotation queue. I'll create a new queue for this. I'll go like random questions from chat link chain and then I'll create that and Again, the idea here is, I'm just looking at a random subset. It's good for me to get an idea of what types of questions people are asking, stuff like that. So those are some of the high level common flows that we see people doing. I'm gonna dive into a few specific ones in the next videos.

<videoId>827QeizQbgU</videoId>
So in this video, I'm going to walk through a more end-to-end use case of using Ling Smith automations to optimize your application over time by constructing a few-shot example data sets. And so the example application that I'm going to build is an application that writes tweets for me. And so, you know, I like my tweets written in a particular style. And so I'm gonna build an application that can do that. And I'm going to show how to optimize that via a combination of user feedback and human annotation queue over time to build an application that writes more in the style that I like. So let's get set up. We can import, we'll use OpenAI for this, we can import Langsmith, and I'll create this really simple function. It's called tweeter. It takes in a topic, and it basically just calls OpenAI with this really kind of like just standard prompt. Write a tweet about topic and then returns it. What I'm also going to do here is I'm going to pass in the run ID and we'll see why I'm doing this but basically this is this is so that I can specify the run ID ahead of time which then lets me know how to add feedback for a run. So big part of this I'm not going to actually create a front end for this I'm just going to mock it out but basically the big part of this is collecting feedback from end users in some different environment, passing that into LangSmith, and then using that to kind of like optimize your application over time. So here, I pass in the run ID, I pass an NBA, and I get back this tweet. Just watched an insane NBA game. Those players are on another level. Hashtag NBA, hashtag basketball is life, and then some emojis. I kind of like this one. Let's pretend for simplicity, I'm going to like tweets where it ends with an emoji. And that's a latent thing that's impacting whether I like the tweet or not. And so let me leave kind of like positive feedback on that. Let me try again. Let's do like soccer. Okay, so this doesn't end with an emoji. So even if this is a good tweet, I'm actually going to leave negative feedback on this. Okay, so that's the basic idea of what's going on. Now let's set up some rules in LangSmith that will help optimize this over time. So what I'm going to do is I'm going to go into my project and I created a special project if you notice here I set the lane chain project to be optimization and so I'll go into optimization and I'm going to set up a rule that takes everything with positive feedback and adds it to a data set. So let me go into this filter I'm going to add this filter. Feedback user score is 1. I'm going to add a rule. Tweeting optimization, sample rate of 1. I'm going to send this to a data set. Data set name, tweeting. Let me create a new data set. tweeting, let me create a new data set, tweeting optimization and let's create that, let's save, boom. Okay, awesome. So now I'm gonna interact with my application a little bit more. So now let's write one about the NFL. It doesn't end in an emoji, so I'm going to leave negative feedback or no feedback at this point. I'll show how to incorporate negative feedback later on, but for now let's not leave any feedback. Let's do NDA again. back. Let's do NDA again. Still no emoji. Let's keep on iterating until I get one that ends in a... All right, let's try to get one that ends in an emoji. All right, so this is proving a little bit more difficult than we go. So this ends with some emojis. So I'm going to leave now positive feedback on this. Let's keep on iterating until we get another one that ends with an emoji. Alright, this one ends with an emoji. Let's leave positive feedback on this. So now what's going to happen is these things that I left positive feedback for, they'll start to get added to a dataset over time. And so these automations run every minute. And so I'm going to need to give it a little bit, but I should be able to go to a dataset and start to see these things pop up as examples in the data set. So here we are in data sets and testing. I can search for tweet optimization and I can see that I have my two examples here. So if I click in I can see the input and the output and I can do the same for over here and I can see that they're the ones that end in emoji. for over here and I can see that they're the ones that end in emoji. So now what I want to do is I want to make my prompt a few shot example prompt. And I want to start pulling these examples in and using them as examples for my application. Okay, so back in this notebook, I am going to pull down examples from this data set. So I'm going to use the links with client. I'm going to list examples, set the data set name equals to tweeting optimization. And again, this is what I've named the data set. And I can run this and I can get my example, which right now are two. And I can see that I have a bunch of information about this example. And the most important part is the inputs and the outputs. So I have here the inputs, topic, soccer, and outputs. And then I have an output key, and it's this tweet. So what I want to do is I want to use these as few shard examples in a prompt. And so as part of that, this is what it could look like. So let's say we could take these examples and put them into some string like this. And so we'll have kind of like this input output pairing. And then let's recreate our tweeting optimizer. And here inside, we'll actually pull down the examples. So we'll refresh this each time. This is maybe a little bit overkill because this will be another network call. So you could do this outside. But for this example, I'm going to do it inside this function. I'm going to create this string and then I'm going to edit my prompt. So it's still going to say write a tweet about topic, but then I'm adding these new things. Here are some examples of how to do this well. And then I pass in this example string up here. And so hopefully what we'll start to see as we give it a few short examples, it starts to implicitly kind of like learn what kinds of tweets I like and don't like. And so if we run it here and ask it to write a tweet about the Oscars, okay, awesome. So it added some emojis to the end. And so I can give positive feedback on that and it starts to pick that up. One thing that I wanna show now is how to do this same thing but start incorporating kind of like negative feedback. And so there's actually a few like interesting ways that you could do this. You could create the same automation and basically send all rows with negative feedback to another data set and then include those in the prompt and be like these are examples of tweets that the user did not like. So that's one thing you could do. But for a little bit more of variety, did not like. So that's one thing you could do. But for a little bit more of variety, I'm going to actually send it negative tweets to an annotation queue and then manually kind of like edit this and so this shows kind of like the human in the loop component. So maybe, so let's see. So here, let me run this a few times until I get a tweet that I don't actually like. Let me change the topic to something like AI. Okay, so here it doesn't end in an emoji. So great, I'm gonna leave negative feedback on that. Then what I'm gonna do is I'm gonna go into my tweeting optimization project. I'm going to set up a filter for where my feedback user score is zero and then I'm going to add a rule. Negative feedback and I'm going to send this to an annotation queue. I'm going to create a new queue, which is tweeting optimization. Let me create that. Let me save that. And awesome. Okay. So now when I go to my annotation queue, I should start to see these runs with negative feedback show up. start to see these runs with negative feedback show up. So here we are in the annotation queues and I can click into the tweeting optimization annotation queue I just created. And here is the negative run that I got. And actually there's four of them because I gave some downloads before and those showed up. So here, what I'm going to do is I'm going to edit them and then add them to a dataset. So what I can do is I can just like edit this, delete that, now ends an emoji. Awesome, now I can add it to a dataset. Perfect. Done with that one. Go on to the next one. Gonna edit this one, correct this to what I want it to be, add to a data set. Let's add it to, there we go. Keep on going. All right, this one doesn't have any, so I'm gonna remove the hash. Actually, you know what? I'm just gonna skip this one. So I'm just gonna be done with this without even adding it to a data set. And here, boom, add this to a data set. We're done, we're all caught up. Awesome. So now if I go back to the notebook, I can start to pull down the examples again. And I can see now, I have a lot more examples. And if I run this updated Twitter anymore, let's choose a new topic, like humans. It ends in emoji. If I go back to Langsmith, I can go to my project, I can go to my optimization project, I can click into here. And yeah, this is the run that I had. So this is how we built an application that sets up links with automations, that takes user feedback, takes ones with good user feedback, automatically adds it to a dataset, takes one with bad user feedback, automatically adds it to an annotation queue. A human can then go in and correct it, and it starts to build up this data set. And this data set, I'm then plugging back into the application, and it's using that in future iterations to start tweeting, in this case tweeting, but in your case, it could be whatever application you're building. It's optimizing that performance and making it better over time. And so this is one use case that we're really, really excited about. And we built a lot of the functionality specifically to enable this.