 Hey, Chris, remember that time we came up with the idea of doing attention in just one hour? I do, Greg. Yes. Yeah. Why is it that every time we go deeper into the transformer, into attention, there's just more to explore? Well, you could say that there are just too many layers to... Too many layers to attention and the transformer. Indeed, man. We are about to go on a journey today. Are you ready? We're going to get you coming back in a while here to show us exactly how this looks in code. But we're ready to tackle this. Attention. Let's go, everybody. My name is Greg. That's Chris. You guys know we're here to tackle this. Attention, let's go everybody. My name's Greg, that's Chris. You guys know we're here to try to break things down as simple as possible for you. And our latest is Attention. So what we're gonna try to dig into today is everything we need to know, set this thing up properly, and allow you to really try to grok the transformer decoder style-only architecture, the GPT architecture, and what's at the heart of it. All right. Questions are available in Slido. So jump in the chat and ask along the way. But we are going to get rocking and rolling here. Attention from scratch. Let's go. All right. So aligning our aim, but we are going to get rocking and rolling here. Attention from scratch. Let's go. All right, so aligning our aim, what we want to do is we want to understand attention and the calculation. We want to understand embeddings and positional encodings. And we want to then be able to code from the start all the way through attention. And then, yes, of course, we're going to go ahead and put together the rest of the Transformer, but we're not going to focus too much on that. So first, we want to kind of recall a little bit about GPT-style Transformers. We want to talk about self-attention. And then we're going to go ahead and code it from scratch. That's what we're going to do today. But first, let's just talk about attention at a very high level. Of course, attention is all you need. The famous paper 2017 started to really lead the way towards all of these large language models that we see today. And what we're really talking about is when we put a sequence in of words, we're using this mechanism of attention, also known as self-attention, in the context that we're looking at. We're not doing a cross-attention, we're doing a self-attention or attention, where we're looking at each token and we're deciding which others are most important to sort of capture the meaning. Ultimately, we're using this in the end to then go and predict the next word in the sequence. But that's the prediction of the next word in the sequence is not what attention is doing. Attention is simply paying attention. The same word can have many different meanings, for instance. And so what we want to do is we want to leverage this idea of attention and we want to sort of expand our view of the world of words by sort of blowing it up. We want to use many different layers of attention. And within each layer, each decoder block, you might say, we're going to also have many heads. And so whether you have many heads in a layer, or many layers, or many blocks, it's kind of this big idea that two heads are better than one, three heads are better than two, and so on. The more heads you have to potentially pay attention to something, the more things you can simultaneously pay attention to. And so this sort of this brains in a vat analogy, and straight from the paper, we're able to jointly attend to different representation subspaces at different positions. That's pretty epic, right? OK, so little background. GPT style, what does this mean? Well, we've talked about the transformer elsewhere, but the idea is the GPT style is not the whole transformer. The transformer that we are going to talk about today is only this decoder, this right-hand side of the transformer from the original paper. Of course, the encoder is really focused on understanding the input. This is sort of the BERT-style stuff. The decoder is then generating one word at a time, predicting the next word in the sequence. That's really what we're doing at the end of the day with the decoder. Now, the attention layer is sort of taking the role of this understanding piece, but it's not doing that alone. There's also some things that happen pre-attention that we wanna talk about. This GPT style, this autoregressive style, this all about predicting future values based on past values, autoregression future based on the past. Now, this is obviously good for generative tasks. A way we can visualize this is if we have some sort of sequence we've started, and then we want to start predicting what comes next. So if this case we're looking at a GPT-2 model that knows the laws of robotics from Isaac Asimov, we might start to get this sequence, this prediction happening. Okay now quick look at architectures that we've had along the way as These things have come out GPT-1 came out Circa 2018 this was a 12 layerlayer decoder-only stack. Each layer, we're going to sort of use the words layer and block interchangeably here, because we're talking about attention layers. And there's only one attention layer per decoder block. We have 12 layers or blocks. We also have 12 heads per layer. And about 117 million parameters this bad boy had trained on about four gigs of text. As we moved forward into 2019, GPT-2 comes out. There are like 48 layers now, 48 blocks. And we don't know exactly how many heads, but if we see a 10x, 10x on parameters and data used for training, again, these are each decoder blocks. Now, we can look at this and expand this out a little bit. And we can see for these GPT-2s, they did a bunch of different sizes. So this big these GPT-2s, they did a bunch of different sizes. So this this big daddy GPT-2 was kind of this one with 48 decoder blocks aka 48 attention layers. GPT-3 took this to a whole other level and went 96 layers, 96 heads per layer, 175 billion parameters, and now over 500 gigs of text. So just to be super clear, we're not talking about the encoder. We're not talking about the piece that actually is taking the information from the encoder. We're talking about straight up GPT style. All right. Now, I want to set the table for attention. As we think about this decoder-only style, each decoder block has two specific things going on. One is this masked self-attention. The other one is this feed-forward neural network. We're not really going to talk in detail about that, although Chris will mention it during the demo today. The masking is very simple. Masking says we can't look at tokens to the right because we're doing a next word prediction. The unmasked self-attention means we can. And so we're not doing this look at everything, including this stuff to the right, because we're not interested in really just trying to understand the whole thing. This would happen in, say, an encoder block in a BERT-style model. What we're doing is we're masking it so we don't know what comes next. Now, anytime we start predicting something, the next word, anytime we start our sequence, we're going to have this sort of start of sequence token. This will come back today. And let's sort of represent that by this sort of syntax here. And then, you know, as we go through, we'll predict the next word. And then as we go through again on the next pass, we're going to predict the next word. Now, of course, the is a very common word to start a sequence with. So you can imagine if you really didn't give a prompt, you might come up with this. And then the thing, that seems pretty reasonable as well. Now, if we look and see exactly how this is happening, because we have to get to this start of sequence somehow, what's really going on is the model is looking up the embedding of the input word. So the input word for just a blank sequence is a start of sequence token. It's looking up the embedding for that. And these embeddings are actually created during training. Okay, put a pin in that, we'll come back to that in just a second. It's looking at the embedding. And really what we're doing too is we're, after the first token, we're making sure that before we go into that attention layer, before we go into the decoder block, we're also providing some information about the order of words. Because this really matters when we're talking about sequences, right? Now, you can kind of visualize this again. If we take a closer look here, we can look at different time steps where you're feeding in an embedding, you're doing the positional encoding, you're sending it to the decoder, and then you're predicting the next word. And you can see this in a nice little sequence here. But what we want to do is we want to understand this layer, the embedding layer, and this layer, the encoding layer, before we get to attention, because it's really going to help us. So let's talk about embeddings for a second. Now, embeddings are simply this idea of turning text into numbers. And so we can sort of visualize this with a couple of little quizzes here. Where would you put the word apple in this embedding space we have here, in this two-dimensional space? And we got some toys over here. We got some cars over here. We got some fruit. C, right? Apple goes with C. Now, we can get a little more complicated here if we sort of have a two-dimensional space like this, where would we put the word cow, right? And because puppy is to dog as calf is to cow, we would probably put it at C. But this is only two dimensions. Now, what are these dimensions? Maybe the dimension is something like size and age, something like that. That's only 2D though. We need more dimensions if we want to be able to place words like the attention mechanism or YouTube live event. We need to be able to understand many more dimensions if we want to do this generally for all words. And the way that we do that is we do that with different types of embedding models. Now, the embedding models from classic NLP come in two flavors. One is they capture context. They capture how words are related to one another in N dimensional space. Where are they relative to one another? And then there's also no context. So what we want is we want kind of a lot of dimensions so that we could imagine measuring all sorts of things about the world and position words relative to one another in that way. Now, all of these dimensions, all of these sort of independent variable dimensions, this is sort of an important idea, especially when we get into application areas with attention. For instance, consider the idea of matrix rank that when we do fine tuning these days on large language models becomes very, very important because this is the idea of how many of those dimensions actually matter for what it is that we're paying attention to. This is sort of the key insight behind applications like low-rank adaptation or LoRa. But that aside, when we sort of studied this back in the day, NLP, the sort of no context, I'm not really worried about where things are relative to one another in that n-dimensional space. These are like your bag of words, your term frequency inverse document frequency is just normalizing the bag of words. Better are going to be those things that do capture context, your Word2Vec type applications. But also, and important for today, neural networks. Now, of course, Word2Vec visualized embeddings very beautifully. This is a classic example here. You can do these additions and subtractions that sort of make the idea of the word space make a lot of sense. But this isn't the only way to create an embedding model, this sort of Word2vec approach. The deep learning approach is to use a neural network. And in this sort of approach, your single kind of hidden layer is also called an embedding layer. Okay, and what it's doing is it's allowing us to understand how the words are related to one another. Now generally what we'll see in transformer applications is we'll see an input layer that is of much lower dimension than the output layer. So what we want to do is we want to expand our view. If we have a lot of words, we want to expand and blow up and zoom in on those subtle differences between those words. So if we're using the tokens or words as input, we wanna make sure our embedding dimension is large enough to capture all of those important dimensions, that sort of intrinsic rank. And we wanna do that in such a way that it feeds right into the transformer. Now, this is the idea of embedding layers as linear layers. So when we talk about embedding layers as linear layers, you'll see this come exactly into play in the transformer. This is just saying that that embedding layer has those weights. They're specific to the context of the words within our vocabulary. And what we do is we actually train this. We train this embedding layer when we deal with transformers and GPT-style architectures. So they're initialized randomly, say normal distribution 0 to 1. And then they're updated through back prop, not backdrop, just like any other deep learning neural network training. So what happens here is that when we have this sort of embedding layer, we're actually doing a little training on a linear layer that is a simple neural network. Now what's really cool is that we actually for every single attention calculation we actually have not one, not two, not three, but four linear layers. That is, we have one, two, three, four, five neural networks just to go from embedding through the first attention layer. Lots going on here already, right? And so we wanna kind of now zoom in on this idea of positional encoding that's also coming into play before we get to attention. Now the positional encoding is important, but the fundamental insight here is that since our model doesn't contain recurrence or convolution, the whole idea of attention is all you need, we have to inject some sort of information about the relative or absolute positions of tokens in the sequence. So there's the context of words relative to one another. That's great. But then there's the sequence information. And we have to capture that somehow. And the way that we capture that is with this particular method. Now, there's lots of methods to be able to capture sequence information, but there's a couple key advantages that were found during this research for this paper. You know, they thought that this would definitely work, allow the sort of transformer to easily attend by relative positions. That was true. Again, this isn't the only way to do that. But then importantly, it was going to allow the model to extrapolate the sequence length and make inference about sequences that were much longer than the input sequences themselves. And this turns out to be important because we want to be able to spit out as many words as we want based on as short of an input as we want. And, you know, perhaps the most interesting, related to our previous discussion, finding from this paper was that they were like, well, you know, we could have also done the whole learned embeddings, learned encodings thing, similar to the way that we're training the embedding linear layer. And they actually did that. embeddings, learned encodings thing, similar to the way that we're training the embedding linear layer, and they actually did that. And they said, well, you know, it turns out it's not necessary to use these learned sequences down here on sort of line E. They used a positional embedding instead of this sinusoidal encoding, and they found it's basically exactly the same performance as if I just use this simple encoding instead. And that was like, okay, well, this additional encoding has this great way of allowing us to increase the sequence length we can look at as well, and that's pretty dope, so let's go with that. We have this tokenization, this embedding, this positional encoding that happens. As we go from tokenization to embedding, to positional encoding, let's zoom in a little bit here. Let's take a simple phrase and let's break this down in a way that's really too simple, but actually really does help conceptually. So once upon a time, if we want to tokenize this, let's say we tokenize once upon a time. And we start doing this by assigning these numbers to each of these words. Now, when we get to the embedding phase, what we're going to do is we're going to say, OK, each of these once upon a time, we're now going to stretch them out into 768 dimensional vectors. This is going to allow us to sort of then compare contextually the difference between these words on 768 different dimensions, and that's pretty sick. Now even though we can do that, these are still sort of independent vectors. We haven't really done anything to talk about how they're sequenced. This is where positional encoding comes in. Now, if we roll through the calculations of the positional encoding, what we actually are looking at with this sine and this cosine is that for i equals 0, and this should say 767. We're playing around with embedding layer numbers a little bit earlier. For i equals 0, we're going to use this sign representation. For i equals 1, we're going to alternate. We're going to go to this cosine representation. And we can sort of do these calculations along the way so we get this sort of like zero one zero and so on and Since this is position equals zero we'll get kind of zero one zero one zero one for this position equals zero word Okay, now when we go to the next word We're gonna get something a little more interesting, because now our position is non-zero, so we get some actual decimals. And again, we're doing this sine, cosine, sine, cosine. Now we can visualize this, and we can visualize this for this setup that we had, where this first token position is this row right here, and we see this 010101. This second word position, we see this 0.84, 0.57, 0.81, and so on. Now, we're not really seeing this sine and cosine thing happening unless we zoom out a little bit. So if we looked at many more tokens in a sequence versus simply or if we looked at many more embedding dimensions we could sort of see for each of these 1, 2, 3, 4 tokens in the sequence, we still can't really see this sine, cosine thing. But if we look at many more dimensions here and many more tokens in the sequence, we can start to see that this, we start to see the sine, cosine-like waves going through this positional embedding. And this is sort of, again, this is the thing that allows us to be able to look at much, much longer sequences. Okay, so where does all this lead? It leads back to adding this in to the embedding layer. Now, of course, the embedding layer contains the contextual semantic relationships. All we want to do is add in the sequence data, but we want to retain those semantic relationships. Because if we just concatenated this, then we're in a different embedding space completely. Our words are not really relative to one another the way they used to be. So that's why we added in, we are now of a exactly the same dimension vector, but we've added in the positional encoding data. Okay, finally this brings us to attention. Now that we have this set up, we brings us to attention. Now that we have this set up, we're ready to rock and roll. We're ready to look at how attention is taking this information and is sorting through it, paying attention to what it needs to. We're going to take a close look at scaled dot product attention. In order to do this, we need to understand that we have Q, K, and V, query, keys, and values. And this feeds into the scale dot product attention. This feeds into the multi-headed attention. And this feeds into the decoder stack. We're going to take a brief aside for an analogy for query keys and values to rag systems today. Of course, retrieval augmented generation is about finding references in your documents based on a query, adding those references to your prompts and improving your answers. So what we do is we ask a question, we have a query, we send it to an embedding model, right? That vector is now something that we go ask a question, we have a query, we send it to an embedding model. That vector is now something that we go and we look inside our vector database for stuff that's similar to it. That similarity calculation allows us to then take our similar stuff and put it into a prompt template that is going to allow us to capture the important information, the reference material, before we put it into our LLM. And so this similarity returns these references that then improves our answer. So we can kind of say that in RAG, we're asking a question. That's the query. Our key is something like the docs in the vector database. We're sort of connecting our query to the rest of the docs in the vector. And we're sort of trying to find what's similar. And then the value at the end is all of our documents scored for similarity. Why is this relevant to attention? Well, because we're dealing with the scaled dot product attention. And cosine similarity is exactly a dot product. So what we're doing is we're essentially taking this idea of finding similar stuff and we're kind of adapting it to this attention mechanism. It's not called similarity, but actually it's called compatibility in the paper. Thisibility function is exactly the query times the key times being the dot product. Now, of course, you need to take a transpose with a vector dot product. But the idea is you get some number as a result of this. And what we want to do is we want to understand this idea of attention. An attention function can be described as mapping a query and a set of key value pairs to an output, where the query keys and values are all vectors. The output is a weighted sum of the values, where the weight assigned to each value is computed by this function. Now, let's take a look at, specifically, this scale.productAttention. We've got this matrix multiplication layer here. This is what's happening, this QK transpose. This is that compatibility function. Then we've got our scaling. This is important because we wanna be able to make sure that we're not losing our ability to pay attention to stuff of very different magnitudes. Then we have our softmax, which basically allows us to normalize our sort of what we've paid attention to. And finally, we come back and we map this back to our values. This is sort of that in the rag context sort of all documents scored for similarity idea. So let's go through each one of these one by one. This matte mole, this is a linear layer. OK, this is the dot product. So remember that idea of the linear layer, the little neural network, right? We're actually creating a linear layer, a simple, simple neural network with each of these. All right? And then as we sort of go to the next phase, we're now scaling. We're scaling because maybe we have very, very, very big numbers and very, very, very small numbers. And we wanna make sure that we're not losing track of that when we go and we do the soft max. Because if we have very, very big, very, very small and we do a soft max, we might lose some of the very, very smalls and pay too much attention to the very, very bigs, right? And so we want to kind of get out ahead of ourselves. The soft mask, the soft max is exactly ensuring that we have our big list and it all sums to one. But we want to make sure, again, from that scale, we want to make sure that we can measure the subtle gradients of compatibility of next tokens, compatibility of potential next tokens from, you know, one mask is simply the masked multi-headed attention where we're preventing that leftward information flow. We've already talked about that. Then finally, what we're doing at the end is we're mapping what that query key dot product found compatible, what it attended to back to the embedded encoded information that we started with. Let me say that a different way here. We have that embedding information, captures context. We have that encoded information, captures sequence. We're putting all that in, and we're converting that information into query keys and values. And the values sort of take that information and they flow it through the scale.product. What the query and the key are doing is they're finding what to pay attention to and then normalized, kind of spitting out what it should be paying attention to and then normalized, kind of spitting out what it should be paying attention to. Then we're sort of taking that what we should be paying attention to information. We're combining it back again with the initial embedding and encoding information. So again, let's sort of read this one more time. Attention can be described as mapping a query and set of key value pairs to an output, where the query keys and values are all vectors. This output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. So we can sort of visualize this. Another great visualization from Jay Alomar here, where the embedding is then being used to create the query keys and values. We're seeing this dot product. We're seeing the scaling. We're seeing the softmax. We're multiplying it by the initial value vector, in this case, vector. And then our total attention is this sum. Now, let's take our example once upon a time, and let's look at this query key value in a simplistic way here. So query is the new token. Key and value are both every token in the sequence, including the new token. So we imagine one pass through. First pass to query is once. The key and value are both start of sequence once, start of sequence once. What do we predict by the end of our decoder? Stack we predict upon. So the second pass, our query is upon. Key and value, start of sequence once upon, output A. Third pass, query is A, key and value, start of sequence once upon A, output time, and so on. Right? OK. So this gives us a great starting point for actually understanding how this really works, because of course, it's never that simple. In practice, these aren't vectors. They're matrices. And we'll see this in code for sure. But we can sort of visualize this here. What we're doing is we're actually setting up these weight matrices. We're setting up these other linear layers. Okay. We're setting up these and we're sort of randomly initializing some linear layers that are linear matrices. Right. They're 2D. And we're taking our embeddings and we're combining the embedding and encoding information with the weight matrix to create our queries, keys, and values. And so we essentially get this sort of embedding and encoding information combining with the linear layer, this weight matrix, to create what is actually flowing through our calculation, the Q, K, and V matrices. All right, now, this is, of course, visualized as we take it up a layer. It's visualized here, where the Q, K, and V that we put in are actually these matrix forms. And here, we can start to see these linear layers, these neural networks, are actually happening at the end of our attention calculation, but also at the beginning. And so realistically, we're doing this matrix form thing where we're taking the softmax of this isn't just a dot product of two vectors, but it's matrices now. And that's allowing us to go ahead and compute that output. Now the one thing we haven't talked about yet that we need to touch on briefly is that we also have multiple heads in each attention layer. But this is pretty easily dealt with. All we do is we say, you know, I'm gonna take attention heads 0 and 1 and 2 and I'm gonna create different linear layer initialized matrices that I'm going to use to calculate my Q0, Q1, Q2, and so on for as many attention heads as I want. I'm going to flow those through separately, and then I'm going to concatenate them at the end. So we see this sort of concatenation happening at the end. This is the exact idea of calculating attention separately in each of the different heads, right? Because it's like, what are you paying attention to? Well, let the independent head decide, then come back, put all the brains in the vat and concatenate these babies together, allowing us then ultimately to jointly attend to information from different representation subspaces at different positions. With that, I'm going to send it over to my man, Chris the Wiz to show us exactly how this looks in code. We're going to try to get to as many questions as we can today, but enjoy the ride on the code everybody from here. Let's see how to code up attention from scratch. Thanks, Greg. Yes. We are going to start as we normally do, which is with just the beginning. So this is what we're going to be modeling. We're going to be using Carpathi's Nano GPT as a base. Since it does have a lot of the code usable and ready to go. We're going to use it as a base primarily to make sure we have some idea of what the decoder-only architecture is doing. So you'll notice that we have this architecture diagram, right, where we have our output embeddings or positional encoding. We have our causal attention. We have our layer norm. We have our feed-forward network. We have another layer norm, and then we have our causal attention, we have our layer norm, we have our feed forward network, we have another layer norm, and then we have our actual predictive head. So this is what we're going to be creating. Now, before we create that, we wanna do a couple things. The first thing is get some data. So we're gonna use the tiny Shakespeare data set, and we're going to need to do some pre-processing. So one of the things that Greg mentioned near the beginning was this idea that our embeddings layer is gonna convert text into numbers. And it does that with the assistance of what's called a tokenizer. So before we go to that embedding layer, we wanna make sure that we're already dealing with some kind of numeric representation of our text, and that is going to be our tokens. So we're going to first grab tokens, and then we're going to start interacting with our model. Now the notebook is going to have a more extensive, you know, kind of explanation of byte pair encoding, which is the kind of pre-tokenization or tokenization that is used for GBT2, but we're gonna largely skip over that for now. But the idea is simple. We wanna take text and we wanna convert it into numbers. And the way that we do this is by using a couple of clever techniques where we consider common pairs of letters or common words as specific individual numbers. And then each letter or each base piece of information, a byte is going to be kind of the root token. So this means we can represent any text with our tokenization method. It doesn't have to be words we've seen before or tokens we've seen before, which is very handy. So we're going to go ahead and leverage this to tokenize our dataset. But first we want to train our tokenizer on our dataset so that we can create those common pairs or sequences of individual units of information. So the way we're going to do this is just with the transformer. pairs or sequences of individual units of information. So the way we're gonna do this is just with the transformer. We're gonna go ahead and we are going to train it up and then we're going to load it up. Now, once we've done this, we have the ability to convert our text like this, just a passage from Shakespeare, right? Into a series of tokens where we can see Hark, comma, my name be Romeo. And then we also see due to the fact that we have this subword tokenization method, we also see this idea of summer and apostrophe s being split apart because this apostrophe S might mean something. Now, if we look at what this looks like in the actual, you know, behind the scenes and what the model is going to see, it's just a bunch of numbers. And those numbers are associated with the specific text tokens. This is what we're going to use to feed into our embeddings layer is these tokens or numbers. And that's what we're going to effectively stretch out with that embeddings layer into those vectors that Greg was talking about. You can absolutely use a different kind of tokenization. It's not critical that you use byte pair encoding. You can use whatever tokenization best suits your data. So keep that in mind. Then we're gonna tokenize the data set. Keep it brief, but we tokenize every single string in our data set. And then we get back just a massive list of numbers. So originally, we had one big text file. And now we have one huge array of tokens. Great. Next up we're gonna start the process of training the model. Now in order to do this we need to do a bunch of initialization so we're going to do this here. We're gonna do some imports and I'm largely gonna breeze through this section because it's not what we're focused on today but the notebook should explain each of these steps individually for you in more detail, if you wanted to pause or work through it more slowly. So we're basically just gonna set up some parameters that indicate how much we want to evaluate and log. We're gonna indicate the data set we wanna use. We're gonna set up some typical hyperparameters, including our batch size and our block size. Our block size is anonymous with our context window or max sequence length. So whenever we're talking about that sequence length, we're going to be talking about this 512 here for the rest of the notebook. Now that's very important because as Greg explained to us, the number of tokens in a sequence is going to indicate which position they are in. Right? So we have from the 0th token to the 512th token. Pretty cool. Then we're going to set up some basic model architecture. We're going to emulate the small version of GBT2. So we're going to have six layers with six heads each. And we're going to use an embedding dimension of 516. So this is the, when we were describing, right? When Greg was describing that embedding space where we had, you know, cow and dog were close to each other and puppy and calf were close to each other, right? Instead of using those two dimensions, we're gonna be using 516 dimensions, which is pretty dope. Then we're going to have our hyperparameters. Our hyperparameters are pretty standard across the board. You can watch some of our other YouTube videos where we talk a little bit more about these when we're talking about training or fine tuning. But for now, these are kind of stock settings that we have to apply, and they're independent of the actual mechanism of attention. And then we just have some hardware stuff. This is all basically just stuff we need to do to make sure that this works on our hardware. And we're going to do the same thing here, which is we're going to create something that we can use. And what that's going to do is it's going to feed our model the correct data when we ask for it, which is this getBatch helper function. And this kind of indicates what is getBatch. You can see that we have these indices. You can see the first 10 indices of our x are going to be 68, 60, and 81, and so on. And the first 10 elements of our x are going to be 68, 16, 81, and so on. And the first 10 elements of our y are going to be this same sequence, but offset by one. So we have 16, 81, 23, 5, 8. The idea here is that this token's label is the next token. And this token's label is the next token. So you can see here, that's what it's doing. And that's how our data will be used to trade. And then we're gonna set up some more model arguments and build our actual model. And once we've built our actual model, we get this. This is our summary of our model. You can see that we have this WTE or Word Text Embeddings. We have our WP, Word Position Embeddings. We have a dropout. Then we have our module list, which has our six attention blocks that have the WP, word position embeddings. We have a dropout. Then we have our module list, which has our six attention blocks that have the layer norm, our causal attention, another layer norm, and then our MLP, or feed forward network. You'll notice that these are all linears without activation. And our feed forward network has that activation, which is using JLU from the GBT2 paper. So this is the idea, right? This, all of this right here, is exactly recreating this diagram, right? So we have our attention, we have our embeddings, we have our layer norms, we have our feed forward. So that's what that is recreating, right? So let's talk about each layer kind of step by step quickly. First of all, tokenization, as we discussed before, we could take our input text and then tokenize it into these numbers. That shape is going to be batch sized by our sequence length. Then we're going to embed each of those. Now in the code, this is simple. We're just gonna use the torch embedding layer to do this. We're gonna configure it to have the shape of vocab size by the embed dimensions. So what this means is that we have a embedding for each of our tokens in our, you know, our vocabulary. Sorry, brain froze there. tokens in our vocabulary. Sorry, brain froze there. Daniel, the position encoding is happening in this position right here. This WPE is where we're gonna have our positional encoding. You'll notice that the embedding layer dimensions are that vocab size by the embed dimension. And then our positional embeddings are the sequence length, that 512, by our embedding dimension. Very cool. So this is how embeddings look in code, nice and easy. And this is what we're doing. We're converting each of these tokens into a list of numbers or vectors, and those vectors are just the embeddings that represent where our token lives in embedded space. It has a shape of batch size by sequence length by embedding dimension. Next up is our positional encoding. Our positional encoding has two implementations. The one we use in this instance is going to be the one used in the GPT-2 paper, which is learned embeddings, which is going to have that shape of block size or context window by the embedding dimension. But we can also use this other representation, which is the one that Greg showed us and talked about, which is using this equation or the set of equations to construct or, you know, calculate our positional encodings. And you can see here that our positional encodings are just added element-wise per token to our actual embedding. So we get the same shape object, batch size, sequence length, embedding dimension with our positional encoding. We're just adding that positional information along each of these vectors. So the position of this vector will be zero, this one will be one, this one will be two, this will be three, and so on. And then the i-th element, this will be the zero-th element, all the way up to the 768th element. And then finally, we get to attention. Everyone's favorite. In code, attention is very straightforward, thankfully. The concepts behind it, as Greg walked us through, are quite in depth, but the code is pretty straightforward. The only trick that you'll see here is instead of having three separate attention layers, we're gonna basically combine them into one batched embedded or linear layer. So that's the only trick. It's exactly the same thing. It's just using a different convention. You'll notice as well that we have a number of different parameters that relate to our regularization techniques. So we have some dropout on both our attention and our residual networks. And then we do care a lot about the number of heads that we have, as well as our embedding dimension. When we're actually calculating attention, it's pretty straightforward. All this code looks pretty intense, but we're going to scroll on through to the key components. This line here is exactly the scaled dot product attention. You can see that we have our Q at k transpose, and then we scale that by that k-th dimension. Now, this k-th dimension is related to the number of heads that we have. We're going to kind of just work past that for a second. And we're just going to imagine that this is a scaled factor that's proportional to the actual size of k. So this is our scaled dot product attention. And then we're going to use our attention mask. You'll notice that all we do is we mask up to the sequence length, and then we let the rest of the array be these minus infinities. Now, what Greg said earlier was that if we don't scale our dot product attention, we might wind up in a situation where we zero out values, and we really don't want to do that because then we lose information about them. Well, this minus infinity is a incredibly small number. It is the smallest number we can represent in Python. And so what that means is that we effectively remove those from mattering when we're doing our actual calculations. So that's what this mask is doing. It's basically saying anything passed up to our sequence length, we just don't care about. It's going to become 0. It's going to contribute nothing. Then we're going to do our softmax. This is done with the f.softmax. And then finally, we're going to matmul our attention scores back to our value vector. And this is what's happening in this diagram as well. Just zoom out a little bit here. We take our QK. We matmul them together. We scale them. We apply our causal mask. we apply the softmax, and then we matmul that result back together with V, and we have the sizes here so that you can see exactly what's happening. Now, these sizes are independent of the batch size or the attention heads. So you would add additional dimensions for the batch size and the attention heads ahead of each of these pieces. One very important thing to remember, let's say we had our sequence length or our embedding dimension, let's say we had that be, you know, we had eight heads, right? So we're actually going to have 64 in this place instead of 512 because we're going to divide our 512 by 8. So all that happens when we use multi-head attention is that we scale these values relatively to the number of heads that we have. That's why it's critical that they divide cleanly. And that's the attention mechanism. After that, it's just, again, some more setup. We're gonna initialize our optimizer. We're going to compile if our model is compatible with compiling. We're going to create our loss estimation helper function, create our learning rate scheduler, and then we're finally gonna go through our training loop. Our training loop is basically just going to compute each of those losses and then back prop to update our weights, all of those weights that Greg talked about, the four matrices per attention layer, plus our feed forward, plus our layer north, plus our, there's a lot to update, right? So we're gonna update all of that and once we're done training we have the ability to generate and we can generate some fairly coherent Shakespeare text which follows that format that you would expect and has the kind of poetic e-language that you would expect this model was trained for only 5,000 iterations on a very small data set, but it does a good job at emulating Shakespeare. And that is the attention mechanism. The one thing that I want to point out before I kick you back to Greg is that this piece here exactly corresponds to this diagram, right? So that's all that's happening in the code for that diagram is right here And that's it. So in code not so bad, even if the concepts are a little bit tricky And with that I will pass you back to Greg. Whoo. Yeah. Okay. All right Beefy event today Chris just to wrap up everybody before we do a few questions here we talked embeddings and this is talking about where words are relative to one another in n-dimensional space. We might say tokens to be a little more specific. Positional encodings is looking at where those words are relative to one another in a sequence. And then self-attention at the high level is for each word or token, which other words or tokens are most important to understand meaning. Remember, self-attention is not doing the next word prediction. It's about that understanding. And then the scaled dot product attention is mapping what the queries and the keys have attended to back to the embedded and encoded information. And a real useful way to think about QKMV is to think about the rag context and that'll sort of help you along the way. So let's go ahead and take a couple questions. We've got some in the chat here, Chris. And just, I mean, you know, it's like every time we think we're gonna do an event on something, it's actually, we go deeper on something else. And I don't know how many actual events are necessary in the sequence for the full transformer, but it's a lot, isn't it? It's a lot. It is most certainly a lot. Yes. You know, and we'll get it right one of these days, everybody. And we'll get it right one of these days, everybody. We'll do a proper service of exactly each piece. But I think one of the questions that is sort of two questions that are related, one is, could you please explain the difference between the key value and the output? Can't seem to wrap my head around that. And then how is the value matrix calculated? I'm going to go back to a slide if we keep the slides up for a second. I think that, you know, if we, and, you know, personally, I've been trying to wrap my head around this. Chris has been helping me. But, you know, I think K and V, it's important to understand, are exactly the same heading into these calculations. exactly the same heading into these calculations. That is, they take the embedded and encoded information, they flow that through a randomly initialized linear layer, and they're sort of there, ready to go. K goes in through all of this, and V doesn't V just sort of sticks around and so what you see in the calculation to address the first question is the whole calculation of soft max Q K transpose over this scaling factor is Trying to decide sort of based on the query What should we be attending to and then it then it's normalized because of the softmax. And then we're sort of saying, okay, that normalized matrix, we're gonna map it to V directly to make it more specifically aligned with the encoding and embedding information. Chris, is that right? Something like that? I think so. I think the way that I would think about it is that all of these are the is that right? Something like that? I think so. I think the way that I would think about it is that we all of these are the same, right? So in self-attention, at least, all of these QKV vectors are literally the same thing, which is great, easy for us. But they're modified or impacted by their respective weight matrices, and that learned information from those weight matrices is what differentiates them at the end of the day. So we're learning different things by passing them through these different weight matrices, and the idea is, as Greg said a few times, we're comparing that QK, we're using it as kind of this similarity match or compatibility match, and then that value is like, we need something at the end of the day to ground us back to our actual sequence. And that's what that V is going to be doing for us. Yeah, yeah. So, so, so Daniel asks, so k is exactly equal to V. And it's like, yes, Daniel, K is exactly equal to V. Now, the thing that is really crazy is that actually, and what Chris just sort of said there, and maybe you missed it, is like Q is also exactly equal to K, is also exactly equal to V in code. Conceptually, it doesn't really help to think about it like that Because we want to think about Q as the new token, but Chris. I don't know if maybe you can comment a little bit on that No, I think that's exactly right. They are exactly the same And that's because of self-attention right we are letting all tokens in the sequence attended to each other And so this is the idea, because we're doing that, we need them to all be the same. And it's one of those things where, you're exactly right, Greg. Conceptually, it might not do a lot of heavy lifting, but in the code, they are exactly the same. When we talk about things like current token, or we talk about things like, you know, comparing this token against another token, under the hood, this is all being conglomerated into one big matrix multiplication. There is no difference, but it helps a lot from a conceptual standpoint to consider it as if it is kind of this token by token mechanism. Yep, yep, yep. Man. Yeah. That's exactly right, Daniel. That's how I feel 100 percent. But it's wild to see sort of what you need to understand and how exactly it's implemented in code. Chris, we talked too long today. If you guys want to see go deeper on anything, shout it out in the chat, let us know. Drop a comment on YouTube. We'd love to continue to bring you what you need to sort of get where you're trying to go. But thanks, it's time for wrap up, Chris. You nailed it. So that's a wrap today, everybody. What we're gonna continue to do is bring you content on the stuff that matters. We've got a RLHF event next week that's gonna start our alignment series from RLHF to RLAIF to DPO in the coming month or so. We've also got another event on the art of rag evaluation coming up soon that we're partnering with Lang chain and Ragas on so that's super cool and then of course if you guys want to go deeper we've got an upcoming course happening starting February 13. It's our brand new AI engineering boot camp. It's end to end. We took everything that business really wants that is aligned with industry related to prompt engineering, RAG, fine tuning, deploying prototypes into production, what to do after they get there. and we put it all into one seven-week course. So definitely check that out if you're into it. We'll be running those cohorts for the foreseeable future. If you have any questions, reach out. But if you have feedback on today, go ahead and we'll drop a feedback form in the chat. You can always leave feedback on Luma. We do pay attention to it, and if there's anything you'd like to see next, let us know. That's it for today, everybody. That's a wrap. Until next time, keep building, shipping, and sharing, and we'll keep doing the same. See you soon.