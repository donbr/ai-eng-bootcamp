 Hey everyone, are you sick and tired of people showing you this same old diagram? Chris, how about you, man? Yeah, I absolutely am, Greg. It's a very messy looking diagram. There's a lot of words here, but not a ton of information unless you really have already studied the diagram in great depth. Yeah, Chris, and just for the record here, is this actually chat gbt that we're seeing? Uh no not really Greg no it's not. No not really okay so let's get to the bottom of this once and for all. The problem Chris and I are going to help you solve today is the next time somebody hits you with this diagram this terrible slide you're going to know exactly what's up. AI Makerspace does Transformers today. Let's go. My name is Greg Loughnane, and I am the founder and CEO of AI Makerspace. Thanks so much for taking the time to join us for the event today. If you have any questions, you can drop them straight into the Slido link in the description box on the YouTube page. We're of course going to get to as many as we can by the end of today's event. And what I'd like to do is I'd like to go ahead and welcome Chris back up to the stage. Chris is my good friend, CTO of AI Makerspace and LLM Wizard, known to many as a great content creator on YouTube, an awesome instructor in and out of the classroom. He's always building, shipping and sharing his work. Chris, are you ready to transform the way people look at this diagram? I absolutely am, Greg. Yes. Yeah. Heck yeah, man. Well, let's go ahead and get right into it. Chris, we'll see you back in just a little bit. One of the things that AI makerspace we like to do is we like to go ahead and align our aim. AI makerspace or aim, we want to make sure you know what you're getting out of this session if you stick around with us. By the end of today, you will have developed an intuitive first principles understanding of how transformers work and what exactly is going on. What are we feeding into these systems? What are we getting out of them? You'll understand how and when to pick up the different styles of transformer tools and what kind of tasks those tools are good at and not so good at and why. So what we'll do is we'll kind of try to tackle this big transformer idea one bit at a time. That's gonna lead us to attention, famously, all you need. We'll see about that. Then we'll look at some different styles. We'll look at some different common names you may have heard, but maybe you don't exactly know what's going on under the hood, Bart, BERT, and GPT style models. We'll look at specific use cases, we'll see how to code them up, and this will all give you a flavor for how we're gonna be able to dive much deeper after we get our high level intuition today. So what is a transformer anyways? There's a lot of ways to start this discussion, and most people like to start with something like this. Well, you remember convolutional neural networks or recurrent neural net. We're not going to do any of that. We're going to assume that you're here and you're interested in transformers. And maybe you don't have the most prerequisite knowledge of taking every single class on deep learning AI and going deep into doing stuff with images and words and just being really fluent in deep learning in general. What we're gonna do is we're gonna start from the basics. So let's start with vocab. There are some keywords that we need to make sure that we know. And if we don't know them, we can't even talk about transformers because we start getting confused off the bat. First couple of key words here that we need to tackle. Number 1, tokenization. Number 2, embeddings. Number 3, encoding. We're going to talk about positional encoding first. This is not to be confused with the second type of encoding that we'll talk about a little encoding first. This is not to be confused with the second type of encoding that we'll talk about a little bit later. Everything you need to know is in this image. We see tokenization up top here, the financial times is, and we see some pretty colorful pictures that are labeled word embeddings and positional encoding down below. What we want to do is we want to break this thing down and make sure that we understand what's going on. First off, tokenization. If we have a string of words, a sentence, oftentimes we want to, so we can feed it into a computer, chop that thing up into pieces. Those pieces that we chop it up into, those are just tokens. That's what we call tokens. Now, tokens are usually not words. They're usually subwords. But we can think of it and develop an intuition by thinking about tokens as words. And to give you a couple of examples as things start to get interesting with tokens, take, for example, the name O'Neill. If you were going to tokenize the name O'Neill, you might get down to an O'Neill and feel pretty good, like, yeah, that's the right tokenization strategy. And it's not so clear for all words, though. So for instance, aren't, are not. If you get down to rnt, not feeling so good about rnt. Not exactly sure that's capturing the semantic meaning behind the word. So for us, let's just think about tokens as if we had an input like friends, Romans, countrymen, lend me your ears, comma, comma, comma, semicolon. We're ditching the commas and semicolon, and we're sort of tokenizing at the word level. All right, tokens. Got it. Words go into tokens. Tokens are then turned into embeddings. What embeddings are is they take tokenized text and they convert that tokenized text into numbers, into vector representations. Those vector representations are important because they allow us to compare the words to one another from a semantic meaning perspective. That's a lot of big words, so let's make sure we understand this from first principles. Embedding quiz number one. Where would we put the word apple? Feel free to throw it in the chat, but it's clearly not going to go with sports, buildings, or over there in the middle of nowhere, apple's gonna go with the other fruits over at C. But what about if we consider this second example in Betting's quiz two? Where would we put the word cow? Well, puppy is to dog as calf is to cow, right? So C, it's not A, as many people often get wrong, but it's C. And it's because of the relationship between each of the words in our space here, in our vector space, you might say. And so the X dimension here, the horizontal dimension, now we're looking at maybe something like as the animal ages, maybe it's age. And the Y dimension here, vertically, maybe we're looking at something like how the animal grows. Maybe it's size. But like, where does the transformer fit on here? Where does Optimus Prime fit on here? It's not clear, right? We need more dimensions. We need more than two. Well, how many do we need? Well, maybe we need a lot, depending on how many things we want to talk about. This example from Coheer shows word embeddings that use a 4,096 dimensional vector to represent. You can get really crazy with this. And you can also do embeddings at the subword level, at the sentence level. But the big idea is that we're turning the tokens into embeddings. Those are numbers. That allows us to compare things semantically. So for instance, ten years ago, word to vec came out. Guess how that was named? And it came out and allow us to kind of look at words like king and man. Those are semantically close. And look at words like woman and queen. Those are also semantically close. And the difference between those two sets of words is approximately the same. So we can do interesting things like king minus man plus woman equals approximately queen. And this worked. This actually worked and this was fantastic and a great breakthrough. What we decided was that embeddings like this were very, very good at many downstream NLP tasks. Downstream tasks has stuck around. It's industry speak for what we call any of those supervised learning or generative AI tasks that use any sort of pre-trained model, like for instance, WordEffect. This is going to become important later as we start picking up pre-trained models to do downstream tasks. One more piece of the puzzle here, we have tokens, we have embeddings, and we have encoding. embeddings and we have encoding. One of the things that we want to think about is well we have the embeddings, those are based on the words, now what we want to do is we want to make sure that we tell the model how the words are set up relative to one another. We want to make sure that the first word in the sentence is shown first. We want to make sure that the first word in the sentence is shown first. We want to make sure that the second word in the sentence is tracked as being second, and so on. We want to understand where words are relative to one another. We want to provide that context. And that's going to allow us to give meaningful positional information. It's going to allow us to give meaningful positional information. It's going to allow us to encode that information. So in this case, this example, this positional encoding 0011, is simply saying this is the first word in the sentence. Since this needs to be applicable to many different length sentences, these aren't whole numbers and it exists along a spectrum. So the colorization here shows one way to visualize encoding. But the big idea is j'ai sui Ã©tudiant. That is going to be first word, second word, third word. Any other specific thing that we're doing is going to also be positionally encoded, no matter how long the sequence is. This is important because this is exactly the first piece of our transformer diagram. We have our inputs. We have our input embeddings that we create after we token have our input embeddings that we create after we tokenize our input and then we positionally encode those embeddings by simply adding some additional vector to them. That vector provides that additional information to allow us to capture semantic meaning in a bigger context than just within the word. All right, so this is all set up. Tokenization, chopping the words. Embeddings, words into numbers. Positional encoding, words within a bigger context, a sentence. So now that we see the tokenization of the financial times is, and we see word embeddings and positional encoding as colorful blocks, this should start to be making a little bit more sense. So here we are on our incredible slide, and we've checked off positional encoding, input embedding, and inputs. Now, just a note here on output embeddings. If you have inputs and outputs and you're doing a supervised learning task, that's going to be where this output embeddings comes into play. We'll touch on this when we do our example of BART use case shortly. But we have a lot to talk about still. Let's consider that the transformer can be looked at not just as this giant amalgam of things, it can be looked at as just one block, one black box, if you will. We can put input for a translation task, let's say, output. Input in French, output in English. I am a student. Now, if we zoom in on the transformer, we can see that actually it's made up of two key types of blocks, encoder blocks and decoder blocks. And we've shaded here that this is an encoder block and this is a decoder block. What you're seeing here is that first off, we've got our transformer. It's made up fundamentally of encoders and decoders. Notice this is actually plural. Encoders, we have many encoder blocks, and we also have many decoder blocks. Here we have shown just one of each. What we want to do is we want to first of all note, we just talked about encoding and public service announcement. Encoder is not equal to positional encoding. So to encode means simply to convert into a coded form. That's it. So we're using this sort of very generic word across many different concepts, track positional encoding versus encoders. OK, let's dig deeper. Self-attention starts by we can start understanding self-attention by applying the transformer to machine translation. And everything that we need to understand is contained within this GIF, within this moving image. So right now we're in the decoding phase, but as we restart, let's take a look at how the transformer starts by generating embeddings in the encoder phase for each word. I arrived at the. Those embeddings generated, those are the unfilled circles. Then using self-attention, it takes into account info from all the other words. Those new representations are shown by the filled circles. That step is repeated many times. And the decoder then operates similarly, although generates only one word at a time from left to right. It pays attention not only to the other previously generated words, but also to the final representations generated by the encoder. Of course, the task is to generate the next word. Note it pays attention to not just the word it's generating and everything before it, but it pays attention to the understanding that was developed by the encoder. So the encoder is focused on this understanding of the input sequence, this encoder right here. That understanding is then passed directly to the decoder, which is focused on generation. In fact, you can't actually do generation without a decoder. So what we're doing here is we're actually masking the word that we want to generate. We're looking at all words to the left of the word that we're trying to generate and we're considering only that. Before we get the input from the encoder. Now we're also considering the context that we're in in terms of the input as a whole. So what we want to do is we want to sort of combine this encoder, decoder. Each one is valuable for something different idea to see what we can do with it. And this is where understanding attention comes in. Because now we're pointing directly at attention blocks. It's time we talked about attention itself. Encoder-decoder attention, as shown in this particular diagram right here, is shown as distinct from self-attention. But the encoder-decoder attention simply allows the decoder to focus on the appropriate places within the input sequence that the encoder has determined are most meaningful. So that encoder-decoder attention layer attends to the input sequence, and it works just like any other multi-headed self-attention. The self-attention blocks you're seeing here are also self-attention. So encoder-decoder attention and self-attention, they're both self-attention. So encoder, decoder, attention, and self-attention, they're both self-attention. But encoder, decoder is simply taking what we got from the input and leveraging it. OK, what does this look like in practice? Well, in general, we just are saying, when we say attention, we just want to look at one thing in our case one word then we want to find similar things we want to find things that are related we want to find things that are meaningful we want to find things that actually affect that one thing we're looking at. Because when you're only looking at similar things, rather than looking at all of the things and all of their interactions together, you can look at a very, very long context because you're not paying attention to everything the same amount. This is the key idea. So if you look at this example, I have no interest in politics, and we look at the word no, we might say what other words affect the word no the most. Or if we take the word interest and we look at a few different examples, it can be used in many different contexts. For example, I have no interest in politics versus the bank's interest rate rises. You can combine these into a single sentence and you can try to track exactly what the word interest means across the entire sentence. I have no interest in hearing about the rising interest rate of the bank. Yeah, you and me both. For the first use of interest here, we have no and in as the most attended words. While for the second use, we have rate and bank as the most attended words. So what we're seeing is we're seeing that our self-attention mechanism, the attention scores we're actually calculating, these are telling us what to pay attention to. Now it's not always this simple, in fact it almost never is in practice. When we think about attention we have self attention blocks within each of the encoders Within our stack so we have many encoder blocks within each block we have a self attention Within each self attention We have many heads each self-attention we have many heads. More heads, of course, is better. Two heads are better than one, right? And in fact, if you have many, many heads within each self-attention block, you will probably, in general, be able to produce better results. You might think about it like the wisdom of the crowd. You might think about it like many people coming together on one problem is going to produce a good result. However, if too many people come together on a problem that doesn't require that many people, it's going to kind of be wasted resources, right? Now, you might say, how many heads is the maximum number of heads? Well, how long do you have to predict the next token? We can put as many heads as we want, but we don't want to do that because it really eats away at the computational scale. So what we've seen is we've seen that rather than sort of producing these brains that are made up of many, many, many, many, many different heads, we're generally seeing, you know, six heads, eight heads, twelve heads. Kind of more like this few brains in a vat vibe than this sort of hive mind vibe when we come to choosing how many attention heads we have. In fact, what you'll see today in our GPT style model that will show GPT2 is we actually have many heads. And oftentimes it's hard to understand exactly how many heads we have because it's kind of a bit of the secret sauce, the number of attention heads for any given model. But oftentimes they will tell you how many decoder blocks, for instance. In this case, there are 12 in this diagram. So we have how many blocks and how many heads. All right. So that kind of gets us to a bit of an understanding of what's going on with putting stuff in, with looking at decoder blocks. I'm sorry, looking at encoder blocks here, and looking at decoder blocks. Many blocks, many heads within each block. So how can we actually use this? Well, let's consider Bart. Bart is a model that uses the encoder and decoder architecture. You might say it's the type of transformer called the sequence to sequence model. So we have a sequence of text as input, and we generate a sequence of text as output. We actually call these encoder-decoder models BART style or BART-like models. Also, T5 is a similar type of model that uses both an encoder and decoder. These are very good at tasks that require an input and also are generative. So for instance, translation is a classic. Summarization is a very good one. But that understanding of the input is key. BART actually stands for Bidirectional Auto-Regressive Transformer. Let's break this down for a second. That encoder-decoder sequence-to-sequence model, you should be getting pretty familiar with that, bidirectional encoder or BERT-like encoder is simply saying that we understand the sequence from left and from right. We understand it from both directions, not simply left to right, not one direction. Auto-regressive is nothing fancy. It simply means we're predicting future values based on things we saw in the past. In a word context, we're predicting the next word. We're very familiar with that predicting the next word idea. That's all auto-regressive means. So what we're gonna do is we're gonna do a little example here now where We're very familiar with that predicting the next word idea. That's all auto regressive means So what we're gonna do is we're gonna do a little example here now where we're gonna show Bart base model and We're going to do an example where we're looking at the bill summarization So we're gonna take US congressional and California state bills We're gonna summarize them we're gonna fine- tune a BART model. And we're going to see how this encoder decoder style architecture performs on such a task. So for that, we're going to hand it off to Chris, the LLM wizard himself, to show us exactly how it's done. Chris? Hey what's up? Yes, so we're gonna talk about BART. We're not gonna talk too much about the technical details of BART. Thanks Greg for walking us through those. But we are going to talk about how we might want to leverage this thing in the wild, what kinds of things it's good at, and then we'll see an example. So, you know, the idea of BART is that we do have that, you know, both options, right, between the encoder and decoder style architecture. So we have, not only do we have that kind of good knowledge distillation, but we also have great generation. So what we're gonna do is leverage that to create a summarization model. So we're gonna take this base BART or BART base from Facebook and we're going to fine tune it on a sequence to sequence task which is going to be a summarization task. And if we can drop the notebook link in the chat, that'll be great. And basically the idea is we're gonna load our tokenizer in our model, hopefully the same stuff that you're used to by now, and then we're going to load our data set. Our data set is gonna be this Billsum data set. You can see here that we have this huge big chunk of text with lots of legal definitions and legal language. And then we have this kind of more natural language or plain English summary of that bill text. And that's what we're gonna be creating today. We're gonna get our data set. You'll notice that we have a thousand rows in this data set, but we're gonna just pair that down a little bit to 500 rows so that we, you know, we're training on just a subset of the data set so it doesn't take too long. We're gonna split this into train, validation, and test set so that we can train, validate, and test our model. Then we can look that our full data set now has test our model, and we can look that our full data set now has 500 rows in train, 100 in validation, 100 in test. Numbers are totally arbitrary here, just meant to showcase what we can do with this kind of model. So the first thing we're gonna do, and we're gonna do this in all of these options, or in these examples, is we want to, of course, pre-process our text. The model doesn't actually understand text, as Greg told us about, right? The model only cares about, you know, the actual numeric representation or that tokenized representation. So we're going to build this pre-processing function that's going to tokenize all of our text inputs into a format that our model expects. So you can see here we have our model inputs, we have our labels, and then we're going to return those. Our model inputs are going to be a tokenized text inputs and our labels are going to show our tokenized label text. So we have our main body of text, which is that bill that we saw, and then we have our tokenized labels, which is gonna be our summary, right? So we have tokens for our input sequence, which is very long, and then tokens for our output sequence, which is quite short. We're going to map that helper function across our whole data set. We could use batching here just to speed it up. And then we're gonna build a data collator for seek to seek models. Data collators just help us do everything a lot more efficiently and effectively using the hugging face training ecosystem. So it's just kind of a step that you're going to see often. Then we're going to remove all of the columns that have nothing to do with what our model sees. So we don't confuse the model. So we're just going to remove all of the original columns since we've now added our labels, our input IDs, as well as, you know as all of our tokenized information. So we don't have to worry about the original text anymore. We have the token representation. Next we need like an evaluation pipeline. Basically what this is gonna do is it's going to let us evaluate our model and see how it's doing so we can track some metrics. We're gonna be using the Rouge score as well as we're gonna be using the sentence tokenizer from NLTK. The basic idea here is we're just going to compute our Rouge scores across our decoded predictions and our labels. So we're gonna compare how similar those two pieces of information are. This is an important step, right? Choosing what metric that you wanna track during evaluation is critical. We need it to be able to determine how our model's performance is doing, basically. If we don't have that evaluation step, it's very difficult for us to tell how well we're doing, right? Once we have the evaluation pipeline set up, we've got our data loaded in the correct sequence. We are good to go to start training our base BERT model or BERT base model. We're going to use batch size of eight and we're going to train for eight epochs. And again, we're going use that Facebook BART base. These are just hyperparameters. You can set these however you'd like. You would need to tune them if you want a truly performant model. We're not worried too much about the performance of the model, more how we actually get this thing going and what kinds of things we can train this model to do well. We're gonna be using that seek to seek objective. So we are training on seek to seek here. The idea is that it's sequence to sequence, right? We have a long sequence to a short sequence. So that's what we want to use for our summarization task. Basically, we're going to evaluate it every epoch. We're gonna set our initial learning rate. We're gonna set our weight decay. We're gonna save the number of total models. We'll save at the end. And then we will go on to train. We're gonna pass in our training data set as well as our validation data set. Remember that these are pre-tokenized so we don't have to do any processing here. We're gonna set our data co-relator up to allow the model to train more efficiently, pass in our tokenizer, and then compute our metrics. Once that's done, we can call trainer.train, and this is going to let us train our model. As you can see, our training loss goes down, our validation loss goes down, our validation loss goes down, and our Rouge scores relatively increase. Once we're done training the model, we can push it to Hugging Face using the notebook login to pass your Hugging Face token, and then we can use the push to hub method on our trainer to push this to our hub, where we can play around with it. So we can go to our BART summarization playground, and we can get our classic Eiffel Tower summarization, right, where we put in this big long chunk of text about the Eiffel Tower, and then we see a summarization which gives us mostly the same information but well summarized. We could also use it on our specific example that we set up with the California build text, and then this is going to summarize this entire legal jargon into natural language once the model's loaded. You can check that out by clicking the link in the notebook. And that's the idea of BART. BART is really good at those kinds of sequence-to-sequence tasks, machine translation, summarization, but it's also great in terms of generation, creating marketing material, you can see here that it just excels, even with very little training at that summarization task. So we'll go back to Greg to learn about our next model, but this has been an example of BART and what we can do with it and how we might fine tune it. Yeah, awesome, Chris. What we're gonna look at next is we're gonna sort of reconsider how Word2Vec was able to represent those words for us in a really sort of meaningful way so that we could compare them to one another. But models like Word2vec that represent words well aren't doing it within context, aren't doing it so that we know exactly where all the words are relative to one another and what to pay attention to. This is where BERT shines. Because when we add context, we capture a lot more of that semantic meaning. When we think about BERT, this is a so-called auto-encoding transformer. Models, you might hear people talk about as being BERT-like. That's that sort of encoder-only setup architecture. And again, this is all about understanding the input. This is very valuable in question answering or in sentiment analysis. But as we saw in our transformer overview, we're not really able to generate an output in the same way when we don't have a decoder. So what's often needed with BERT is we need to do some additional work to really make sure it's outputting the right thing for our task. The key with BERT is that it's the bidirectional encoder. So we've said BERT enough. Let's define it. It's the bidirectional encoder. So we've said BERT enough. Let's define it. It's the bidirectional encoder representations from transformers. It should start to be making sense to you now why we call it BERT. And it's simply using that bidirectionality, that understanding of the sequence from left and from right to give us that encoding. So the data in the model we're going to use for our BERT example is the 20 newsgroup data. And we're going to fine tune BERT, similar to the way we fine tuned BART, for sentiment analysis. We're going to use the distill BERT-based uncased model. And the distill BERT is a slightly more efficient BERT model that's been distilled down to sort of a minimally sufficient size. And so what we'll do is we'll go ahead and check out our next example on how to use BERT for sentiment analysis with Chris right now. Over to you, man. Hey, yes, we're back and we're going to talk about everybody's favorite BERT. BERT's an absolute legend of a model. If you've seen it before or if you've heard of it before, you've probably heard of it a lot. There are about 100 different variations of BERT. There are about 100 different ways we can use BERT, but the way that I think is most indicative or art-typical of BERT is the classic, you know, sentiment analysis or classification. So BERT is just an encoder stack, right? It's basically super good at distilling information, but not very good at anything else. I'm joking, of course, but the idea is that the actual BERT model is very good at understanding text, actual BERT model is very good at understanding text, understanding what we're talking about, comprehending language. BERT is super good at that. Now, by itself, we're not gonna be able to do a whole lot with that, but we can train the model on different objectives, like classification, and leverage its understanding to really create a performative classifier that works very well with very little data and very little training. We're going to talk about the 20 Newsgroup dataset, which is a fantastic dataset that's just a collection of 20 different news groups. There are 20,000 documents across 20 different topics like PC hardware, hockey, science medicine. We got religion here, we got politics here. So the idea is we've got a bunch of different pieces of text and they all have different groups they're part of. If you want to read more, you can check out the links in the description. When it comes to actually training BERT, we have to do the same kind of process we did before, right? We have to determine the number of labels we want to train with. We have to determine the number of labels we want to train with. We have to create our tests and train sets. We're gonna use the TensorFlow version of training this time just to shake things up and show you how powerful the Transformers library is. If you go through the process here, basically all we're gonna do is take out the train data, take out the test data, put them into a data set. Then we're going to encode our label column, which is going to be our, you know, in the form they're in right now, all of our labels are in text form, but we want them in numeric form for when we train our model, so we can do that here. Then we're going to again create another split. This split is gonna be used for our validation. So we're once again back to a train set, a test set, and a validation set. We've got some text here that talks a little bit about what fine tuning or transfer learning is all about, so you can use this to guide your intuition as you go through the notebook. We're going to be talking about it a lot, you know, in upcoming weeks during our course, but, you know, just a primer there. We're going to be using the base model of Distilbert base uncased. So this is the Distilbert base uncased model. It's a very powerful model despite being so, so tiny. I'm gonna go ahead and share this tab and then I'll send a link so that you guys can see it for yourself. The idea is that this is a BERT model that has been distilled. The uncased here just means that it's all lowercase. That's the only thing it's saying when it says uncased. We're gonna grab our tokenizer for our BERT model, and then we're gonna do the same thing we did before, right? We have this text in our newsgroups, but we need to turn it into a format that our model can understand, so we're gonna pre-process it with our tokenizer. We're gonna map that tokenizer across our whole data set, and then we're gonna have our tokenized text. You can see here that we have our text, our label, our input IDs, and our attention mask. Just as we did before, we're gonna drop the text now that we have our label, input IDs, and attention masks. We simply do not need the text now that we have our label input IDs and attention masks. We simply do not need the text. All we care about is our input IDs, attention masks, and our label. We're then going to convert this to a TensorFlow data set, which can be done straightforwardly, basically with 2TF data set. You'll notice we are only sending over the attention mask, input IDs, and labels, since the text isn't what the model cares about. We'll then create some hyperparameters, which is going to include our epochs, the total number of training steps we're going to use, and then set up our optimizer and scheduler. Then we can use the TF automodel for sequence classification through Then we can use the TF automodel for sequence classification through hugging face Transformers library to create a TF model for sequence classification Because remember even though we're using this language model at its heart what we actually want to do is we want to append to it a Classification head so that we can actually use the outputs from BERT, which understands language so well, to work as a classifier. So the idea is we're leveraging that base BERT model. We're going to fine-tune this classifier head on top of it to leverage how powerful it is at understanding language in order to determine which groups text should be put into. We're just going to use accuracy since we have a balanced data set. You can use whatever metrics you prefer. Obviously, accuracy is not going to be a great metric if you have an imbalanced data set or you have other considerations that you're worrying about like optimizing recall or precision. We're going to go ahead and call.fit, since it's a TensorFlow model, and we can fit on our training set, and then we can use our validation set to guide us along the way. You'll notice that our training loss, our training accuracy all increase. Similarly, our validation loss and validation accuracy get better, so they improve, they don't increase. That was a misspeak, of course. Loss tends down, accuracy tends up. And you can see that we're not lagging too far behind, so it's not overfitting like crazy. We're only training for three epochs here. And then we have our test accuracy, which is very much in line with our validation accuracy above, tells us that we're doing pretty good. Once again, we're gonna push this model to the hub so that we can play with it on the hugging face hub and take a peek at what we can classify. So we can come to this tab and look at the model itself. We can compute something about I love cars, which is label 17, which is related to the car topic or auto topic in our newsgroup 20 data set. So this is the idea. We're going to leverage how powerful BERT, or the powerful understanding of language that BERT has to classify different sequences into the respective categories. And with that, I'll kick it back to Greg, who's going to talk to us about our last style of architecture, which is very important and popular. Thanks, Chris. I love cars too. Very cool, man. We're going to go ahead and now close out this idea of BERT, this encoder-only piece of the stack that we just leveraged. And we're going to move over to the decoder-only. This is the big guy. This is the one everybody wants to really dig in and understand. This decoder-only architecture has really taken the world by storm. It underlies chat GPT, and it is the auto-regressive type. Remember, that just means we're predicting the future based on the past. That's in time, but in word space, we're predicting the next word based on the previous words. The model that we're talking about is, of course, GPT. And GPT-like models are a lot of places. We're seeing lots of innovation in this space. They're very good at generative tasks. And no matter how you feel about the word chatbot, at the end of the day, a lot of these things are, in some way, shape, or form categorizable as chatbots. So we're going to actually close up today with kind of a fun example of using GPT-2 straight up. And again, this is the autoregressive GPT-like decoder. In this particular case, we can report that we do have 12 decoder blocks. We're not sure how many attention heads. It's not very easy to find that information. But this is also going to be the GPT-2 small model, because that's the default one used by Hugging Face. We're going to do a sort of song lyrics fine-tuning here. So we're going to make it really good at coming up with song lyrics. Can't wait to see what kind of song Wiz is going to have it sing for us. And just keep in mind that now we are doing this decoder side of the stack. This is chat GPT-ish. Of course, the sort of flowing in from the encoder, the input and output embeddings, that's not exactly what's going on here. We really need to draw a new diagram for the GPT style architecture on its own. But for now, we're going to go ahead and dive into a use case to see exactly where this is useful and to see exactly how easy it is to fine tune a relatively small, yet performant GPT-2 style model. Chris, back to you. Hey, Greg, thanks so much. So, okay, we've talked about encoder decoder. We've talked about encoder. Now we're talking about decoder only. This is your classic, your staple, you know, your GPT suite of models, this is plopping tokens down, one after the other, always looking behind it, never looking forward. It doesn't care what comes next. It only cares about what came before. And this is going to be an example of how we can fine tune GPT-2, which is that decoder architecture to a specific task, which in this case is going to be lyric generation. These decoder only models are great at creative writing, they're great at language modeling and text generation. You just let them rip and they can just come up with all kinds of fun text. They don't need to rely on that kind of internal representation like these other models do, nearly as much. And they just generate for days. So, we're gonna use this Genius Lyrics data set that I found in Hugging Face. It's pretty cool, just a bunch of lyrics. We're obviously gonna take a subset of it since there are a huge number of examples and we don't wanna sit here training all day. So we're gonna go ahead and just do the same thing we did before, we're gonna use 500 total rows and then we're gonna have our validation set and our test set as well. You'll notice that there's so many columns here, we only care about one column and that is lyrics. So we're going to go ahead and do our, when we do our pre-processing, we're just gonna care about this lyrics column, and we're gonna drop all this other stuff that we just simply do not care about. But first we're gonna set up our model. We're gonna use the auto-tokenizer from pre-trained on the GPT-2 model, which is gonna automatically get us the tokenizer on the GPT-2 model, which is gonna automatically get us the tokenizer for the GPT-2 suite of models. And then we are going to have a auto model for causal LM, causal LM, AKA auto regression, AKA this decoder only architecture is gonna mean that we plot tokens down in order. So we predict what token is likely to come next, and then we use some process to choose the one we want and plop it down, and then we rinse and repeat. We need to add a padding token since GBT2 Tokenizer does not have a padding token by default. This is necessary to make the Huggyface Trainer work. Automatically we are using the small version of GPT-2, which is the 12 by 12 model. So as Greg was talking about, there are 12 blocks. Each block has 12 heads. So we have 144 total attention heads, which is gonna give us quite a lot of, you know, ways that we can pay attention to different parts of our sequences. As said before, we can pay attention to different parts of our sequences. As said before, we're just going to tokenize our lyrics. We're going to map this tokenizer onto every single row in our data set, and then we are going to get rid of everything that we had before. So we're gonna get rid of all of our columns and only be left with our input IDs and our attention masks. You'll notice that we don't have labels here. We don't really need labels for this task because we can use the input sequences themselves as labels. We're going to use this helper function called group texts to basically do exactly that. We're going to group text into chunks and be able to predict these labels. After that, we can go ahead and create our new data set, which is going to be from our grouped texts. We're going to use batching just to do exactly as we had before. You know, batching just makes it go faster, and that's all we need to know. it go faster and that's all we need to know. Once we have this set up, we can use the Huggy Face trainer library, transformer trainer library again to make this happen quickly for us. We're gonna train for just 30 epochs. You could definitely train for more if you wanted to, but that's it. We're gonna do 30 just as an example. We're gonna have the evaluation strategy of every epoch. We're gonna set an initial learning rate equal to whatever you'd like really. Again, we're not focusing so much on performance here as we are what these models are good at and how we might want to train them and what kinds of tasks we can fine tune them on. We set up our weight decay, how many epochs we're gonna train for, how many versions of the model we'll save, how many, you know, when we save the model, it's gonna be every step here. We need to do eval at every step, and then we're gonna load the best performing model at the end. Then we're just gonna let it rip. You know, we set everything up and we're good to go. We wanna create a cosine scheduler. We can do that easily with the, you know, Transformers library, very powerful. We're gonna set up some configs for our text generation so that when we're evaluating our model's text generation, we know what hyperparameters we're evaluating with. After that, we can call.parameters we're evaluating with. After that we can call.train and.train is going to train the model. You'll notice that our model super overfits, right? We have our training loss plummeting and our validation loss kind of blown up through the roof. This is expected, right? We're training this on a very specific task. There is some awareness that it's going to overfit as these models are incredible learners. We can do the same thing we did before where we just pushed this bad boy to the hub. And then we can see how it works, right? We're gonna use this kind of, you know, process to generate a number of sequences. We're gonna show those sequences. We're gonna pretty heavily penalize repetition when we're generating. The model does tend to repeat itself as it's trained in a lot of songs. And a lot of songs have choruses which are fairly repetitive. And so this repetition penalty helps us avoid some of that, you know, those ruts that it gets into or just says, oh yeah, over and over again. We're gonna do our generation and then we're going to convert our generation from these kinds of unreadable token sequences to text. And then we will generate. We have such fun songs as I am a little rough with my body on the rocks. As you grow old, it's going to be an on and on. I am the way you see me. I the rocks. As you grow old, it's going to be an on and on. I am the way you see me. I just know how to get what I want. And it's not a battle. Excellent. I am the truth. You can be on fire and water. Just give me a little time. We have all kinds of songs, basically. You know, and the idea is that it's it's. You know, we're able to fine tune our model on these tasks and we get these kind of pop song sounding lyrics. And that is all we are going through today. So we've looked at those three models and we've shown examples of how to fine tune each of them and what we care about when we're fine tuning and how we need to manipulate our data depending on the task and what each model is kind of best in class at. And yes, I think that is what we're going to do. So I will kick it back to Greg and we'll wrap up. Wiz loving the songs, man. Yeah, absolutely. We need to get you singing them next time. Yeah, we'll get you back in just a sec. So to conclude, vocab really matters here. We need to understand what we're talking about when we're talking about tokens and embeddings and encodings and encoders and decoders. Self-attention here, that's the key innovation. That's the key mechanism that is allowing us to do with a really simple architecture, what took really complex architectures before, and those attention heads are allowing us to pay attention to different pieces of the world when we train these massive models at scale. BART, BERT, and GPT-style models are good for different things. And so keep that in mind as you're out there building. Although a lot of attention today is paid to GPT-style models, the other ones still do have their use cases in their place. And then remember, only models with decoders generate output. So if you use BERT, you're going to have to do a little bit of work on the back end to make sure if it's classification, you're putting the classification head on it, and so on. So with that, I hope the next time that you see this diagram and this slide, you know a little bit better. And if you want to go deeper, you may have seen that Chris's end of his notebook showed attention is all you need. We've actually built out the attention mechanism in code for you as a starting point for folks that want to learn on their own. If you'd like to learn with a cohort and start learning with us though You want to understand exactly what's going on with inside the attention mechanism? Inside the brains inside the blocks and the heads you want to actually code this up yourself from scratch GPT GPT 2 3 and so on style You want to do some RLHF too? That's what our new course is designed for. So I'm gonna pull Chris back up on stage to just answer a few questions right now. If you have any burning ones please do scan the QR code and let us know what they are. We'd love to hear from you what you're interested in for developing future content and definitely connect with both Chris and I. Chris, great question. Here from Islam. Positional encoding and self-attention both have a similar role in caring about the context of the input, right? How can I really differentiate between the two? Positional encoding and self-attention both have similar role in caring a lot about the context of the input, right? How can I really differentiate? I think that's a good question. I mean, the idea is that we use the positional encoding to allow the self-attention mechanism to care about the context of the input. So the idea is that without that positional encoding, we don't really have a way, I know we said we wouldn't talk about it, but just allow me one, without some kind of way to interpret a sequence, either through some kind of RNN or CNN, right, we don't really have a way to know about the order of the tokens or where they exist in relationship to each other. And so the idea is, you know, we need to inject that information into the context with our positional encoding, but the self-attention is the only thing that actually leverages that information in terms of the two things that you mentioned. So positional encoding injects the information, but the self-attention is the thing that consumes that information, if that makes sense. Love it, love it, love it. And yes, just in case you're wondering, all codes, all slides, while we might not post them to GitHub, because it's just a quick Google Colab today, we are going to make sure that everybody gets those. If you signed up for this event, you'll get an email. We'll also put it on the YouTube live page and put it in the chat. Question from Todd, close us out here. When did you first realize that transformer-based models were impressive and they could lead to something? The world, chat gbt, AI researchers, when? Like for you, Chris, what was that like? Well, I mean, it was it was attention is all you need. I know it's such a stupid meme, but like the idea that you could know so much about language, so in a way that didn't require you to just, you know, iterate through a sequence really expanded the way that we could consider language from a, you know, a purely computational perspective, I mean the paper and then the subsequent models that came from it were huge. I think for me personally, the kind of time where I really started to deeply care about transformers or thought to myself, I mean this architecture is clearly the best thing ever, is when the BERT style models came out. So when that BERT paper was like, this is, you know, this is gonna change the world for sure. We kind of already had the idea that it was gonna change the world, but BERT really cemented it's, you know, especially in terms of like a specific and valuable use case, right? So BERT did stuff that was valuable and it did it for a good cost and there was nothing quite like BERT. And so there you go. I think that's the, you know, the BERT style models is really what to me signaled this was more than just cool. It was like, you know, we're not coming back. Yeah. I mean, I think Chris, I think you were way ahead of the curve. I mean, you know, I don't think most people heard about it then. You know, to me, I think the narrative that I've noticed and come up with after watching this over the years is kind of GPT-2 time when that came out, people started paying attention. They started getting worried and they started being like, I don't know if we should put this kind of stuff out to the world. So that's when sort of the engineers started paying attention and really like, what's going on here? This seems to be some very powerful tooling. And, you know, a couple of years later, fast forward, obviously, chat GPT, everybody knows what time it is. It's almost a year old now, and so, you know, the world has changed since chat GPT, so there's nothing quite like that. But I would say GPT 2 really started this for all the engineers, chat GPT for everybody else. And Mert will chat about how we can integrate flight in the next community session or round table in Discord. This brings us to the end of today's event. Thank you so much, everyone, for your participation. And thank you so much to Coding Temple for partnering with us today. If this resonated with you and you'd like to dive deeper into LLM Engineering, we've got a course kicks off tomorrow. We'd love to have you and see you in class. We also have a couple more events coming soon next Wednesday November 8th. We'll be talking about fine tuning, fine tuning 101 from LLMs to embeddings and we're gonna talk about where you're gonna use fine tuning in any particular use case that is going to be kind of near the state-of-the-art edge in generative AI today. Finally, if you're a business leader and you're interested in leading generative AI transformations, we've got another workshop on November 13th on LLM leadership, how to build a competitive AI advantage for you or for your organization. So thanks for joining again, and until next time, keep building, shipping, and sharing, and we'll do the same. Thanks, everybody.