{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbr/ai-eng-bootcamp/blob/main/Transformer_Workshop_Code_Temple_Examples_of_Different_Model_Architectures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BART (Encoder-Decoder Style Model)\n",
        "\n",
        "BART, or Bidirectional AutoRegressive Transformer found in [this](https://arxiv.org/pdf/1910.13461v1.pdf) paper, is a Encoder-Decoder style model that leverages the traditional architecture found in the \"Attention is All You Need\" paper. They make a simple modification to the activation function from ReLU to GeLU.\n",
        "\n",
        "This model excels at a number of tasks, including but not limited to: Machine Translation, Summarization, Categorization of Input Sentences, and Question Answering.\n",
        "\n",
        "We'll showcase BART with a Text Summarization fine-tuning task today."
      ],
      "metadata": {
        "id": "azHKVT27Ou2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU rouge-score evaluate transformers torch accelerate -qU"
      ],
      "metadata": {
        "id": "mBNGZWasPd_3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline, set_seed\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import Seq2SeqTrainer\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "import datasets\n",
        "from datasets import load_metric, Dataset\n",
        "from datasets import DatasetDict"
      ],
      "metadata": {
        "id": "H2gaMmp_R6ub"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First up, we'll load our model and tokenizer!"
      ],
      "metadata": {
        "id": "2BesA8nEyGNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_ckpt = \"facebook/bart-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
      ],
      "metadata": {
        "id": "yzysjZC2SMv4",
        "outputId": "fd95de93-cd29-4548-c62d-274f756d541b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-88596bb3bd0f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_ckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"facebook/bart-base\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using the `billsum` dataset from Hugging Face which you can be found [here](https://huggingface.co/datasets/billsum/viewer/default).\n",
        "\n",
        "Each of the rows contains a block of text from a legal bill - and then a plain english summary."
      ],
      "metadata": {
        "id": "GDGPhEanyInj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"billsum\", split=\"ca_test\")"
      ],
      "metadata": {
        "id": "a5VuvhU-SuNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "eAh2CgR7S76U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create a train/test/eval split to train our model."
      ],
      "metadata": {
        "id": "Oo4-xXn5zTrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "total_rows = 500\n",
        "test_val_ratio = 0.2\n",
        "\n",
        "val_rows = total_rows + math.floor(total_rows * test_val_ratio)\n",
        "test_rows = val_rows + math.floor(total_rows * test_val_ratio)\n",
        "\n",
        "subset_dataset = datasets.DatasetDict(\n",
        "    {\n",
        "        \"train\" : Dataset.from_dict(dataset[:total_rows]),\n",
        "        \"validation\" : Dataset.from_dict(dataset[total_rows:val_rows]),\n",
        "        \"test\" : Dataset.from_dict(dataset[val_rows:test_rows])\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "LaYFkV-fUPES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_dataset"
      ],
      "metadata": {
        "id": "qoCyQ5kFVD5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to preprocess our data into tokenized representations.\n",
        "\n",
        "These tokenized representations are what the model will actually see during training!"
      ],
      "metadata": {
        "id": "AKUVWF3kzXfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 1024\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        examples[\"summary\"], max_length=max_target_length, truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "GURcXP3PVRet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = subset_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "q_P5M6L3VqQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "CmmDdwijVxwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can remove all unessecary text columns."
      ],
      "metadata": {
        "id": "bHtuBOMnze4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns(dataset.column_names)"
      ],
      "metadata": {
        "id": "mhT1bO5DVzl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up an evaluation pipeline that will help us monitor our model's performance!"
      ],
      "metadata": {
        "id": "wFQU8LhWzpfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk -qU"
      ],
      "metadata": {
        "id": "Fzogwrm_WKLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "7XcbqPpuWJm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "rouge_score = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "Qups_83wWmvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Decode generated summaries into text\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    # Decode reference summaries into text\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # ROUGE expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    result = rouge_score.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "bBjD0ur9WDLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can finally get to training!"
      ],
      "metadata": {
        "id": "IXNymC6Rztvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to train with the `Seq2Seq` objective as we're trying to convert one long sequence into a shorter sequence."
      ],
      "metadata": {
        "id": "7dC5oz2Fzhqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "num_train_epochs = 8\n",
        "# Show the training loss with every epoch\n",
        "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
        "model_name = model_ckpt\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=f\"{model_name}-finetuned-CNN-DailyNews\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5.6e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    predict_with_generate=True,\n",
        "    logging_steps=logging_steps)"
      ],
      "metadata": {
        "id": "BPx867OnV4LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,)"
      ],
      "metadata": {
        "id": "vc6s3qeNV-Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "NaS53mJ0WStF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can push our model to the Hugging Face Hub to test and play around with!"
      ],
      "metadata": {
        "id": "9234-1rx0FAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub -qU"
      ],
      "metadata": {
        "id": "IALVVAGJeiLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "9OFw_2lpemZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(\"ai-maker-space/Transformers-Workshop-BART-Summarization\")"
      ],
      "metadata": {
        "id": "iU6uTvkCezgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check out the final model [here](https://huggingface.co/ai-maker-space/Transformers-Workshop-BART-Summarization?text=SECTION+1.+ENVIRONMENTAL+INFRASTRUCTURE.+%28a%29+Jackson+County%2C+Mississippi.--Section+219+of+the+Water+Resources+Development+Act+of+1992+%28106+Stat.+4835%3B+110+Stat.+3757%29+is+amended--+%281%29+in+subsection+%28c%29%2C+by+striking+paragraph+%285%29+and+inserting+the+following%3A+%60%60%285%29+Jackson+county%2C+mississippi.--Provision+of+an+alternative+water+supply+and+a+project+for+the+elimination+or+control+of+combined+sewer+overflows+for+Jackson+County%2C+Mississippi.%27%27%3B+and+%282%29+in+subsection+%28e%29%281%29%2C+by+striking+%60%60%2410%2C000%2C000%27%27+and+inserting+%60%60%2420%2C000%2C000%27%27.+%28b%29+Manchester%2C+New+Hampshire.--Section+219%28e%29%283%29+of+the+Water+Resources+Development+Act+of+1992+%28106+Stat.+4835%3B+110+Stat.+3757%29+is+amended+by+striking+%60%60%2410%2C000%2C000%27%27+and+inserting+%60%60%2420%2C000%2C000%27%27.+%28c%29+Atlanta%2C+Georgia.--Section+219%28f%29%281%29+of+the+Water+Resources+Development+Act+of+1992+%28106+Stat.+4835%3B+113+Stat.+335%29+is+amended+by+striking+%60%60%2425%2C000%2C000+for%27%27.+%28d%29+Paterson%2C+Passaic+County%2C+and+Passaic+Valley%2C+New+Jersey.--+Section+219%28f%29%282%29+of+the+Water+Resources+Development+Act+of+1992+%28106+Stat.+4835%3B+113+Stat.+335%29+is+amended+by+striking+%60%60%2420%2C000%2C000+for%27%27.+%28e%29+Elizabeth+and+North+Hudson%2C+New+Jersey.--Section+219%28f%29+of+the+Water+Resources+Development+Act+of+1992+%28106+Stat.+4835%3B+113+Stat.+335%29+is+amended--+%281%29+in+paragraph+%2833%29%2C+by+striking+%60%60%2420%2C000%2C000%27%27+and+inserting+%60%60%2410%2C000%2C000%27%27%3B+and+%282%29+in+paragraph+%2834%29--+%28A%29+by+striking+%60%60%2410%2C000%2C000%27%27+and+inserting+%60%60%2420%2C000%2C000%27%27%3B+and+%28B%29+by+striking+%60%60in+the+city+of+North+Hudson%27%27+and+inserting+%60%60for+the+North+Hudson+Sewerage+Authority%27%27.+SEC.+2.+UPPER+MISSISSIPPI+RIVER+ENVIRONMENTAL+MANAGEMENT+PROGRAM.+Section+1103%28e%29%285%29+of+the+Water+Resources+Development+Act+of+1986+%2833+U.S.C.+652%28e%29%285%29%29+%28as+amended+by+section+509%28c%29%283%29+of+the+Water+Resources+Development+Act+of+1999+%28113+Stat.+340%29%29+is+amended+by+striking+%60%60paragraph+%281%29%28A%29%28i%29%27%27+and+inserting+%60%60paragraph+%281%29%28B%29%27%27.+SEC.+3.+DELAWARE+RIVER%2C+PENNSYLVANIA+AND+DELAWARE.+Section+346+of+the+Water+Resources+Development+Act+of+1999+%28113+Stat.+309%29+is+amended+by+striking+%60%60economically+acceptable%27%27+and+inserting+%60%60environmentally+acceptable%27%27.+SEC.+4.+PROJECT+REAUTHORIZATIONS.+Section+364+of+the+Water+Resources+Development+Act+of+1999+%28113+Stat.+313%29+is+amended--+%281%29+by+striking+%60%60Each%27%27+and+all+that+follows+through+the+colon+and+inserting+the+following%3A+%60%60Each+of+the+following+projects+is+authorized+to+be+carried+out+by+the+Secretary%2C+and+no+construction+on+any+such+project+may+be+initiated+until+the+Secretary+determines+that+the+project+is+technically+sound%2C+environmentally+acceptable%2C+and+economically+justified%3A%27%27%3B+%282%29+by+striking+paragraph+%281%29%3B+and+%283%29+by+redesignating+paragraphs+%282%29+through+%286%29+as+paragraphs+%281%29+through+%285%29%2C+respectively.+SEC.+5.+SHORE+PROTECTION.+Section+103%28d%29%282%29%28A%29+of+the+Water+Resources+Development+Act+of+1986+%2833+U.S.C.+2213%28d%29%282%29%28A%29%29+%28as+amended+by+section+215%28a%29%282%29+of+the+Water+Resources+Development+Act+of+1999+%28113+Stat.+292%29%29+is+amended+by+striking+%60%60or+for+which+a+feasibility+study+is+completed+after+that+date%2C%27%27+and+inserting+%60%60except+for+a+project+for+which+a+District+Engineer%27s+Report+is+completed+by+that+date%2C%27%27.+SEC.+6.+COMITE+RIVER%2C+LOUISIANA.+Section+371+of+the+Water+Resources+Development+Act+of+1999+%28113+Stat.+321%29+is+amended--+%281%29+by+inserting+%60%60%28a%29+In+General.--%27%27+before+%60%60The%27%27%3B+and+%282%29+by+adding+at+the+end+the+following%3A+%60%60%28b%29+Crediting+of+Reduction+in+Non-Federal+Share.--The+project+cooperation+agreement+for+the+Comite+River+Diversion+Project+shall+include+a+provision+that+specifies+that+any+reduction+in+the+non-+Federal+share+that+results+from+the+modification+under+subsection+%28a%29+shall+be+credited+toward+the+share+of+project+costs+to+be+paid+by+the+Amite+River+Basin+Drainage+and+Water+Conservation+District.%27%27.+SEC.+7.+CHESAPEAKE+CITY%2C+MARYLAND.+Section+535%28b%29+of+the+Water+Resources+Development+Act+of+1999+%28113+Stat.+349%29+is+amended+by+striking+%60%60the+city+of+Chesapeake%27%27+each+place+it+appears+and+inserting+%60%60Chesapeake+City%27%27.+SEC.+8.+CONTINUATION+OF+SUBMISSION+OF+CERTAIN+REPORTS+BY+THE+SECRETARY+OF+THE+ARMY.+%28a%29+Recommendations+of+Inland+Waterways+Users+Board.--Section+302%28b%29+of+the+Water+Resources+Development+Act+of+1986+%2833+U.S.C.+2251%28b%29%29+is+amended+in+the+last+sentence+by+striking+%60%60The%27%27+and+inserting+%60%60Notwithstanding+section+3003+of+Public+Law+104-66+%2831+U.S.C.+1113+note%3B+109+Stat.+734%29%2C+the%27%27.+%28b%29+List+of+Authorized+but+Unfunded+Studies.--Section+710%28a%29+of+the+Water+Resources+Development+Act+of+1986+%2833+U.S.C.+2264%28a%29%29+is+amended+in+the+first+sentence+by+striking+%60%60Not%27%27+and+inserting+%60%60Notwithstanding+section+3003+of+Public+Law+104-66+%2831+U.S.C.+1113+note%3B+109+Stat.+734%29%2C+not%27%27.+%28c%29+Reports+on+Participation+of+Minority+Groups+and+Minority-Owned+Firms+in+Mississippi+River-Gulf+Outlet+Feature.--Section+844%28b%29+of+the+Water+Resources+Development+Act+of+1986+%28100+Stat.+4177%29+is+amended+in+the+second+sentence+by+striking+%60%60The%27%27+and+inserting+%60%60Notwithstanding+section+3003+of+Public+Law+104-66+%2831+U.S.C.+1113+note%3B+109+Stat.+734%29%2C+the%27%27.+%28d%29+List+of+Authorized+but+Unfunded+Projects.--Section+1001%28b%29%282%29+of+the+Water+Resources+Development+Act+of+1986+%2833+U.S.C.+579a%28b%29%282%29%29+is+amended+in+the+first+sentence+by+striking+%60%60Every%27%27+and+inserting+%60%60Notwithstanding+section+3003+of+Public+Law+104-66+%2831+U.S.C.+1113+note%3B+109+Stat.+734%29%2C+every%27%27.+SEC.+9.+AUTHORIZATIONS+FOR+PROGRAM+PREVIOUSLY+AND+CURRENTLY+FUNDED.+%28a%29+Program+Authorization.--The+program+described+in+subsection+%28c%29+is+hereby+authorized.+%28b%29+Authorization+of+Appropriations.--Funds+are+hereby+authorized+to+be+appropriated+for+the+Department+of+Transportation+for+the+program+authorized+in+subsection+%28a%29+in+amounts+as+follows%3A+%281%29+Fiscal+year+2000.--For+fiscal+year+2000%2C+%2410%2C000%2C000.+%282%29+Fiscal+year+2001.--For+fiscal+year+2001%2C+%2410%2C000%2C000.+%283%29+Fiscal+year+2002.--For+fiscal+year+2002%2C+%247%2C000%2C000.+%28c%29+Applicability.--The+program+referred+to+in+subsection+%28a%29+is+the+program+for+which+funds+appropriated+in+title+I+of+Public+Law+106-+69+under+the+heading+%60%60FEDERAL+RAILROAD+ADMINISTRATION%27%27+are+available+for+obligation+upon+the+enactment+of+legislation+authorizing+the+program.+Speaker+of+the+House+of+Representatives.+Vice+President+of+the+United+States+and+President+of+the+Senate.)!"
      ],
      "metadata": {
        "id": "1ZkDvhlTkKYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT (Encoder Only Architecture)\n",
        "\n",
        "We'll be using BERT (found in [this paper]()) as our example of an Encoder-only transformer model.\n",
        "\n",
        "BERT-style models excel at Sentiment Analysis, Question Answering, Text Prediction, and other language comprehension tasks."
      ],
      "metadata": {
        "id": "ehdiasWZZgRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [20 Newsgroups dataset](http://qwone.com/~jason/20Newsgroups/) is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.\n",
        "\n",
        "[Here are some details](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset) about the dataset from Scikit Learn!\n",
        "\n",
        "Let's load the data and get it into a usable format!"
      ],
      "metadata": {
        "id": "ZGxI5iJ30O7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "train = fetch_20newsgroups(subset = \"train\")\n",
        "test = fetch_20newsgroups(subset = \"test\")"
      ],
      "metadata": {
        "id": "xeVQ9ZWHcBPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a look around the data! Take note of things like data types, column names, and everything else!\n",
        "\n",
        "Also take note of how many classes there are in our labels and mark it down in the cell below!"
      ],
      "metadata": {
        "id": "nB1v6tfX0dPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_LABELS = 20"
      ],
      "metadata": {
        "id": "gyw6RAXAcFav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "X, y = pd.Series(train[\"data\"]), pd.Series(train[\"target\"])\n",
        "X_test, y_test = pd.Series(test[\"data\"]), pd.Series(test[\"target\"])"
      ],
      "metadata": {
        "id": "av_TRI8XcGXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our raw data - let's convert that into some pd.Series objects using pandas!"
      ],
      "metadata": {
        "id": "MBhA9sKy0e8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's get the Hugging Face datasets ([documentation here](https://huggingface.co/docs/datasets/index)) library so we can convert our data into a more usable format."
      ],
      "metadata": {
        "id": "UAYQx_R20gdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame({\n",
        "    \"text\" : X,\n",
        "    \"label\" : y\n",
        "})\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    \"text\" : X_test,\n",
        "    \"label\" : y_test\n",
        "})\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "test_ds = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "54D73GDacJwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can cast our label columns to datasets.features.ClassLabel objects using class_encode_column! (documentation [here](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.class_encode_column))"
      ],
      "metadata": {
        "id": "pArlUd4N02C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.class_encode_column(\"label\")\n",
        "test_ds = test_ds.class_encode_column(\"label\")"
      ],
      "metadata": {
        "id": "le_y76zycfFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll want to first convert our separate series objects into a combined pd.DataFrame with columns: text and label, for our Xs and ys respectively.\n",
        "\n",
        "After that, it's as easy as loading the pd.DataFrame into a Dataset object!"
      ],
      "metadata": {
        "id": "piH_sguD0mqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dsd = train_ds.train_test_split(test_size=0.1, seed=19, stratify_by_column=\"label\")"
      ],
      "metadata": {
        "id": "W-Aj07yQcgRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dsd['validation'] = data_dsd['test']\n",
        "data_dsd['test'] = test_ds"
      ],
      "metadata": {
        "id": "q5J2pEDocjUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dsd"
      ],
      "metadata": {
        "id": "50tDuZd_ckgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, we'll be fine-tuning a simple classifier for our above data using Hugging Face's transformers library (documentation [here](https://huggingface.co/docs/accelerate/index)) as well as the accelerate library. (documentation [here](https://huggingface.co/docs/transformers/index))\n",
        "\n",
        "Before we dive in, let's take a pit stop to discuss what fine-tuning is - in broad strokes.\n",
        "\n",
        "- Fine-tuning is a transfer learning approach where a pre-trained machine learning model is further trained on new data, often to specialize in a certain task. This process can involve training the entire network or only a subset of it, with untrained layers remaining 'frozen'.\n",
        "\n",
        "- This method is prevalent in Natural Language Processing (NLP) and convolutional neural networks. In the latter, early layers capturing lower-level features are typically frozen, while in NLP, large models like GPT-2 are fine-tuned for specific tasks, improving their performance. However, full fine-tuning can be computationally costly and might lead to overfitting.\n",
        "\n",
        "- Although fine-tuning is commonly executed through supervised learning, it can also be done using weak supervision or reinforcement learning. For instance, language models like ChatGPT and Sparrow are fine-tuned using reinforcement learning from human feedback.\n",
        "\n",
        "Okay, so now that we have had a brief overview of what fine-tuning actually is - let's set ourselves up to do some!"
      ],
      "metadata": {
        "id": "hrkzy0o11IaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_id = \"distilbert-base-uncased\""
      ],
      "metadata": {
        "id": "-tIlWC_CcnDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_id)"
      ],
      "metadata": {
        "id": "kvLt4xCEcqlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 256\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=MAX_LEN)"
      ],
      "metadata": {
        "id": "CI_8C9Msct2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = data_dsd.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "ZZ2s1SVHcvvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text"
      ],
      "metadata": {
        "id": "xAtW5N_jcxUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "X-gpgmR6cyWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll want to include the attention_mask, input_ids, and label for each set - as well as shuffling the training set."
      ],
      "metadata": {
        "id": "0HXJtWuq1ZwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "tf_train_set = tokenized_text[\"train\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_validation_set = tokenized_text[\"validation\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\",\"label\"],\n",
        "    shuffle=False,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        "    )\n",
        "\n",
        "tf_test_set = tokenized_text[\"test\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\",\"label\"],\n",
        "    shuffle=False,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        "    )"
      ],
      "metadata": {
        "id": "6r2SmyvOcz17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "EPOCHS = 3\n",
        "batches_per_epoch = len(tokenized_text[\"train\"]) // BATCH_SIZE\n",
        "total_train_steps = int(batches_per_epoch * EPOCHS)\n",
        "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
      ],
      "metadata": {
        "id": "-xOhSEUIc1_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "my_bert = TFAutoModelForSequenceClassification.from_pretrained(bert_model_id, num_labels=NUM_LABELS)"
      ],
      "metadata": {
        "id": "Lb1ONJK0c5Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use the naive example of accuracy for this notebook - but feel free to use whatever metric you believe will work best."
      ],
      "metadata": {
        "id": "_HjTKfPO1d4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_bert.compile(optimizer=optimizer,  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "pBiiqEdpc7qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "my_bert.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)"
      ],
      "metadata": {
        "id": "i68PvOnjc-Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_loss, bert_acc = my_bert.evaluate(tf_test_set)"
      ],
      "metadata": {
        "id": "N4SwH3SVdABl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGINGFACE_ACCT_NAME = \"ai-maker-space\"\n",
        "MODEL_NAME = \"Transformers-Workshop-BERT-NewsGroupClassification\""
      ],
      "metadata": {
        "id": "2GTdrtvEj5Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_bert.push_to_hub(f\"{HUGGINGFACE_ACCT_NAME}/{MODEL_NAME}\")\n",
        "tokenizer.push_to_hub(f\"{HUGGINGFACE_ACCT_NAME}/{MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "7_mRz6Tmj3GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-2 (Decoder-only Architecture)\n",
        "\n",
        "Next up, and perhaps more importantly, we have our GPT-style models. These models are built from decoder-only architecture and work in an autoregressive fashion. Essentially, these models generate tokens one-by-one in sequence based on the tokens that precede it.\n",
        "\n",
        "You can read more about GPT-2 in [this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "\n",
        "Decoder-only Architectures excel at text generation, language modeling, and creative writing.\n",
        "\n",
        "We're going to spending a lot of time on this style architecture in our course - so we'll be zooming through this section!"
      ],
      "metadata": {
        "id": "fbZF4mtIdIQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be leveraging a lyric dataset to fine-tune our GPT-2-small model, you can find the dataset [here]()"
      ],
      "metadata": {
        "id": "-rR2SOmF44gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lyric_dataset = load_dataset(\"brunokreiner/genius-lyrics\")"
      ],
      "metadata": {
        "id": "bloKdatXeUwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyric_dataset"
      ],
      "metadata": {
        "id": "wOf9c9idmHeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "total_rows = 500\n",
        "test_val_ratio = 0.2\n",
        "\n",
        "val_rows = total_rows + math.floor(total_rows * test_val_ratio)\n",
        "test_rows = val_rows + math.floor(total_rows * test_val_ratio)\n",
        "\n",
        "subset_dataset = datasets.DatasetDict(\n",
        "    {\n",
        "        \"train\" : Dataset.from_dict(lyric_dataset[\"train\"][:total_rows]),\n",
        "        \"validation\" : Dataset.from_dict(lyric_dataset[\"train\"][total_rows:val_rows]),\n",
        "        \"test\" : Dataset.from_dict(lyric_dataset[\"train\"][val_rows:test_rows])\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "aprkYkFPmK8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_dataset"
      ],
      "metadata": {
        "id": "5h-wZb2DmeGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "\n",
        "gpt_model_id = \"gpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(gpt_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(gpt_model_id)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ],
      "metadata": {
        "id": "6SSp3IoBsMBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"lyrics\"])"
      ],
      "metadata": {
        "id": "2wgKCPPasfvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = subset_dataset.map(tokenize_function, batched=True, num_proc=1)"
      ],
      "metadata": {
        "id": "GPYZ_-H0sbMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns(lyric_dataset[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "4_r451GmsrVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "vxWahqtXs7R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "K1HTZ7i9tJRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = int(tokenizer.model_max_length / 4)\n",
        "\n",
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=1,\n",
        ")"
      ],
      "metadata": {
        "id": "HTZrewNetFg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"lyric-gpt\"\n",
        "\n",
        "num_train_epochs = 30\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    f\"output/{model_name}\",\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=1.00e-4,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_total_limit=10,\n",
        "    save_strategy='epoch',\n",
        "    save_steps=1,\n",
        "    report_to=None,\n",
        "    logging_steps=5,\n",
        "    do_eval=True,\n",
        "    eval_steps=1,\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "yozsin57tP6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"],\n",
        "    eval_dataset=lm_datasets[\"validation\"]\n",
        ")"
      ],
      "metadata": {
        "id": "ZdOeFleNtokI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_cosine_schedule_with_warmup\n",
        "train_dataloader = trainer.get_train_dataloader()\n",
        "num_train_steps = len(train_dataloader)\n",
        "trainer.create_optimizer_and_scheduler(num_train_steps)\n",
        "trainer.lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "      trainer.optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=num_train_steps\n",
        ")\n",
        "\n",
        "trainer.model.config.task_specific_params['text-generation'] = {\n",
        "                    'do_sample': True,\n",
        "                    'min_length': 100,\n",
        "                    'max_length': 200,\n",
        "                    'temperature': 1.,\n",
        "                    'top_p': 0.95,\n",
        "                    }"
      ],
      "metadata": {
        "id": "SdS6z8g_ts4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "data = trainer.train()"
      ],
      "metadata": {
        "id": "zOncXaIZty9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've trained our model - let's upload it to the Hugging Face Hub!"
      ],
      "metadata": {
        "id": "HiKkczSN1q8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(\"ai-maker-space/Transformers-Workshop-GPT-Generation\")"
      ],
      "metadata": {
        "id": "qeVz8J591uLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"ai-maker-space/Transformers-Workshop-GPT-Generation\")"
      ],
      "metadata": {
        "id": "WDxOPDWU17gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find the model [here](https://huggingface.co/ai-maker-space/Transformers-Workshop-GPT-Generation?text=I+am)!"
      ],
      "metadata": {
        "id": "mDogLXLc5EAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the generation works!"
      ],
      "metadata": {
        "id": "Su2S3HhQ2E2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = \"I am\"\n",
        "num_sequences =  5\n",
        "min_length =  100\n",
        "max_length =   160\n",
        "temperature = 1\n",
        "top_p = 0.95\n",
        "top_k = 50\n",
        "repetition_penalty =  1.4\n",
        "\n",
        "encoded_prompt = tokenizer(start, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
        "encoded_prompt = encoded_prompt.to(trainer.model.device)\n",
        "output_sequences = trainer.model.generate(\n",
        "                        input_ids=encoded_prompt,\n",
        "                        max_length=max_length,\n",
        "                        min_length=min_length,\n",
        "                        temperature=float(temperature),\n",
        "                        top_p=float(top_p),\n",
        "                        top_k=int(top_k),\n",
        "                        do_sample=True,\n",
        "                        repetition_penalty=repetition_penalty,\n",
        "                        num_return_sequences=num_sequences\n",
        "                        )\n",
        "\n",
        "generated_sequences = []\n",
        "\n",
        "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
        "        generated_sequences.append(text.strip())\n",
        "\n",
        "for generation in generated_sequences:\n",
        "  print(generation)"
      ],
      "metadata": {
        "id": "lRLReb-82DuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention is All You Need, Right?\n",
        "\n",
        "We'll begin by looking at how the basic Transformer Block is set up using PyTorch - and to do that, we'll start with our dependencies!\n",
        "\n",
        "We'll start with the classic image of the Transformer from the classic paper [\"Attention is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf).\n",
        "\n",
        "![img](https://i.imgur.com/4pA8cS6.png)"
      ],
      "metadata": {
        "id": "weriET9wMvPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention\n",
        "\n",
        "The first step to creating the transformer is straightforward enough: We need to code up that Attention mechanism!\n",
        "\n",
        "We need two components to make this happen:\n",
        "\n",
        "1. Scaled Dot-Product Attention\n",
        "2. Multi-Head Attention\n",
        "\n",
        "Let's look at their respective images from the paper!\n",
        "\n",
        "![img](https://i.imgur.com/1Sp9EXp.png)\n",
        "\n",
        "The basic idea is as follows:\n",
        "\n",
        "We allow different Attention Heads to attend to different parts of the sequence with different representation subspaces.\n",
        "\n",
        "All those words to say that each of our Attention Heads will care about different things throughout the course of training - as the old adage goes: Many heads are better than one!"
      ],
      "metadata": {
        "id": "xQbbUtUnLugP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA3XHCYBMpMv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create our MultiHeadAttention Module!"
      ],
      "metadata": {
        "id": "St6HLFStKhV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Input Dimension of Model\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Number of Heads (h)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Q Linear Layer\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # K Linear Layer\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # V Linear Layer\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output Linear Layer\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    ### Left Side of the Above Image\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    ### Right Side of the Above Image\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ],
      "metadata": {
        "id": "3SqrOPZBKR0r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}