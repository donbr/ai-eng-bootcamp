videoId wd7TZ4w1mSw
Part 1
Hi, this is Lance from Langchain. We're starting a new series called RAG from scratch. That's going to walk through some of the basic principles for RAG and kind of build up to advanced topics. So one of the main motivations for RAG is simply that LLMs haven't seen all of the data that you might care about. So like private data or very recent data would not be included in the pre-training runs for these LLMs. And you can see here on the graph on the x-axis that the number of tokens that they're pre-trained on, which is of course very large, but of course it's still always going to be limited relative to private data that you care about or for example recent data. But there's another interesting consideration is that LLMs have context windows that are actually getting increasingly large. So you know going from like thousands of tokens to many thousands of tokens which represents you know dozens of pages up to hundreds of pages. We can fit information into them from external sources. And a way to think about this is LLMs are kind of a kernel of a new kind of operating system, and connecting them to external data is kind of a very central capability in the development of this kind of new emergent operating system. So retrieval augmented generation or RAG is a very popular kind of general paradigm for doing this which typically involves three stages. So the first stage is indexing some external documents such that it can be easily retrieved based on an input query. So for example we ask a question, we retrieve documents that are relevant to that question, we retrieve documents that are relevant to that question, we feed those documents into an LLM in the final generation stage to produce an answer that's grounded in those retrieved documents. Now we're starting from scratch, but we're going to kind of build up to this broader view of RAG. You can see here there's a lot of interesting methods and tricks that fan out from those three basic components of indexing, retrieval, and generation. And future videos are actually going to walk through those in detail. We're going to try to keep each video pretty short, like five minutes. But we're going to spend a lot of time on some of those more advanced topics. First, over the next three videos, I'll just be laying out the very basic kind of ideas behind indexing, retrieval, and generation, and then we'll kind of build beyond that into those more advanced themes. And now I want to show just a quick code walkthrough because we want to make these videos also a little bit interactive. So right here, and this repo will be shared, it's public. I have a notebook open and I've just basically installed a few packages. And I've set a few environment variables for my Langsmith keys, which I personally do recommend. It's really useful for tracing observability, particularly when you're building RAG pipelines. So what I'm gonna show here is the code for our RAG quick start, which is linked here. And I'm gonna run this, but I'm then gonna kinda walk through everything that's going on. So actually, if we think back to our diagram, all we're doing here is we're loading documents. In this case, I'm loading a blog post. We're then splitting them. And we'll talk about that in future short videos on why splitting is important, but just for now, recognize we're splitting them or setting a chunk size of 1,000 characters. So we're splitting up our documents. Every split is embedded and indexed into this vector store. So we said we picked open-air embeddings. We're using Chrome as our vector storage, runs locally. And now we'd find this retriever. We then have defined a prompt for rag. We've defined our LLM. We've done some minor document processing. We set up this chain which will basically take our input question, run our retriever to fetch relevant documents, put the retrieved documents, and our question into our prompt, pass it to the LLM, format the output as a string, and we can see here's our output. Now we can open up Langsmith and we can actually see how this ran. So here was our question and here's our output, and we can actually look. Here's our retriever. Here's our retrieved documents. That's pretty nice. And ultimately here was the prompt that we actually passed into the LLM. You're an assistant for QA tasks, use the following pieces of retrieve content to answer the question. Here's our question. And then here's all the content. This we retrieved. And that drills in our answer. So this just gives a very general overview of how RAG works. And in future short videos we're gonna like break down each of these pieces in a lot more detail. Thanks.

videoId bjb_EMsTDKI
Part 2
Hi, this is Lance from Langchain. This is the second video in our series, Rack from Scratch, focused on indexing. So in the past video, you saw the main kind of overall components of rag pipelines, indexing, retrieval, and generation. And here we're going to kind of deep dive on indexing and give just a quick overview of it. So the first aspect of indexing is we have some external documents that we actually want to look at. deep dive on indexing and give just a quick overview of it. So the first aspect of indexing is we have some external documents that we actually want to load and put into what we're trying to call a retriever. And the goal of this retriever is simply given an input question, I want to fish out documents that are related to my question in some way. Now the way to establish that relationship or relevance or similarity is typically done using some kind of numerical representation of documents. And the reason is that it's very easy to compare vectors, for example, with numbers relative to, you know, just free-form text. And so a lot of approaches have been developed over the years to take text documents and compress them down into a numerical representation that then can be very easily searched. Now, there's a few ways to do that. So Google and others came up with many interesting statistical methods where you take a document, you look at the frequency of words, and you build what they call sparse vectors such that the vector locations are a large vocabulary of possible words. Each value represents the number of occurrences of that particular word. And it's sparse because there's, of course, many zeros. It's a very large vocabulary relative to what's present in the document. And there's very good search methods over this type of numerical representation. Now, a bit more recently, embedding methods that are machine learned. So you take a document and you build a compressed, fixed-length representation of that document have been developed with correspondingly very strong search methods over embeddings. correspondingly very strong search methods over embeddings. So the intuition here is that we take documents and we typically split them because embedding models actually have limited context windows. So, you know, on the order of maybe 512 tokens up to 8,000 tokens or beyond, but they're not infinitely large. So documents are split, and each document is compressed into a vector. And that vector captures a semantic meaning of the document itself. The vectors are indexed. Questions can be embedded in exactly the same way. And then a numerical kind of comparison in some form, using very different types of methods, can be performed on these vectors to fish out relevant documents relative to my question. And let's just do a quick code walkthrough on some of these points. So I have my notebook here, I've installed here. Now I've set a few API keys for Latinx Smith, which are very useful for tracing, which we'll see shortly. Previously, I walked through this kind of quick start that just showed overall how to lay out these RAC pipelines. And here what I'll do is I'll deep dive a little bit more on indexing. And I'm going to take a question and a document. And first, I'm just going to compute the number of tokens in, for example, the question. And this is interesting because embedding models in LLMs more generally operate on tokens. And so it's kind of nice to understand how large the documents are that I'm trying to feed in. In this case, it's obviously a very small, in this case, question. Now, I'm going to specify OpenEye embeddings. I specify an embedding model here. And I just say embed, embed query. I can pass my question and my document. And what you can see here is that runs. And this is mapped to now a vector of length 1536. And that fixed length vector representation will be computed for both documents, and really for any documents. So you're always kind of computing this fixed length vector that encodes the semantics of the text that you've passed. Now I can do things like cosine similarity to compare them. And as we'll see here, I can load some documents. This is just like we saw previously. I can split them and I can index them here, just like we did before. But we can see under the hood, really what we're doing is we're taking each split, we're embedding it using open-eye embeddings into this kind of vector representation. And then that's stored with a link to the raw document itself in our vector store. And next we'll see how to actually do retrieval using this vector store.

videoId LxNVgdIz9sU
Part 3
Hi, this is Lance from Langchain, and this is the third video in our series, RAG from Scratch, building up a lot of the motivations for RAG from the very basic components. So, we're going to be talking about retrieval today. In the last two short videos, I outlined indexing and gave kind of an overview of this flow, which starts with indexing our documents, retrieval of documents relevant to our question, and then generation of answers based on the retrieved documents. And so we saw that the indexing process basically makes documents easy to retrieve, and it goes through a flow that basically looks like you take our documents, you split them in some way into these smaller chunks that can be easily embedded. Those embeddings are then numerical representations of those documents that are easily searchable and they're stored in index. When given a question that's also embedded, the index performs a similarity search and return splits that are relevant to the question. Now, if we dig a little bit more under the hood, we can think about it like this. If we take a document and embed it, let's imagine that embedding just had three dimensions. So each document is projected into some point in this 3D space. Now, the point is that the the location space is determined by the semantic meaning or content in that document. So to follow that then, documents in similar locations in space contain similar semantic information. And this very simple idea is really the cornerstone for a lot of search and retrieval methods that you'll see with modern vector stores. So in particular, we take our documents, we embed them into this, in this case, a toy 3D space, we take our question, do the same. We can then do a search, like a local neighborhood search, you can think about in this 3D space around our question to say, hey, what documents are nearby? And these nearby neighbors are then retrieved because they have similar semantics relative to our question. And that's really what's going on here. So again, we took our documents, we split them, we embed them, and now they exist in this high dimensional space. We've taken our question, embedded it, projected in that same space. And we just do a search around the question for nearby documents and grab ones that are close. And we can pick some number. We can say we want one or two or three or n documents close to my question in this embedding space. And there's a lot of really interesting methods that implement this very effectively. I link one here. And there's a lot of really interesting methods that implement this very effectively. I link one here. And we have a lot of really nice integrations to play with this general idea. So many different embedding models, many different indexes, lots of document loaders, and lots of splitters that can be kind of recombined to test different ways of doing this kind of indexing or retrieval. recombined to test different ways of doing this kind of indexing or retrieval. So now I'll show a bit of a code walkthrough. So here we defined, we kind of walked through this previously. This is our notebook. We've installed a few packages. We set a few environment variables using Langsmith. And we showed this previously. This is an overview showing how to run RAG, like kind of end to end. In the last short talk, we went through indexing. And what I'm going to do very simply is I'm just going to reload our documents. So now I have our documents. I'm going to resplit them. And we saw before how we can build our index. Now here, let's actually do the same thing, but in the slides we actually showed kind of that notion of search in that 3D space. And a nice parameter to think about in building your, your retriever is K. So K tells you the number of nearby neighbors to fetch when you do that retrieval process. And we talked about, you know, in that 3D space, So K tells you the number of nearby neighbors to fetch when you do that retrieval process. And we talked about, you know, in that 3D space, do I want one nearby neighbor or two or three? So here we can specify K equals one, for example. Now we're building our index. So we're taking every split, embedding it, storing it. Now what's nice is I ask a question, what is task decomposition? This is related to the blog post. And I'm going to run get relevant documents. So I run that. And now how many documents do I get back? I get one as expected based upon k equals one. So this retrieved document should be related to my question. Now I can go to Langsmith and we can open it up and we can look at our retriever and we can see here was our question. Here's the one document we got back. And okay, so that makes sense. This document pertains to task decomposition in particular, and it kind of lays out a number of different approaches that can be used to do that. This all kind of makes sense. And this shows kind of in practice how you can implement this, this kind of KNN or K nearest neighbor search really easily just using a few lines of code. And next, we're going to talk about generation. Thanks.

videoId Vw52xyyFsB8
Part 4
Hi, this is Lance from LangChain. This is the fourth short video in our Rag from Scratch series that's going to be focused on generation. Now, in the past few videos, we walked through the general flow for kind of basic rag, starting with indexing, fall by retrieval, then generation of an answer based upon the documents that we retrieved that are relevant to our question. This is kind of the very basic flow. Now an important consideration in generation is really what's happening is we're taking the documents you retrieve and we're stuffing them into the LLM context window. So if we kind we walk back through the process, we take documents, we split them for convenience or embedding. We then embed each split and we store that in a vector store as this easily searchable numerical representation or vector. We take a question, embed it to produce a similar numerical representation. We can then search, for example, using something like K and N in this kind of high dimensional space for documents that are similar to our question based on their proximity or location in this space. In this case, you can see 3D as a toy, kind of toy example. Now we've recovered relevant splits to our question. We pack those into the context window, for example. Now we've recovered relevant splits to our question. We pack those into the context window, and we produce our answer. Now this introduces the notion of a prompt. So the prompt is kind of a placeholder that has, for example, in our case, keys. So those keys can be like context and question. So they basically are like buckets that we're gonna take those retrieved documents and slot them in. We're gonna take our question and also slot it in. If you kind of walk through this flow you can kind of see that we can build like a dictionary from our retrieved documents and from our question and then we can basically populate our prompt template with the values from the dict and then becomes a prompt value which can be passed to an LLM like a chat model resulting in chat messages which we then parse into a string and get our answer. So that's like the basic workflow that we're gonna see. And let's just walk through that in code very quickly to kind of give you like a hands-on intuition. So we had our notebook we walked through previously, install a few packages, I'm setting a few like Smith environment variables, we'll see it's nice for kind of observing and debugging our traces. Previously we did this quick start, we're going to skip that over. And what I will do is I'm going to build our retriever. And what I will do is I'm going to build our retriever. So again, I'm going to take documents and load them. And then I'm going to split them here. We've kind of done this previously, so I'll go through this kind of quickly. And then we're going to embed them and store them in our index. So now we have this retriever object here. Now I'm going to jump down here. Now here's where it's kind of fun. This is the generation bit. And you can see here I'm defining something new. now. Here's where it's kind of fun. This is the generation bit and you can see here I'm defining something new. This is a prompt template and My prompt template something really simple. It's just gonna say answer the following question based on this context It's gonna have this context variable and a question. So now I'm building my prompt. So great now I have this prompt Let's define an LLM. I'll choose 3 5 Now this introduce the notion of a chain. So in Langchain, we have an expression language called L-C-E-L, Langchain Expression Language, which lets you really easily compose things like prompts, LLMs, parsers, retrievers, and other things. But the very simple kind of example here is just let's just take our prompt, which you defined right here, and connect it to an LLM prompt which you defined right here and connect it to an LLM which you defined right here into this chain. So there's our chain. Now all we're doing is we're invoking that chain. So every line chain expression language chain has a few common methods like invoke, batch, stream. In this case we should invoke it with a dict. So context in question that maps to the expected keys here in our template. And so we run invoke, what we see is it's just going to execute that chain and we get our answer. Now, if we zoom over to Langsmith, we should see that it's been populated. So yeah, we see a very simple runnable sequence. Here was our document, and here's our output, and here is our prompt. Answer the following question based on the context. Here's the document we passed in, here is the question, and then we get our answer. So that's pretty nice. Now there's a lot of other options for rag prompts. I'll pull one in from our prompt tub. This one's kind of a popular prompt, so it just has a little bit more detail. But the main intuition is the same. You're passing in documents, you're asking a lot of them to reason about the documents, give them a question, produce an answer. And now here I'm going gonna find a rag chain, which will automatically do the retrieval for us. And all I have to do is specify, here's my retriever, which we defined before. Here's our question, which we invoke with. The question gets passed through to the key question in our dict, and it automatically will trigger the retriever, which will return documents, which get passed into our context. So it's exactly what we did up here, except before we did this manually. And now, this is all kind of automated for us. We pass that DIC, which is auto-populated, into our prompt, LLM, out the parser, now let's invoke it, and that should all just run. And great. We get an answer and we can look at the trace and we can see everything that happened. So we can see our retriever was run. These documents were retrieved. They get passed into our LLM and we get our final answer. So this kind of the end of our overview, where we talked about, I'll go back to the slides here quickly. We talked about indexing, retrieval, and now generation. Follow-up short videos will dig into some of the more complex or detailed themes that address some limitations that can arise in this very simple pipeline. Thanks.

videoId JChPi0CRnDY
Part 5
Hi, this is Lance from LineChain. Over the next few videos, we're going to be talking about query translation. In this first video, we're going to cover the topic of multi-query. So query translation sits kind of at the first stage of an advanced RAG pipeline. And the goal of query translation is really to take an input user question and to translate it in some way in order to improve retrieval. So the problem statement is pretty intuitive. User queries can be ambiguous. And if the query is poorly written, because we're typically doing some kind of semantic similarity search between the query and our documents, if the query is poorly written or ill-posed, we won't retrieve the proper documents from our index. So there's a few approaches to attack this problem, and you can kind of group them in a few different ways. So here's one way I like to think about it. A few approaches has involved query rewriting. So taking a query and reframing it, like writing it from a different perspective. And that's what we're gonna talk about a little bit here in depth using approaches like multi-query or rag fusion, which we'll talk about in the next video. You can also do things like take a question and break it down to make it less abstract, like into sub-questions, and there's a bunch of interesting papers focused on that, like Least to Most from Google. You can also take the opposite approach of take a question and make it more abstract, and there's actually an approach we're going to talk about later in a future video called Step-Back Prompting that focuses on like kind of higher level question from the input. So the intuition though, for this multi-query approach, so we're taking a question and we're gonna break it down into a few differently worded questions from different perspectives. And the intuition here is simply that it is possible that the way a question is initially worded, once embedded, it is not well aligned or in close proximity in this high dimensional embedding space to a document we want to retrieve that's actually related. The thinking is that by rewriting it in a few different ways, you actually increase the likelihood of actually retrieving the document that you really want to. Because of nuances in the way that documents and questions are embedded, this kind of more shotgun approach of taking a question, fanning it out into a few different perspectives may improve and increase the reliability of retrieval. That's like the intuition really. And of course we can combine this with retrieval. That's like the intuition really. And of course we can combine this with retrieval so we can take our kind of fanned out questions, do retrieval on each one and combine them in some way and perform RAG. So that's kind of the overview. And now let's go over to our code. So this is a notebook and we're going to share all this. We're just installing a few packages. We're setting a Lanksmith API keys, which we'll see why that's quite useful here shortly. There's our diagram. Now, first, I'm going to index this blog post on agents. I'm going to split it. Well, I'm going to load it. I'm going to split it, and then I'm going to index it in Chroma locally. So this is a vector store. We've done this previously, so now I have my index defined. So here's where I'm defining my prompt for MultiQuery, which is your assistant, your task is to basically reframe this question into a few different sub-questions. So there's our prompt right here. We'll pass that to an LLM, parse it into a string and then split the string by new lines. We'll get a list of questions out of this chain. That's really all we're doing here. Now, all we're doing is here's a sample input question. There's our generate queries chain which we defined. We're going to take that list and then simply apply each question to a retriever. So we'll do retrieval per question. And this little function here is just going to take the unique union of documents across all those retrievals. So let's run this and see what happens. So we're going to run this and we're going to get some set of questions or documents back. So let's go to Langsmouth now. We can actually see what happened under the hood. So here's the key point. We ran our initial chain to generate this set of reframed questions from our input. And here is that prompt. And here is that set of questions that we generated. Now what happened is for every one of those questions, we did an independent retrieval. That's what we're showing here. So that's kind of the first step which is great. Now I can go back to the notebook and we can show this working end-to-end. So now we're going to take that retrieval chain, we'll pass it into context of our final rag prompt, we'll also pass through the question, we'll pass that to our rag prompt here, pass it to an LM, and then parse the output. Now let's kind of see how that works. So again, that's okay, there it is. So let's actually go into Lancet and see what happened under the hood. So this was our final chain. So this is great. We took our input question. We broke it down to these like five rephrase questions. For every one of those we did a retrieval. That's all great. We then took the unique union of documents and you can see in our final LLM prompt answer the following contact following question based on the context. This is the final set of unique documents that we retrieved from all of our sub questions. Here's our initial question. There's our initial question. There's our answer. So that kind of shows you I can set this up really easily. I can use LangSmith to kind of investigate what's going on. And in particular, use LangSmith to investigate those intermediate questions that you generate in that question generation phase. And in future talks, we're going to go through some of these other methods that we kind of introduced at the start of this one. Thank you.

videoId 77qELPbNgxA
Part 6
Hi, this is Lance from the Lang chain. This is the second video of our deep dive on query translation in our Rack and Scratch series focused on a method called Rack Fusion. So as we kind of showed before, query translation you can think of as the first stage in an advanced Rack pipeline, we're taking an input user question and we're translating in some way in order to improve retrieval. Now, we showed this general mapping of approaches previously. So again, you have rewriting. So you can take a question and break it down into differently worded or different perspectives of the same question. So that's rewriting. There's sub-questions where you take a question, break it down into smaller problems, solve each one independently. Then there's step back where you take a question and go more abstract, where you ask a higher level question as a precondition to answer the user question. So those are the approaches. We're going to dig into one of the particular approaches for re called rack fusion. Now this is really similar to what we just saw with multi query. The difference being we actually apply a kind of a clever rank ranking step of our retrieved documents which you call reciprocal rank fusion. That's really the only difference. The the input stage of taking a question, breaking it out into a few kind of differently worded questions, retrieval on each one is all the same. And we're going to see that in the code here shortly. So let's just hop over there and then look at this. So again, here is a notebook that we introduced previously. Here's the packages we've installed. We've set a few API keys for a Langsmith, which we see why it's quite useful. And you can kind of go down here to our Rack Fusion section. And the first thing you'll note is what our prompt is. So it looks really similar to the prompt we just saw with multi-query. And it's simply your helpful assistant that generates multiple search queries based upon a user input. And here's the question, output four queries. So let's define our prompt. And here is our query generation chain. Again, this looks a lot like we just saw. We take our prompt, plumb that into an LLM, and then basically parse by new lines. And that'll basically split out these questions into a list. That's all that's gonna happen here. So that's cool. Now here's where the novelty comes in. Each time we do retrieval from one of those questions we're gonna get back a list of documents from our retriever. And so we do it over that, we generate four questions here based on our prompt. We do it over four questions, well, like a list of lists basically. Now reciprocal rank fusion is really well suited for this exact problem. We want to take this list to list and build a single consolidated list. And really all that's going on is it's looking at the documents in each list and kind of aggregating them into a final output ranking. And that's really the intuition around what's happening here. So let's go ahead and... So let's go ahead and look at that in some detail. So we can see we run retrieval. That's great. Now let's go over to Langsmith and have a look at what's going on here. So we can see that here is our prompt, Now let's go over to Langsmith and have a look at what's going on here. So we can see that here was our prompt, your helpful assistant that generates multiple search queries based on a single input. And here is our search queries. And then here are our four retrievals. So that's really good. So we know that all is working. And then those retrievals simply went into this rank function and are correspondingly ranked to a final list of six unique rank documents. That's really all we did. So let's actually put that all together into a full rag chain that's going to run retrieval, return that final list of rank documents, and pass it to our context, pass through our question, send that to our RAG prompt, pass it to an LM, parse it to an output, and let's run all that together and see that working. Cool. So there's our final answer. Now, let's have a look in Langsmith. We can see here was our four questions, here's our retrievals, and then our final rag prompt plumbed through the final list of ranked six questions, which we can see laid out here, and our final answer. So this can be really convenient, particularly if we're operating across like maybe different vector stores, or we want to do like retrieval across a large number of kind of differently worded questions. This reciprocal rank fusion step is really nice. For example, if we wanted to only take the top three documents or something, it can be really nice to build that consolidated ranking across all these independent retrievals, then pass that to the Alemkirch by the final generation. So that's really the intuition about what's happening here. Thanks.

videoId h0OPWlEOank
Part 7
Hi, this is Lance from Langchain. This is our third video focused on query translation in the Rack from Scratch series, and we're going to be talking about decomposition. So query translation in general is a set of approaches that sits kind of towards the front of this overall rag pipeline, and the objective is to modify or rewrite or otherwise decompose an input question from a user in order to improve retrieval. So we kind of talked through some of these approaches previously, in particular various ways to do query writing like Rack Fusion and MultiQuery. There's a separate set of techniques that become pretty popular and are really interesting for certain problems, which you might call breaking down or decomposing an input question into a set of sub-questions. So some of the papers here that are pretty cool are, for example, this work from Google. And the objective really is first to take an input question and decompose it into a set of subproblems. So this particular example from the paper was the problem of last letter concatenation. And so it took the input question of three words, think machine learning, and broke it down into three subproblems, think, think machine, think machine learning as the third subproblem. And then you can see in think machine learning as the third subproblem. And then you can see in this bottom panel, it solves each one individually. So it shows, for example, in green, solving the problem of think machine, where you concatenate the last letter of K with the last letter of machine or the last letter of think K, lesser machine E, concatenate those two K E. And then for the overall problem, taking that solution and then basically building on it to get the overall solution of KeG. So that's kind of one concept of decomposing into subproblems, solving them sequentially. Now a related work called IRCOT, or interleaf retrieval, combines retrieval with chain of thought reasoning. You can put these together into one approach, which you can think of as dynamically retrieval to solve a set of subproblems. Retrieval interleaving with chain of thought, as noted in the second paper, and a set of decomposed questions based on your initial question from the first work from Google. So really the idea here is we're taking one sub-question, we're answering it, we're taking that answer and using it to help answer the second sub-question and so forth. So let's actually just walk through this in code to show how this might work. So this is the notebook we've been working with from some of the other videos. You can see we already have a retriever defined up here at the top. And what we're going to do is we're first going to find a prompt that's basically going to say given an input question let's break it down to set up sub problems or sub questions which can be solved individually. So we can do that. And this blog post is focused on agents. So let's ask a question about what are the main components of an LLM-powered autonomous agent system. So let's run this and see what decomposed questions are. So you can see the decomposed questions are, what is LLM technology, how does it work? What are the components? And then how the components interact. So it's kind of a sane way to kind of break down this problem into a few sub problems which you might attack individually. Now here's where we define a prompt that very simply is gonna take our question, we'll take any prior questions we've answered, and we'll take our retrieval and basically just combine them. And we can define this very simple chain. Actually, let's go back and make sure retriever is defined up at the top. So now we are building our retriever. Good, we have that now. So we can go back down here, and let's run this. So now we are running. And what's happening is we're trying to solve each of these questions individually using retrieval and using any prior question answers. So OK, very good. Looks like that's been done. And we can see here's our answer. Now let's go over to Langtoth and actually see what happened under the hood. So here's what's kind of interesting and helpful to see. For the first question, so here's our first one. It looks like it just does retrieval, which is what we expect, and then it uses that to answer this initial question. Now for the second question should be a little bit more interesting because if you look at our prompt here's our question. Now here is our background available question answer pair. So this was the answer question answer pair from the first question which we add to our prompt and then here's the retrieval for this particular question. So we're kind of building up the solution because we're pending the question-answer pair from question one. And then likewise with question three, it should combine all of that. So we can look at here, here's our question, here's question one, here's question two, great. Now here's additional retrieval related to this particular question, and we get our final answer. So that's like a really nice way you can build up solutions using this kind of interleaved retrieval and concatenating prior question-answer pairs. I do want to mention very briefly that we can also take a different approach where we can just answer these all individually and then just concatenate all those answers to produce a final answer. And I'll show that really quickly here. It's like a little bit less interesting maybe because you're not using answers from each question to inform the next one. You're just answering them all in parallel. This might be better for cases where it's not really like a sub question decomposition, but maybe it's like a set of several independent questions whose answers don't depend on each other. That might be relevant for some problems. And we can go ahead and run. OK, so this ran as well. We can look at our trace. And in this case, yeah, we can see that this actually just kind of concatenates all of our QA pairs to produce the final answer. So this gives you a sense for how you can use the query decomposition and pulling ideas from two different papers that are pretty cool. Thanks.

videoId xn1jEjRyJ2U
Part 8
Hi, this is Lance from LangChain. This is the fourth video in our deep dive on query translation in the Rag from Scratch series. And we're going to be focused on step-back prompting. So query translation, as we said in some of the prior videos, kind of sits at the kind of first stage of kind of RAG pipeline or flow. And the main aim is to take an input question and to translate it or modify it in such a way that it improves retrieval. Now, we talked through a few different ways to approach this problem. So one general approach involves rewriting a question. And we talked about two ways to do that, RAG Fusion, MultiQuery. And again, this is really rewriting a question, and we talked about two ways to do that, rag fusion, multi-query. And again, this is really about taking a question and modifying it to capture a few different perspectives, which may improve the referral process. Now another approach is to take a question and kind of make it less abstract, like break it down into sub-questions and then solve each of those independently. So that's what we saw with least to most prompting and a bunch of other variants in that vein of sub-problem solving and then consolidating those solutions into a final answer. Now, a different approach presented by, again, Google as well is step back prompting. So step back prompting takes the opposite approach where it tries to ask a more abstract question. So the paper talks a lot about using few shot prompting to produce what they call these step back or more abstract questions. And the way it does it is it provides a number of examples of step-back questions given your original question. So this is, for example, they're like per prompt template, you're an expert in world knowledge, I asked you a question, your response should be comprehensive, not contradict with the following. And this is kind of where you provide your original and then step back. So here's some example questions. So like, at year saw the creation of the region where the country is located, which region of the country is the county of Hurtur related? Jansendel is born in what country? What is Jansendel's personal history? That's going to be a more intuitive example. It's like you ask a very specific question about the country someone's born. The more abstract question is like, just give me the general history of this individual without worrying about that particular more specific question. So let's actually just walk through how this can be done in practice. So again, here's kind of like a diagram of the various approaches from less abstraction to more abstraction. Now here is where we're formulating our prompt using a few of the few-shot examples from the paper. So again like input yeah something about like the police performing lawful arrests and what what can members of the police do. So like it basically gives the model a few examples. We basically formulate this into a prompt. That's really all that's going on here. Again, we repeat this overall prompt, which we saw from the paper. Your next pair of world knowledge, your task is to step back and paraphrase a question, generate a more generic step back question, which is easier to answer. Here's some examples. So it's like a very intuitive prompt. So, okay, let's start with the question, what is task composition for LLM agents? And we're gonna say generate step back question. Okay, so this is pretty intuitive, right? What is the process of task composition? So like not worrying as much about agents, but what is the process of task composition in general? And then hopefully that can be independently retrieved. We can independently retrieve documents related to the step back question. And in addition, retrieve documents related to the actual question and combine those to produce kind of final answer. So that's really all that's going on. And here's the response template where we're plumbing in the step back context and our question context. And so what we're gonna do here is we're gonna take our input question and perform retrieval on that. We're also gonna generate our step back question and perform retrieval on that. We're gonna plumb those into the prompt as here's our basically our prompt keys, normal question, step back question, and our overall question. Again, we formulate those as a dict. We plumb those into our response prompt. And then we go ahead and attempt to answer our overall question. So we're gonna run that. That's running. And okay, we have our answer. Now I want to hop over to Langsmith and attempt to show you kind of what that looked like under the hood. So let's see, let's like go into each of these steps. So here was our prompt, right? You're an expert on world knowledge, your task is to step back and paraphrase a Let's see. Let's go into each of these steps. So here was our prompt. You're an expert on world knowledge. You're tasked to step back and paraphrase a question. So here are our few-shot prompts. And this was our step back question. So what is the process of task composition? Good. From the input, what is the composition for LLM agents? We perform retrieval on both. What is process task composition? And what is task composition for LLM agents? We perform both retrievals. We then populate our prompt with both. Original question, answer, and then here is the context retrieved from both the question and the step back question. Here's our final answer. So again, this is kind of a nice technique. It probably depends on a lot of the types of, like, the type of domain you want to perform retrieval on. But in some domains, where, for example, there's a lot of kind of conceptual knowledge that underpins questions you expect users to ask. This step-back approach could be really convenient to automatically formulate a higher level question, to, for example, try to improve retrieval. I can imagine if you're working with textbooks or technical documentation, where you've made independent chapters focused on more high-level concepts, and then other chapters on more detailed implementations, this kind of step-back approach and independent retrieval could be really helpful. Thanks.

videoId SaDzIVkYqyY
Part 9
Hi, this is Lance from LineChain. This is the fifth video focused on query translation in our Rack from Scratch series. We're going to be talking about a technique called HIDE. So again, query translation sits kind of at the front of the overall rag flow. And the objective is to take an input question and translate it in some way that improves retrieval. Now, hide is an interesting approach that takes advantage of a very simple idea. The basic rag flow takes a question and embeds it, takes a document and embeds it, and looks for similarity between an embedded document and an embedded question. But questions and documents are very different text objects. So documents can be like very large chunks taken from dense publications or other sources, whereas questions are short, kind of terse, potentially ill-worded from users. And the intuition behind Hyde is take questions and map them into document space using a hypothetical document or by generating a hypothetical document. That's the basic intuition. And the idea kind of shown here visually is that in principle for certain cases a hypothetical document is closer to a desired document you actually want to retrieve in this you know high dimensional embedding space than the sparse raw input question itself. So again, it's just a means of translating raw questions into these hypothetical documents that are then better suited for retrieval. So let's actually do a code walkthrough to see how this works. And it's actually pretty easy to implement, which is really nice. So first, we're just starting with a prompt. And we're using the same notebook that we used for prior videos. We have a blog post on agents already indexed. So what we're going to do is define a prompt to generate a hypothetical documents. In this case, we'll say write a paper passage to answer a given question. So let's just run this and see what happens. Again, we're taking our prompt, piping it to OpenAI, ChetGBT, and then using string-oppa-parser. And so here's a hypothetical document section related to our question, okay? And this is derived, of course, from the LLM's kind of embedded kind of world knowledge, which is, you know, a sane place to generate hypothetical documents. Now, let's now take that hypothetical document and basically we're going to pipe that into a retriever. So this means we're going to fetch documents from our index related to this hypothetical document that's been embedded. And you can see we get a few retrieved chunks that are related to this hypothetical document. That's all we've done. And then let's take the final step, where we take those retrieved documents here, which we defined, and our question. And we're going to pipe that into this rag prompt. And then we're going to run our rag chain right here, which you've seen before, and we get our answer. So that's really it. We can go to Langsmith, and we can actually look at what happened. So here, for example, this was our final rag prompt. Answer the following question based on this context, and here is the retrieved document that we passed in. So that part's kind of straightforward. We can also look at... Okay, this is our retrieval. Okay, no, this is actually what we generated a hypothetical document here. Okay, so this is our hypothetical document. So we've run ChatOpenAI. We generated this passage as our hypothetical document. And then we've run retrieval here. So this is basically showing hypothetical document generation followed by retrieval. So again, here was our passage we passed in. And then here's our retrieved documents from the tree for which are related to the passage content. So again, in this particular index case, it's possible that the input question was sufficient to retrieve these documents. In fact, given prior examples, I know that some of these same documents are indeed retrieved just from the raw question, but in other contexts, that may not be the case. So folks have reported nice performance using HIDE for certain domains. And the really convenient thing is that you can take this document generation prompt, you can tune this arbitrarily for your domain of interest. So it's absolutely worth experimenting with. It's a neat approach that can overcome some of the challenges with retrieval. Thanks very much.

videoId pfpIndq7Fi8
Part 10
Hi, this is Lance from Langchain. This is the 10th video in our Rack from Scratch series focused on routing. We talked through query translation, which is the process of taking a question and translating in some way. It could be decomposing it, using step-back prompting or otherwise. But the idea here was take our question, change it into a form that's better suited for retrieval. Now, routing is the next step, which is basically routing that potentially decomposed question to the right source. And in many cases, that could be a different database. So let's say in this toy example, we have a vector store, a relational DB and a graph DB, that what we'll redo with routing is we simply route the question based upon the content of the question to the relevant data source. So there's a few different ways to do that. One is what we call logical routing. In this case, we basically give an LLM knowledge of the various data sources that we have at our disposal, and we let the LLM kind of reason about which one to apply the question to. So it's kind of like the LLM is applying some logic to determine which data source, for example, to use. Alternatively, you can use semantic routing, which is where we take a question, we embed it, and for example, we embed prompts. We then compute the similarity between our question and those prompts, and then we choose a prompt based upon the similarity. So the general idea is in our diagram, we talk about routing to, for example, a different database, but it can be very general. It can be routing to a different prompt. It can be, you know, really arbitrarily taking this question and sending it in different places, be it different prompts, be it different vector stores. So let's walk through the code a little bit. So you can see just like before, we've done a few pip installs, we've set up Langsmith, and let's talk through logical routing first. So in this toy example, let's say we had, for example, three different docs. Like we had Python docs, we had JS docs, we had Golang docs. What we wanna do is take a question, route it to one of those three. So what we're actually doing is we're setting up a data model, which is basically going to be bound to our LLM and allow the LLM to output one of these three options as a structured object. So you're really thinking about this as classification. Classification plus function calling to produce a structured output, which is constrained to these three possibilities. So the way we do that is, let's just zoom in here a little bit. We can define a structured object we want to get out from our LLM. In this case, we want, for example, one of these three data sources to be output. We can take this, and we can actually convert it into, openNAI for example, function schema. And then we actually pass that in and bind it to our LLM. So what happens is we ask a question, our LLM invokes this function on the output to produce an output that adheres to the schema that we specify. So in this case, for example, we output like, in this toy example, let's say we wanted like, an output to be data source, vector store or SQL database. The output will contain a data source object and it'll be one of the options we specify as a JSON string. We also instantiate a parser from this object to parse that JSON string to an output like a pedantic object, for example. So that's just one toy example. And let's show one up here. So in this case, again, we had our three doc sources. We bind that to our LLM, so you can see we do with structured output, basically under the hood, that's taking that object definition, turning it into function schema, and binding that function schema to our LLM. And we call our prompt. You're an expert at routing a user question based on programming language that you're referring to. So let's define our router here. Now what we're going to do, well, that's a question that is Python code. So we'll call that and now it's done. And you can see the object we get out is indeed, it's a route query object. So it's exactly, and here's this data model we've set up. And in this case, it's correct. So it's calling this Python docs. So we can extract that right here as a string. Now, once we have this, you can really easily set up a route. So this could be our full chain, where we take this router, which is defined here, and then this choose route function can basically take that output and do something with it. So, for example, if Python docs, this could then apply the question to a retriever full of Python information, or JS, same thing. So this is where you would hook basically that question up to different chains that are like, you know, retriever chain one for Python, retriever chain two for JS, and so forth. So this is kind of like the routing mechanism, but this is really doing the heavy lifting of taking an input question and turning it into a structured object that restricts the output to one of a few output types that we care about in our routing problem. So that's really kind of the way this all hooks together. Now, semantic routing is actually maybe a little bit more straightforward based on what we've seen previously. So in that case, let's say we have two prompts. We have a physics prompt, we have a math prompt. We can embed those prompts. No problem. We do that here. Now, let's say we have an input question from a user, like in this case, what is a black hole? We pass that through. We then apply this run of a lambda function, which is defined right here. What we're doing here is we're embedding the question We're computing similarity between the question and the prompts we're taking the most similar and then we're basically choosing the prompt based on that similarity and you can see let's run that and try it out and We're using the physics prompt and there we go black holes region in, and space. So that just shows you how you can use semantic routing to basically embed a question, embed, for example, various prompts, pick the prompt based on semantic similarity. So that really gives you just two ways to do routing. One is logical routing with function calling. It can be used very generally. In this case, we applied it to different coding languages, but imagine these could be swapped out for my Python, my vector store versus my graph DB versus my relational DB, and you could just very simply have some description of what each is. And then not only will the LLM do reasoning, but it'll also return an object that can be parsed very cleanly to produce like one of a few very specific types, which then you can reason over like we did here in your routing function. So that kind of gives you the general idea, and these are very useful tools, and I encourage you to experiment with them. Thanks.

videoId kl6NwWYxvbM
Part 11
Hi, this is Lance from Langchain. This is the 11th part of our Rack from Scratch video series focused on query construction. So we previously talked through query translation, which is the process of taking a question and converting it or translating it into a question that's better optimized for retrieval. Then we talked about routing, which is the process of going, taking that question and routing it to the right source, be it a given vector store, graph DB, or SQL DB for example. Now we're going to talk about the process of query construction, which is basically taking natural language and converting it into particular domain specific language for one of these sources. Now we're going to talk specifically about the process of going from natural language to metadata filters for vector stores. The problem statement is basically this. Let's imagine we had an index of Lang chain video transcripts. You might want to ask a question. Find me videos on chat Lang chain published after 2024, for example. The process of query structuring basically converts this natural language question into a structured query that can be applied to the metadata filters on your vector store. So most vector stores will have some kind of metadata filters that can do structured querying on top of the chunks that are indexed. So for example, this type of query will retrieve all chunks that talk about the topic of Chat Lang chain published after the date 2024. That's kind of the problem statement. And to do this, we're going to use function calling. In this case, you can use, for example, OpenAI or other providers to do that. And what we're going to do is, at a high level, take the metadata fields that are present in our vector store and provide them to the model as kind of information. And the model then can take those and produce queries that adhere to the schema provided. And then we can parse those out to a structured object, like a PIDENTIC object, which can then be used in search so that's kind of problem statement and let's actually walk through code so here's our notebook which we kind of gone through previously and I'll just show you as an example let's take an example YouTube video and let's look at the metadata that you get with the transcript so you can see you get stuff like description, URL, yeah, publish date, length, things like that. Now let's say we had an index that had basically, it had a number of different metadata fields and filters that allowed us to do range filtering on like view count, publication date, video length, or unstructured search on contents and title. So those are kind of like the, imagine we had an index that had those kind of filters available to us. What we can do is capture that information about the available filters in an object. So we're calling that this tutorial search object kind of encapsulates that information about the available searches that we can do. And so we basically enumerated here, content search and title search or semantic searches that can be done over those fields. And then these filters then are various types of structured searches we can do on like the length, the view count and so forth. And so we can just kind of build that object Now, we can set this up really easily with a simple prompt that says, you're an expert can bring natural language into database queries, you've access to the database tutorial videos, given a question, return a database query, optimize retrieval, so that's it. Now, here's the key point though. When you call this LLM with structured output, you're binding this Pydantic object which contains all the information about our index to the LLM, which is exactly what we talked about previously. It's really this process right here. You're taking this object, you're converting it to a function schema, for example OpenAI, you're binding that to your model, and then you're going to be able to get and then you're going to be able to get a structured object out versus JSON string from a natural language question, which can then be parsed into a pedantic object, which you get out. So that's really the flow. It's taking advantage of function calling as we said. So if we go back down, we set up our query analyzer chain right here. Now let's try to run that just on a purely semantic input. So rag from scratch. Let's run that. And you can see this just does like a content search and a title search. That's exactly what we would expect. Now if we pass a question that includes like a date filter, let's just see if that would work. And there we go. So you kind of still get that semantic search, but you also get search over, for example, published date, earliest and latest published date, kind of as you would expect. Let's try another one here. So videos focus on the topic of Chat Lang chain, they're published for 2024. This is just kind of rewrite of this question in slightly different way, using a different date filter. And then again, you can see we can get, we get content search, title search, and then we can get kind of a date search. So this is a very general strategy that can be applied kind of broadly to different kinds of querying you want to do. It's really the process of going from an unstructured input to a structured query object out following an arbitrary schema that you provide. And so as noted, really, this whole thing we created here, this tutorial search, is based upon the specifics of our vector store of interest. And if you want to learn more about this, I linked to some documentation here that talks a lot about different types of integrations we have with different vector store providers to do exactly this. So it's a very useful trick. It allows you to do exactly this. So it's a very useful trick. It allows you to do kind of query, say metadata filtering on the fly from a natural language question. It's a very convenient trick that works with many different vector DBs. So I encourage you to play with it. Thanks.

videoId gTCU9I6QqCE
Part 12
Hi, this is Lance from Langchain. I'm going to talk about indexing and multi-referentation indexing in particular for the 12th part of our Rag from Scratch series here. So we previously talked about a few different major areas. We talked about query translation, which takes a question and translates it in some way to optimize for retrieval. We talked about routing, which is the process of taking a question, routing it to the right data source, be it a vector store, graph DB, SQL DB. We talked about query construction. We dug into basically query construction for vector stores, but of course there's also text SQL, text decipher. So now we're going to talk about indexing a bit. In particular, we're going to talk about indexing a bit. In particular, we're going to talk about indexing techniques for vector stores. And I want to highlight one particular method today called multi-representation indexing. So the high-level idea here is derived a bit from a paper called proposition indexing, which kind of makes a simple observation. You can think about decoupling raw documents and the unit to use for retrieval. So in the typical case, you take a document, you split it up in some way to index it, and then you embed the split directly. This paper talks about actually taking a document, splitting it in some way, but then using an LLM to produce what they call a proposition, which you can think of as like kind of a distillation of that split. So it's kind of using an LLM to modify that split in some way to distill it or make it like a crisper, like summary, so to speak, that's better optimized for retrieval. So that's kind of one highlight, one piece of intuition. So we've actually taken that idea and we've kind of built on it a bit in kind of a really nice way that I think is very well suited actually for long context LLMs. So the idea is pretty simple. You take a document and you actually distill it or create a proposition like they show in the prior paper. I kind of typically think of this as just pretty just summary of the document. And you embed that summary. So that summary is meant to be optimized for retrieval. So it might contain a bunch of keywords from the document or like the big ideas, such that when you embed the summary, you embed a question, you do search, you basically can find that document based upon this highly optimized summary for retrieval. So that's kind of represented here in your vector store. But here's the catch. You independently store the raw document in a doc store. And when you basically retrieve the summary in the vector store, you return the full document for the LLM to perform generation. And this is a nice trick because at generation time, now with long-context LLM, for example, the LLM can handle that entire document. You don't need to worry about splitting it or anything. You just simply use this summary to create a really nice representation for phishing out that full doc. Use that full doc in generation. There might be a lot of reasons you want to do that. You want to make sure the LM has the full context to actually answer the question. That's the big idea. It's a nice trick. And let's walk through some code here. So we have a notebook all set up, just like before. We've done some PIP installs, set some API keys here for LangSmith. Kind of here's a diagram. Now let me show an example. Let's just load two different blog posts. One is about agents, one is about you know human data quality, and what we're gonna do is let's create a summary of each of those. So this is kind of the first step of that process, where we're going from the raw documents to summaries. Let's just have a look and make sure those ran. So OK, cool. So the first doc discusses building autonomous agents. The second doc contains the importance of high-quality human data and training. OK, so that's pretty nice. We have our summaries. Now we're going to go through a process that's pretty simple. First, we define a vector store that's going to index those summaries. Then we're going to define what we call our document storage. You're going to store the full documents. This multi-vector retriever just pulls those two things together. We basically add our doc store. We add this byte store is basically the full document store. The vector store is our vector store. And now this ID is what we're going to use to reference between the chunks or the summaries and the full documents. That's really it. So now for every document, we'll define a new doc ID. And then we're basically going to take our summary documents. And we're going to extract for each of our summaries, we're going to get the associated doc ID. So there we go. So let's go ahead and do that. So we have our summary docs, which we add to the vector store. We have our full documents, our doc IDs, and the full raw documents, which are added to our doc store and then let's just do a query of vector store like a similarity search on our vector store so memory and agents and we can see okay so we can extract you know from the summaries we can get for example the summary that pertains to agents so that's a good thing now let's go ahead and run a query, get relevant documents on our retriever, which basically combines the summaries which we use for retrieval, then the doc store, which we use to get the full doc back. So we're going to apply our query. We're going to basically run this. And here's the key point. We've gotten back the entire article. And we can actually, if you want to look at the whole thing, we can just go ahead and do this. Here we go. So this is the entire article that we get back from that search. So it's a pretty nice trick. Again, we query with just memory and agents. And we can go back to our diagram here. We created for memory and agents. It starts our summaries. diagram here. We created for memory and agents. It starts our summaries. It found the summary related to memory and agents. It uses that doc ID to reference between the vector store and the doc store. It fishes out the right full doc, returns us the full document, in this case, the full web page. That's really it. Simple idea, nice way to go from basically like nice simple proposition style or summary style indexing to full document retrieval, which is very useful, especially with long context LLMs. Thank you.

videoId z_6EeA2LDSw
Part 13
Hi, this is Lance from Langchain. This is the 13th part of our Rack from Scratch series focused on a technique called Raptor. So Raptor sits within kind of an array of different indexing techniques that can be applied on vector stores. We just talked about multi-representation indexing. I provided a link to a video that's very good talking about the different means of chunking, so I encourage you to look at that. And we're going to talk today about a technique called Raptor, which you can kind of think of it as a technique for hierarchical indexing. So the high-level intuition is this. Some questions require very detailed information from a corpus to answer, like pertain to a single document or single chunk. So we can call those low-level questions. Some questions require consolidation across broad swaths of a document, so across many documents or many chunks within a document. And you can call those higher-level questions. And so there's this challenge in retrieval. And typically,'s kind of this challenge in retrieval, and that typically we do like k nearest neighbors retrieval like we've been talking about, you're fishing out some number of chunks. But what if you have a question that requires information across like five, six, you know, or a number of different chunks, which may exceed, you know, the k parameter in your retrieval. So again, when you typically do retrieval, you might set a K parameter of three, which means you're retrieving three chunks from your vector store. And maybe you have a very high level question that could benefit from evaporation across more than three. So this technique called Raptor is basically a way to build a hierarchical index of documents summaries. And the intuition is this. You start with a set of documents as your leafs here on the left. You cluster them. And then you summarize each cluster. So each cluster of similar documents will consult information from across your context, which is your context could be a bunch of different splits or could even be across a bunch of different documents, you're basically capturing similar ones and you're consulting the information across them in a summary. And here's the interesting thing, you do that recursively until either you hit like a limit or you end up with one single cluster that's a kind of very high level summary of all of your documents. And what the paper shows is that if you basically just collapse all these and index them together as a big pool, you end up with a really nice array of chunks that span the abstraction hierarchy. Like you have a bunch of chunks from individual documents that are just like more detailed chunks pertaining to that single document. But you also have chunks from these summaries, or I would say like maybe not chunks, but in this case the summaries like a distillation. So raw chunks on the left that represent your leafs are kind of like the rawest form of information, either raw chunks or raw documents. And then you have these higher level summaries, which are all indexed together. So if you have higher level questions, they should basically be more similar in semantic search, for example, to these higher level summary chunks. If you have lower level questions, then they'll retrieve these more lower level chunks. And so you have better semantic coverage across the abstraction hierarchy of question types. That's the intuition. They do a bunch of nice studies to show that this works pretty well. I actually did a deep dive video just on this which I linked below. I did want to cover it briefly just at a very high level. So let's actually just do kind of a code walkthrough and I've added to this Rad from Scratch course notebook. But I link over to my deep dive video as well as the paper and the full code notebook, which is already checked in and is discussed at more length in the deep dive. The technique is a little bit detailed, so I only want to give you very high levels kind of overview here, and you can look at the deep dive video if you want to go in more depth. Again, we talked through this subtraction hierarchy. I applied this to a large set of Lang chain documents. So this is me loading basically all of our Lang chain expression language docs. So this is on the order of 30 documents. You can see I do a histogram here if the token counts per document. Some are pretty big, most are fairly small, less than 4,000 tokens. And what I did is I indexed all of them individually. So all those raw documents you can kind of imagine are here on the left. And then I do embedding, I do clustering, summarization, and I do that recursively until I end up with, in this case, I believe I only set like three levels of recursion. And then I save them all in my vector store. So that's like the high level idea. I'm applying this Raptor technique to a whole bunch of line chain documents that have fairly large number of tokens. So I do that. And yeah, I use, actually I use both Cloud as well as OpenAI here. This talks through the clustering method which they use which is pretty interesting. You can kind of dig into that on your own if you're really interested. This is a lot of their code which I cite accordingly. This is basically implemented the clustering method that they use. And this is just simply the document embedding stage. This is basically embedding and clustering. That's really it. Some text formatting, summarizing of the clusters right here. And then this is just running that whole process recursively. That's really it. This is tree building. So basically, I have the raw docs. Let's just go back and look at doc texts. So this should be all my raw documents. So that's right. You can see it here. Doc text is basically just the text and all those line chain documents that I pulled. And so I run this process on them right here. So this is that recursive embedding cluster basically runs and produces that tree. Here's the results. This is me just going through the results and basically adding the result text to this list of texts. Okay, so here's what I do. This leaf text is all the raw documents, and I'm appending to that all the summaries. That's all that's going on. And then I'm indexing them all together. That's the key point. Rag chain, and there you have it. That's really all you do. So anyway, I encourage you to look at this in depth. It's a pretty interesting technique. It works well with long contexts. So for example, one of the arguments I made is that it's kind of a nice approach to consolidate information across like a span of large documents. Like in this particular case, my individual documents were language and expression language docs, each being somewhere in the order of, in this case, most of them are less than 4,000 tokens, some pretty big, but I index them all, I cluster them without any splits, embed them, cluster them, build this tree, and go from there. And it all works because we now have LLMs that can go out to 100 or 200,000, up to a million tokens in context. So you can actually just do this process for big swaths of documents in place without any splitting. It's a pretty nice approach. So I encourage you to think about it, look at it, watch the Deep Dive video if you really want to go deeper on this. Thanks.

videoId cN6S0Ehm7_8
Part 14
Hi, this is Lance from Linechain. This is the 14th part of our Rag from Scratch series. I'm going to be talking about an approach called Cold Bear. So we've talked about a few different approaches for indexing. And just as kind of a refresher, indexing falls kind of right down here in our flow. We started initially with query translation, taking a question, translating it in some way to optimize retrieval. We talked about routing it to a particular database. We then talked about query construction. So going from natural language to the DSL or domain specific language for any of the databases that you wanna work with. Those are metadata filters for vector stores or Cypher for Graph, or SQL for relational DB. So that's kind of flow we talked about today. We talked about some indexing approaches like multi-representation indexing. We gave a small shout out to Greg Cameron to the series on chunking. We talked about hierarchical indexing. I want to include one advanced kind of embedding approach. So we talked a lot about embeddings are obviously very central to semantic similarity search and retrieval. So one of the interesting points that's been brought up is that embedding models of course take a document, you can see her on the top, and embed it, basically compress it to a vector. So it's kind of a compression process. You're representing all the semantics of that document in a single vector. You're doing the same to your question. You're doing similarity search between the question embedding and the document embedding in order to perform retrieval. You're typically taking the K you know, K most similar document embedding is given a question and that's really how you're doing it. Now a lot of people have said, well hey, compressing a full document with all this nuance to a single vector seems a little bit overly restrictive, right? And this is a fair question to ask. There's been some interesting approaches to try to address that and one is this approach, method called Colbert. So the intuition is actually pretty straightforward. There's a bunch of good articles I linked down here. This is my little cartoon to explain it, which I think is hopefully kind of helpful. But here's the main idea. Instead of just taking a document and compressing it down to a single vector, basically a single, what we might call to a single vector, basically, a single, what we might call an embedding vector. We take the document, we break it up into tokens. So tokens are just like, you know, units of content. It depends on the token areas you use. We talked about this earlier. So you basically tokenize it. And you produce basically an embedding or a vector for every token. And there's some kind of positional weighting that occurs when you do this process. So obviously, you look at the implementation to understand the details. But the intuition is that you're producing some kind of representation for every token. And you're doing the same thing for your questions. So you're taking your question, you're breaking it into tokens, and you have some representation or vector per token. And then what you're doing is, for every token in the question, you're breaking it into tokens, and you have some representation or vector per token. And then what you're doing is, for every token in the question, you're computing the similarity across all the tokens in the document. And you're finding the max. You're taking the max, you're storing that, and you're doing that process for all the tokens in the question. So again, token two, you compare it to every token in the document. Compute the max. And then the final score is, in this case, the sum of the max similarities between every question token and any document token. So it's an interesting approach. It reports very strong performance. Latency is definitely a question. So kind of production readiness is something you should look into. But it's an approach that's worth mentioning here because it's pretty interesting. And let's walk through the code. So there's actually a really nice library called Rakituwe, which makes it very easy to play with Colbert. She's pip install out here. I've already done that. And we can use one of their pre-trained models to mediate this process. So I'm basically following their documentation. This is kind of what they recommended. So I'm running this now. Hopefully, this runs somewhat quickly. I'm not sure. I previously have loaded this model, so hopefully it won't take too long. And yeah, you can see it's pretty quick. I'm on a Mac M2 with 32 gigs. So just as like a context in terms of my system. This is from their documentation. We're just grabbing a Wikipedia page. This is getting a full document on Miyazaki. So that's cool. We're gonna grab that. Now this is just from their docs. This is basically how we create an index. So we provide the, you know, some index name, the collection, the dot max document length, and yeah you should look at their documentation for these flags. These are just the defaults. So I'm going to create my index. So I get some logging here. So it's working under the hood. And by the way, I actually have their documentation open, so you can kind of follow along. So let's see, yeah, right about here. So you can kind of follow this indexing process. It's create an index, so you load a trained model. This can be either your own pre-trained model or one of ours from the hub. And this is kind of the process we're doing right now. Create index is just a few lines of code and it's exactly what we're doing. So this is the, you know, my documents and this is the indexing step that we just kind of walked through and it looks like it's done. So you get a bunch of logging here, that's fine. Now let's actually see if this works. So we're gonna run drag search, what in a motion studio did Miyazaki found, set our K parameter, and we get some results. Okay, so it's running and cool, we get some documents out. So you know, it seems to work. Now what's nice is you can run this within line chain as a line chain retriever. So that basically wraps this as a line chain retriever and then you can use it freely as a retriever within line chain. It works with all the other different LMs and all of their components, like re-rankers and SOFR that we talked through. So you can use this directly as a retriever. Let's try this out. And boom, nice and fast. And we get our documents. Again, this is a super simple test example. You should run this maybe on more complex cases, but it's pretty easy to spin up. It's a really interesting alternative indexing approach using, again, like we talked through, a very different algorithm for computing doc similarity that may work better. I think an interesting regime to consider this would be longer documents. So if you want longer, yeah, if you basically want long context embedding, I think you should look into, for example, the max token limits for this approach because it partitions a document into each token. I would be curious to dig into what the overall context limits are for this approach of Colbert, but it's really interesting to consider and it reports very strong performance. So again, I encourage you to play with it and this is just kind of an intro to how to get set up and to start experimenting with it really quickly. Thanks.
