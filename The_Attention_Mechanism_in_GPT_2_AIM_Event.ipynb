{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GLecDiHbogvX",
        "eo_QP1ITFfX2"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbr/ai-eng-bootcamp/blob/main/The_Attention_Mechanism_in_GPT_2_AIM_Event.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Pre-Training of GPT-Style Model\n",
        "\n",
        "In today's notebook, we'll be working through an example of how to do unsupervised pre-training of a GPT-style model.\n",
        "\n",
        "The base model we'll use is Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
        "\n",
        "All of the model code can be found in the [`model.py`](https://github.com/karpathy/nanoGPT/blob/master/model.py) file!\n",
        "\n",
        "> NOTE: We will not be leveraging the parallized training strategy in this notebook - you can find all the required code in the provided repository."
      ],
      "metadata": {
        "id": "UWiGVj6njoDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture Diagram\n",
        "\n",
        "This is the diagram we'll be using to guide our later intuitions about the model architecture!\n",
        "\n",
        "![image](https://i.imgur.com/WtcoFOo.png)"
      ],
      "metadata": {
        "id": "GBGw2a0bfT-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Selection\n",
        "\n",
        "For the notebook today, we'll be using a toy dataset called `tinyshakespeare`. Feel free to use your own corpus here, just make sure it's contained within a single `.txt` file.\n",
        "\n",
        "You could extend this example to use the [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset, which was used to pre-train GPT-2.\n",
        "\n",
        "> NOTE: Training LLMs can take a very long time - in order to get results similar to the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) you will need 8xA100s and train for ~4-5 days using a pararellized strategy (DDP) on the OpenWebText Corpus.\n",
        "\n",
        "Let's start by grabbing our source repository for the day!"
      ],
      "metadata": {
        "id": "eHi04aEnkKEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMRsEQZy6tgc",
        "outputId": "460b436e-842f-43cb-dbd4-f95e7f00036e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 671, done.\u001b[K\n",
            "remote: Total 671 (delta 0), reused 0 (delta 0), pack-reused 671\u001b[K\n",
            "Receiving objects: 100% (671/671), 947.92 KiB | 31.60 MiB/s, done.\n",
            "Resolving deltas: 100% (379/379), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll need to grab some dependencies.\n",
        "\n",
        "`cohere` and `openai` are recent dependencies of `tiktoken`, but we will not be leveraging them today."
      ],
      "metadata": {
        "id": "6l4CqoEDl7ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken requests cohere openai -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_gepPv1Qdj_",
        "outputId": "4edaeb40-dab4-4e65-f10d-639debd54161"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.1 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First things first - let's download our dataset!\n",
        "\n",
        "We'll leverage the `requests` library to do this - and then we will split our resultant data into a `train` and `val` set."
      ],
      "metadata": {
        "id": "70hSjXmZmCt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "current_path = \"/data/shakespeare\"\n",
        "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "\n",
        "if not os.path.exists(current_path):\n",
        "    os.makedirs(current_path)\n",
        "\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.path.dirname(current_path), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]"
      ],
      "metadata": {
        "id": "T7qRWArUNiZ5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's get our `tokenizers` dependency so we can train a tokenizer on our data."
      ],
      "metadata": {
        "id": "wU9BG2CymU-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers -qU"
      ],
      "metadata": {
        "id": "gFnrwKpQPsYh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be training a \"byte-pair-encoding\" or \"BPE\" tokenizer. If you'd like to read more, you can find it [here](https://en.wikipedia.org/wiki/Byte_pair_encoding).\n",
        "\n",
        "Let's work through an example of what Byte-Pair Encoding (BPE) is doing, exactly, from this wonderful example provided by [Hugging Face](https://huggingface.co/docs/transformers/main/tokenizer_summary#byte-pair-encoding-bpe).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rmWXE5ctma9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is BPE?\n",
        "\n",
        "First, we need to do a step called \"pre-tokenization\", which is - as it sounds - a tokenization step that occurs before we tokenize.\n",
        "\n",
        "The essential idea of BPE is that we need to understand common words and \"byte-pairs\" in them. So, in order to find \"common words\" we first need to find...words!\n",
        "\n",
        "Let's take the following text and break it apart into its word components.\n",
        "\n",
        "\n",
        "```\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "```\n",
        "\n",
        "A naive way to do this would just be by splitting on spaces...and that is indeed what technique was used in GPT-2."
      ],
      "metadata": {
        "id": "GLecDiHbogvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "\"\"\"\n",
        "\n",
        "naive_word_list = input_text.split()"
      ],
      "metadata": {
        "id": "m34NDAGCpiz6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can count our words and get their frequency."
      ],
      "metadata": {
        "id": "hR8k-2bopqjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "vocab_and_frequencies = defaultdict(int)\n",
        "\n",
        "for word in naive_word_list:\n",
        "  vocab_and_frequencies[\" \".join(list(word))] += 1\n",
        "\n",
        "sorted(vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_201bSQpvqD",
        "outputId": "012ba411-3c6b-4f46-c293-45dc8fc6cf65"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('t h e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find our \"base vocabulary\", which is going to be each symbol present in our original dataset."
      ],
      "metadata": {
        "id": "NckufSxxp-w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple, List, Set\n",
        "\n",
        "def find_vocabulary_size(current_vocab: Dict[str, int]) -> int:\n",
        "  vocab = set()\n",
        "\n",
        "  for word in current_vocab.keys():\n",
        "    for subword in word.split():\n",
        "      vocab.add(subword)\n",
        "\n",
        "  return len(vocab)"
      ],
      "metadata": {
        "id": "BNcjzjDvvKjp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_vocabulary_size(vocab_and_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf3kCf-WvdBL",
        "outputId": "d8ab6309-7780-4e1d-ea68-50212286667f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, there are 36 symbols in our base vocabulary. Let's convert our data into a form where we can capture each symbol separately."
      ],
      "metadata": {
        "id": "VoMq7GhKqf7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can start constructing our pairs. We will look at all the pairs of symbols as they appear and take into consideration their frequency in our corpus."
      ],
      "metadata": {
        "id": "OGxrHYmftDTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pairs_and_frequencies(current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  pairs = {}\n",
        "\n",
        "  for word, frequency in current_vocab.items():\n",
        "    symbols = word.split()\n",
        "\n",
        "    for i in range(len(symbols) - 1):\n",
        "      pair = (symbols[i], symbols[i + 1])\n",
        "      current_frequency = pairs.get(pair, 0)\n",
        "      pairs[pair] = current_frequency + frequency\n",
        "\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "sTwvfTAErQN7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_and_frequencies = find_pairs_and_frequencies(vocab_and_frequencies)"
      ],
      "metadata": {
        "id": "FudOaKmYv9-y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGIJfkk7wFYw",
        "outputId": "d1d77d69-07f3-4394-da8d-d0cc2b3d596d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('t', 'h'), 11),\n",
              " (('i', 'n'), 10),\n",
              " (('r', 'e'), 8),\n",
              " (('h', 'e'), 8),\n",
              " (('a', 't'), 7)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the frequent pairs - we can merge those pairs into a single token.\n",
        "\n",
        "Let's see how this process looks in code."
      ],
      "metadata": {
        "id": "OqORqdzwsZ6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def merge_vocab(most_common_pair: Tuple[str], current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  vocab_out = {}\n",
        "\n",
        "  pattern = re.escape(' '.join(most_common_pair))\n",
        "  replacement = ''.join(most_common_pair)\n",
        "\n",
        "  for word_in in current_vocab:\n",
        "      word_out = re.sub(pattern, replacement, word_in)\n",
        "      vocab_out[word_out] = current_vocab[word_in]\n",
        "\n",
        "  return vocab_out"
      ],
      "metadata": {
        "id": "L7ohHm2kshoY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_vocab_and_frequencies = merge_vocab(\n",
        "    sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],\n",
        "    vocab_and_frequencies\n",
        ")"
      ],
      "metadata": {
        "id": "Ab760KKuwzZ6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(new_vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0XtvLbpxbSx",
        "outputId": "3e6646d6-7a63-48dd-e53e-dd45958ccd56"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('th e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After one merge, we can see that `t h` has been converted to `th`!\n",
        "\n",
        "Let's see how that impacted our vocabulary."
      ],
      "metadata": {
        "id": "9DPkBzj2u-me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "find_vocabulary_size(new_vocab_and_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO_xegCtxjQf",
        "outputId": "5b1a3b75-942c-419b-b2ee-c9857c4858d7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our vocabulary has increased by 1 as we've added the `th` symbol to it!\n",
        "\n",
        "In essence, BPE will continue to do this process until your desired vocabulary size (a hyper-parameter) is met!"
      ],
      "metadata": {
        "id": "o3M13D60xzZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Our Tokenizer\n",
        "\n",
        "Now that we have some background on how BBPE works, lets move on to training our tokenizer for our model!\n",
        "\n",
        "Let's walk through the steps we'll take:\n",
        "\n",
        "1. Initialize our `Tokenizer` with a `BPE` model. Be sure to include the `unk_token`.\n",
        "\n",
        "  - [`Tokenizer`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizer)\n",
        "  - [`Models`](https://huggingface.co/docs/tokenizers/api/models#models)\n",
        "\n",
        "2. We'll include a normalizer, applied at the sequence level, and we'll use `NFD()` to do so. More reading on Unicode Normalization Forms [here](https://unicode.org/reports/tr15/#Normalization_Forms_Table).\n",
        "\n",
        "  - [`NFD()`](https://huggingface.co/docs/tokenizers/api/normalizers#tokenizers.normalizers.NFD)\n",
        "\n",
        "3. We'll also add our `ByteLevel()` pre-tokenizer, and our `ByteLevelDecoder()` decoder.\n",
        "\n",
        "  - [`ByteLevel()`](https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)\n",
        "  - [`ByteLevelDecoder()`](https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.ByteLevel)"
      ],
      "metadata": {
        "id": "BePYCbHly02H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFD, Sequence\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = Sequence([NFD()])\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.decoder = ByteLevelDecoder()"
      ],
      "metadata": {
        "id": "OrztE09OPosB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll want to add some special tokens to our tokenizer to ensure in has access to common token patterns.\n",
        "\n",
        "Let's use the following:\n",
        "\n",
        "- `\"<s>\"`    : bos_token - beginning of sequence token\n",
        "- `\"</s>\"`   : eos_token - end of sequence token\n",
        "- `\"<pad>\"`  : padding_token - token used to pad sequences\n",
        "- `\"<unk>\"`  : unk_token - token used to represent unknown tokens.\n",
        "- `\"<mask>\"` : mask_token - token used to mask parts of our sequence\n",
        "\n",
        "We're also going to set a target vocabulary of 50,000 tokens."
      ],
      "metadata": {
        "id": "dDqkNNdM1KsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = BpeTrainer(\n",
        "    vocab_size=50000,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\n",
        "      \"<s>\",\n",
        "      \"<pad>\",\n",
        "      \"</s>\",\n",
        "      \"<unk>\",\n",
        "      \"<mask>\"\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "x9iQVhN3P3RN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nothing left to do but point it at our data-source and let it train!\n",
        "\n",
        "We'll use the `.train()` method to accomplish this task.\n",
        "\n",
        "> NOTE: Pay attention to the desired inputs of the `.train()` method.\n",
        "\n",
        "- [`Tokenizer.train()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.train)"
      ],
      "metadata": {
        "id": "yQ8X9vZe2Fyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(files=[input_file_path], trainer=trainer)"
      ],
      "metadata": {
        "id": "LinLHotSP7gv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can save our tokenizer - and then load it as a `GPT2Tokenizer` through the Hugging Face Library!"
      ],
      "metadata": {
        "id": "V2JNYiqB2qKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/tokenizer'\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "tokenizer.model.save(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk6QjDGHQy2K",
        "outputId": "875ae3f8-25c8-41dc-d03c-fcef4795dbe2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/tokenizer/vocab.json', '/content/tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOOlbggdRFrN",
        "outputId": "b5f2624c-2660-47a4-dc35-3363cede657d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"
      ],
      "metadata": {
        "id": "us1vofdhQ45C"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it tokenizes our inputs!"
      ],
      "metadata": {
        "id": "0-Bnq7lV2xWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Hark, my name be Romeo! I am but a beautiful summer's day!\""
      ],
      "metadata": {
        "id": "dnYnFa3fTRLf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentence = tokenizer.tokenize(input_sentence)\n",
        "tokenized_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSHY5VufRbBj",
        "outputId": "dfe9196f-8949-441b-a05e-9a5d0ddc9751"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hark',\n",
              " ',',\n",
              " 'Ġmy',\n",
              " 'Ġname',\n",
              " 'Ġbe',\n",
              " 'ĠRomeo',\n",
              " '!',\n",
              " 'ĠI',\n",
              " 'Ġam',\n",
              " 'Ġbut',\n",
              " 'Ġa',\n",
              " 'Ġbeautiful',\n",
              " 'Ġsummer',\n",
              " \"'s\",\n",
              " 'Ġday',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_tokens = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "encoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZrWzQQlTU41",
        "outputId": "c391782d-73b3-46e5-f2cf-ca444f5a86d8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12077, 9, 124, 637, 121, 826, 5, 87, 295, 219, 72, 9113, 2999, 141, 511, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_tokens = tokenizer.decode(encoded_tokens, clean_up_tokenization_spaces=False)\n",
        "decoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oS6lE-NLRnzk",
        "outputId": "01b8c09e-9ade-4069-c7a6-d3077a1925bc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hark, my name be Romeo! I am but a beautiful summer's day!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing Dataset\n",
        "\n",
        "Now that we have trained our tokenizer - let's create a dataset we can leverage with the `nanoGPT` library.\n",
        "\n",
        "We'll simply encode our training and validation data - and then save them in binary files for later!\n",
        "\n",
        "> NOTE: Pay attention to the format you want your dataset in. We want ids, which means we want to use the [`.encode()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.encode) method of our tokenizer."
      ],
      "metadata": {
        "id": "ji3sF-rA21YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = tokenizer.encode(train_data)\n",
        "val_ids = tokenizer.encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "calHML6JPnCU",
        "outputId": "816a4fe3-0377-4c90-aa79-932287951260"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 291,284 tokens\n",
            "val has 34,223 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export to bin files\n",
        "data_path = \"/data/shakespeare/\"\n",
        "\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(data_path), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(data_path), 'val.bin'))"
      ],
      "metadata": {
        "id": "nKJ1KqiiPkRh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at our first 100 training tokens to see what format they are in!"
      ],
      "metadata": {
        "id": "DFbbvIi7xsgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9z7ia8AxqEn",
        "outputId": "eda78d95-07b7-4831-d626-8e02f5ac1f07"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  21,  388,  876,   13,   68, 6804,  373,  153, 2501,  622, 2092,\n",
              "          9,  496,  136,  433,   11,   68,   68,   16,   89,   13,   68,\n",
              "         34, 7882,    9,  433,   11,   68,   68,   21,  388,  876,   13,\n",
              "         68,   40,   73,  252,  227, 3778, 1304,  103,  781,  351,  103,\n",
              "       7504,   15,   68,   68,   16,   89,   13,   68,   33,   97, 5790,\n",
              "         11, 3778,   11,   68,   68,   21,  388,  876,   13,   68,   21,\n",
              "        388,    9,  104,  330, 3317, 1177,  145, 3563, 1766,  103,   80,\n",
              "       1006,   11,   68,   68,   16,   89,   13,   68, 7797,  330,  486,\n",
              "          9,  153,  330,  486,   11,   68,   68,   21,  388,  876,   13,\n",
              "         68], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training The Model\n",
        "\n",
        "Now that we have our tokenized dataset, let's get to training our model!\n",
        "\n",
        "We have a lot of set-up to do before we click \"`.train()`\", so let's jump right into it!\n",
        "\n",
        "First, let's literally jump into the `nanoGPT` repository we cloned earlier."
      ],
      "metadata": {
        "id": "c0I3VrRC3XIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUU2jaalUdqm",
        "outputId": "064b7a0d-3172-44f7-c987-188efe60f84c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll do some critical imports."
      ],
      "metadata": {
        "id": "13p1e8sa3k0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# from the local repo\n",
        "from model import GPTConfig, GPT"
      ],
      "metadata": {
        "id": "weNR37BwUYNg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper-Parameters\n",
        "\n",
        "We have a laundry list of hyper-parameters to set up - let's walk through them and what they mean."
      ],
      "metadata": {
        "id": "kY_vWZG-3uM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I/O\n",
        "\n",
        "- `out_dir` - simple enough, this is the output directory where our checkpoints are saved"
      ],
      "metadata": {
        "id": "OykCjVQK5EX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = 'out'"
      ],
      "metadata": {
        "id": "viM3qlWt5PVS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialization\n",
        "\n",
        "Since we're training from scratch, we'll use `init_from = 'scratch'`."
      ],
      "metadata": {
        "id": "A5iwwrNL5H4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_from = 'scratch'"
      ],
      "metadata": {
        "id": "OK1z2m3C312T"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval and Logging\n",
        "\n",
        "- `eval_interval` - this is the number of steps between evaluation stages, we'll want to see this ~`250`. Our model will be incredibly prone to over-fitting, and this will let us monitor with relative frequency.\n",
        "- `log_interval` - this is how often our training progress will log. You can set this ~`10`. It's dealer's choice, really.\n",
        "- `eval_iters` - this is how *many* iterations we want to evaluate for.\n",
        "- `eval_only` - this would evaluate our model - but not train it. We'll leave this as `False` for now.\n",
        "- `always_save_checkpoint` - this will always save our most recent checkpoint, regardless of metrics. For this example, we'll set this to `True`."
      ],
      "metadata": {
        "id": "2YlolKOj4_dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval = 250\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "eval_only = False\n",
        "always_save_checkpoint = True"
      ],
      "metadata": {
        "id": "MbFN5Ltq4_mo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset\n",
        "\n",
        "We can set our dataset here - we'll use the one we created earlier!"
      ],
      "metadata": {
        "id": "a488zaF_4zQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'shakespeare'"
      ],
      "metadata": {
        "id": "_QC7vWXC40Hp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Typical Hyper-Parameters\n",
        "\n",
        "- `gradient_accumulation_steps` - we can use gradient accumulation to \"simulate\" larger batch sizes by combining multiple different optimization steps together, without needing the additional memory for large batch sizes. We don't need to worry so much about this for the toy problem - but this hyper-parameter can be configured for larger training runs. [Here](https://lightning.ai/blog/gradient-accumulation/) is some great reading on the topic.\n",
        "- `batch_size` - Typical batch_size - the larger the merrier (up to a point) we'll be using `16` to ensure we do not exceed the memory quota of our GPU.\n",
        "- `block_size` - this can be thought of as another term for the `context window` of our model. Since our model cannot take variable length inputs - we use this to set all inputs to our desired size. We'll use a value of `512` to ensure speedy training."
      ],
      "metadata": {
        "id": "XP9rBgGc426Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_accumulation_steps = 1\n",
        "batch_size = 16\n",
        "block_size = 512"
      ],
      "metadata": {
        "id": "EM_ybLPP43Pd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Architecture\n",
        "\n",
        "- `n_layer` - this is the number of decoder layers we will use in our model. More would be considered better (up to a point) and the original GPT-2 paper uses `12`, but we will be using a truncated `6` for ease and speed of training.\n",
        "- `n_head` - this is the number of attention heads in each decoder layer!\n",
        "- `n_embd` - this is the embedding dimension of our model, this is analagous to our `model_d` from the previous notebook.\n",
        "- `dropout` - this sets our dropout value, since our model is small and going to be extremely prone to overfitting, consider setting this at a fairly aggresive level (`0.2` was used in the example training found in the notebook`).\n",
        "- `bias` - wether or not to use bias inside the LayerNorm/Linear layers.\n",
        "\n",
        "> NOTE: You need to ensure your `n_embd` is cleanly divided by your `n_head`. That is to say:\n",
        ">\n",
        "> `n_embd % n_head == 0`."
      ],
      "metadata": {
        "id": "UZ-8bDIY45GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 516\n",
        "dropout = 0.2\n",
        "bias = False"
      ],
      "metadata": {
        "id": "gMyyDBxB6k4H"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizer Hyper-Parameters\n",
        "\n",
        "Basic Optimizer Hyper-Parameters:\n",
        "\n",
        "- `learning_rate` - it's our learning rate! We'll want to set this fairly high ~`1e-3` since we're training on such a small dataset.\n",
        "- `max_iters` - how many iterations do we train for. More iters means longer training times. Feel free to tinker with this value! `5000` is a great place to start.\n",
        "\n",
        "Learning Rate Decay Settings:\n",
        "\n",
        "- `decay_lr` - set decay flag\n",
        "- `weight_Decay` - how much to decay lr by\n",
        "- `lr_decay_iters` - should be set to ~max_iters.\n",
        "- `min_lr` - the minimum lr, should be ~ lr / 10\n",
        "\n",
        "Clipping and Warmup:\n",
        "\n",
        "- `grad_clip` - value to clip gradients to. useful for preventing vanishing gradients.\n",
        "- `warmup_iters` - how many iterations to warmup for. Warmup is useful to allow your training to slowly warmup. It will use a low lr for a number of steps to avoid any massive initial spikes. Since we're training a very small model - we can avoid using many wamrup steps.\n",
        "\n",
        "> NOTE: Many learnings taken from the [Chincilla paper](https://arxiv.org/pdf/2203.15556.pdf) for selecting default or appropriate values."
      ],
      "metadata": {
        "id": "3NWDTaAz7gwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adamw optimizer\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5_000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "\n",
        "# lr decay settings\n",
        "decay_lr = True\n",
        "weight_decay = 1e-1\n",
        "lr_decay_iters = 5_000\n",
        "min_lr = 1e-4\n",
        "\n",
        "# clipping and warmup\n",
        "grad_clip = 1.0\n",
        "warmup_iters = 100"
      ],
      "metadata": {
        "id": "qe-669jwUptI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These hyper-parameters are necessary to set given the task we're training and given the environment we're training in."
      ],
      "metadata": {
        "id": "ucldc4mz9yeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend = 'nccl'\n",
        "device = 'cuda'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "compile = True\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys}\n",
        "# -----------------------------------------------------------------------------\n",
        "master_process = True\n",
        "seed_offset = 0\n",
        "ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "os.makedirs(out_dir, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHiGlMOp8Nux",
        "outputId": "04285fbc-1f65-4dbe-d8be-2a9821674dce"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 8,192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch Settings\n",
        "\n",
        "We need to set a few `torch` settings, including the seed, to allow us to train correctly on our GPU.\n",
        "\n",
        "Not much is required for us to understand here - these are just necessary lines of code. Boilerplate."
      ],
      "metadata": {
        "id": "eKmdfbye-BNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "yh34QGD6VARU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader\n",
        "\n",
        "This block will:\n",
        "\n",
        "1. Set the data path\n",
        "2. Load the dataset we tokenized earlier from the `.bin` we saved\n",
        "3. Define a `get_batch` function that will return us a random section of our data as well as a the corresponding \"label\" for that data and move it to the GPU for easy use inside our training loop."
      ],
      "metadata": {
        "id": "gKeNwYaZ-Zoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join('/data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "tOjaPyJpVEgx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what an example of our batches would look like.\n",
        "\n",
        "To remind ourselves:\n",
        "\n",
        "- `train_data` - has ~2.9 million entries\n",
        "- `block_size` - is 512\n",
        "- `batch_size` - is 16"
      ],
      "metadata": {
        "id": "1Z7vMU34yRbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ix = torch.randint(len(train_data) - block_size, (batch_size,))\n",
        "x = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "y = torch.stack([torch.from_numpy((train_data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])"
      ],
      "metadata": {
        "id": "9_-Y5RZ-yX2a"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Our randomly selected indices were: {ix}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JDxXph4yh2g",
        "outputId": "d63541f7-533c-4259-b953-8138ccf3d5cb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our randomly selected indices were: tensor([ 99775, 155569, 263696,  32920,  52919, 231541, 153767, 229238, 136782,\n",
            "        263618,  39008,  14208,  39429, 189430, 194466,  76798])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The first 10 elements of `x` at the first randomly selected index is:\\n{x[0][:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAzHJH-kzK5E",
        "outputId": "30877a64-381c-462d-fe93-6ae5dfef4f8c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 10 elements of `x` at the first randomly selected index is:\n",
            "tensor([   68,    16,    81,  2358, 19949,   116,   172,  1280,     9,    68])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The first 10 elements of `y` at the first randomly selected index is:\\n{y[0][:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbaF6v8ezkWn",
        "outputId": "3b1b6c2e-3086-4a11-f41b-3a3e38b040cc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 10 elements of `y` at the first randomly selected index is:\n",
            "tensor([   16,    81,  2358, 19949,   116,   172,  1280,     9,    68,    16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the first component selects a random index from our training data (accounting for our block size)"
      ],
      "metadata": {
        "id": "N62oDfdWy0XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Initialization of Model\n",
        "\n",
        "Here we init our number of iterations as 0, and our best val loss as a very high number."
      ],
      "metadata": {
        "id": "EbDlW-68_atH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num = 0\n",
        "best_val_loss = 1e9"
      ],
      "metadata": {
        "id": "6hsepdVBVzQU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain our vocab size from our trained tokenizer."
      ],
      "metadata": {
        "id": "A4Uj9qBI_vXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = tokenizer.vocab_size\n",
        "meta_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m53DcCdFV0_a",
        "outputId": "c31d1db0-d7b5-436d-f191-8bead1e830a5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20099"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create our model args dict."
      ],
      "metadata": {
        "id": "V7bcNelYARmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout)"
      ],
      "metadata": {
        "id": "JfIWEbanV7ZS"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate our model with the provided `model_args`.\n",
        "\n",
        "These are derived from the hyper-parameters we set above."
      ],
      "metadata": {
        "id": "2WWcbkiCAUI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if init_from == 'scratch':\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xly4iA0V-vF",
        "outputId": "320f6756-9d02-438c-b554-a25f66bf3e1e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 29.55M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we go! If you used the default values - you should have a model with 29.55M parameters!\n",
        "\n",
        "Let's set our block_size to the correct size as determined in our configuration steps."
      ],
      "metadata": {
        "id": "BpViOsxLAl6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size"
      ],
      "metadata": {
        "id": "TrEawNxdWRhm"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can look at our model in all its glory!"
      ],
      "metadata": {
        "id": "eRgguPLKAuZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaE3KSTnAtJs",
        "outputId": "0191e448-4a1e-4333-83a0-1dc9338e7795"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(20099, 516)\n",
              "    (wpe): Embedding(512, 516)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=516, out_features=1548, bias=False)\n",
              "          (c_proj): Linear(in_features=516, out_features=516, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=516, out_features=2064, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=2064, out_features=516, bias=False)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=516, out_features=20099, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture Breakdown\n",
        "\n",
        "Now that we've built our model - let's look at each component and see how it works!"
      ],
      "metadata": {
        "id": "XiTlDpQQaq0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "We've already talked about tokenization - so let's expand on it a bit here:\n",
        "\n",
        "![image](https://i.imgur.com/oYuAayM.png)\n",
        "\n",
        "As you can see - our sentences are simply converted from a string to a sequence of numeric values - each which represents a word or subword!"
      ],
      "metadata": {
        "id": "SHwAK0lfaxtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Layers\n",
        "\n",
        "The first step will be do convert our tokenized sequence of inputs into an embedding vector. This allows use to understand a rich amount of information about input sequences and their semantic meanings.\n",
        "\n",
        "As the embedding layer will be training along side the rest of the model - it will allow us to have an excellent vector-representation of the tokens in our dataset.\n",
        "\n",
        "Let's see how it looks in code!"
      ],
      "metadata": {
        "id": "N_4mk2yKbFF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "```"
      ],
      "metadata": {
        "id": "ntf8i9EabKwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see a visual example!\n",
        "\n",
        "![image](https://i.imgur.com/Q8fiuw2.png)"
      ],
      "metadata": {
        "id": "qh9pH_4dbWan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding\n",
        "\n",
        "We need to impart information about where each token is in the sequence, but we aren't using any recurrence or convolutions - the easiest way to encode positional information is to inject positional information into our input embeddings.\n",
        "\n",
        "We can use two different methods to obtain our positional encodings - either a learned positional encoding, or a calculated positional encoding.\n",
        "\n",
        "Let's look at the code from the example used in `nanoGPT`, and also an example of code that uses a calculated positional encoding.\n",
        "\n"
      ],
      "metadata": {
        "id": "NNPEiOejbcBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "wpe = nn.Embedding(config.block_size, config.n_embd)\n",
        "```"
      ],
      "metadata": {
        "id": "AaJQVo0ibujN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A familiar face! In this case, our positional encodings are quite similar to our embeddings layer - but we're setting the dimensionality to care about the `block_size` (context window) as opposed to the `vocab_size`!"
      ],
      "metadata": {
        "id": "2QEmKK8Dbybh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another method we could use is the calculated positional embeddings - which could be implemented like so:"
      ],
      "metadata": {
        "id": "KRFQN34qfnZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, seq_len: int, dropout: float, verbose=False) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.verbose=verbose\n",
        "\n",
        "    positional_embeddings = torch.zeros(seq_len, d_model)\n",
        "    positional_sequence_vector = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    positional_model_vector = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    positional_embeddings[:, 0::2] = torch.sin(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings[:, 1::2] = torch.cos(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings = positional_embeddings.unsqueeze(0)\n",
        "\n",
        "    self.register_buffer('positional_embeddings', positional_embeddings)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.positional_embeddings[:, :x.shape[1], :]).requires_grad_(False)\n",
        "    if self.verbose:\n",
        "      print(f\"Positional Encodings (1st 5 elements): {x[0, :5]}\")\n",
        "    return self.dropout(x)\n",
        "```"
      ],
      "metadata": {
        "id": "wCrtpp6pfsgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we're implementing this set of equations from \"Attention is All You Need\"!\n",
        "\n",
        "![image](https://i.imgur.com/UhAJ0H0.png)"
      ],
      "metadata": {
        "id": "eODsnnADfxGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at this process visually to better understand:\n",
        "\n",
        "![image](https://i.imgur.com/JKwbESf.png)"
      ],
      "metadata": {
        "id": "k4Wh1cAxlY1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Mechanism\n",
        "\n",
        "Now we get to the \"meat\" of the model - the scaled dot-product attention mechanism!\n",
        "\n",
        "Let's first take a look at how it's implemented in code!"
      ],
      "metadata": {
        "id": "E3Zfr5tAgF33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "```"
      ],
      "metadata": {
        "id": "f5zI82M8gTfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's discuss a few key components and describe how they align with the attention mechanism outlined in the paper!\n",
        "\n",
        "First:\n",
        "\n",
        "```python\n",
        "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "```\n",
        "\n",
        "This is the where we find our \"scaled dot-product\" attention scores!\n",
        "\n",
        "Second:\n",
        "\n",
        "```python\n",
        "att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "```\n",
        "\n",
        "This is where we're applying our \"causal mask\" that ensures we're only ever looking back, and can't look forward past our current token.\n",
        "\n",
        "Third:\n",
        "\n",
        "```python\n",
        "att = F.softmax(att, dim=-1)\n",
        "```\n",
        "\n",
        "This is where we're computing the soft-max of our attention scores.\n",
        "\n",
        "Fourth:\n",
        "\n",
        "```python\n",
        "y = att @ v\n",
        "```\n",
        "\n",
        "This is where we're doing the last MatMul between our computed soft-max attention scores and our value vector!"
      ],
      "metadata": {
        "id": "_a7c1OnbgdW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://i.imgur.com/lJOWXcE.png)"
      ],
      "metadata": {
        "id": "oM2VtkhLkfaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Initializing Our Model"
      ],
      "metadata": {
        "id": "TOkvFxqClDDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up our GradScaler - more information on this process [here](https://pytorch.org/docs/stable/amp.html#gradient-scaling)."
      ],
      "metadata": {
        "id": "LzoEY6gcBOSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ],
      "metadata": {
        "id": "BNUThRt4WT5H"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set up our optimizer below. Be sure to include the correct values. You can check the `model.py` file for more information on what is expected in the `configure_optimizers` method [here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L263C85-L263C85)."
      ],
      "metadata": {
        "id": "6Zs5Hcf9BBUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay,\n",
        "    learning_rate,\n",
        "    (beta1, beta2),\n",
        "    device_type\n",
        ")\n",
        "\n",
        "checkpoint = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YesGeUnoWViL",
        "outputId": "0b3d3727-1945-4091-ea99-90125bce1c22"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 26, with 29,805,708 parameters\n",
            "num non-decayed parameter tensors: 13, with 6,708 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compile our model!\n",
        "\n",
        "If you're using the T4 or V100 instance of Colab - this will not provide a signficant speed-up, but if you're using Ampere architecture (A100) you should notice a significant difference between the compiled and uncompiled model.\n",
        "\n",
        "Read more about `torch.compile()` [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)."
      ],
      "metadata": {
        "id": "ZF5YWJoKB4og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0FNU0T0WXdI",
        "outputId": "544c2d60-fa00-440c-d5f0-c399f86afcc0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up our loss estimation function here, which will help us estimate an arbitrarily accurate loss over either training or validation data by using many batches.\n",
        "\n",
        "You'll notice that we quickly convert the model into `.eval()` model and then back to `.train()` mode."
      ],
      "metadata": {
        "id": "p6lRcVsZCXRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "lUB5zVLVWbhM"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating our LR Scheduler\n",
        "\n",
        "Beyond just slowly reducing our learning rate over time - we can use an LR Scheduler to allow us to move our learning according to a desired pattern.\n",
        "\n",
        "We will use a \"cosine with warmup\" schedule and our learning rate, thusly, will follow this pattern:\n",
        "\n",
        "![img](https://i.imgur.com/KoFEl0b.png)\n",
        "\n",
        "There are many different schedulers, and many different ways to handle learning rate, and you can read about just a few of them [here](https://d2l.ai/chapter_optimization/lr-scheduler.html)!"
      ],
      "metadata": {
        "id": "fLsOpaACDDkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ],
      "metadata": {
        "id": "7-mNpWBSWdHh"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to set some specific values in our env to allow training in Colab."
      ],
      "metadata": {
        "id": "cqFePCZmE1Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7nDL6s4YT6E",
        "outputId": "8f290204-4666-4fd1-b150-661b8ee70abf"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Training Loop\n",
        "\n",
        "Now we can finally grab our first batch and set our initial time to calculate how long our iterations are taking!"
      ],
      "metadata": {
        "id": "Nhqmxeo0Eg0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch('train')\n",
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model = model\n",
        "running_mfu = -1.0 # model flops utilization\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHbyEapRWmpc",
        "outputId": "130222a6-c0b1-4a04-b384-f150fc5cb2e4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 9.9354, val loss 9.9274\n",
            "iter 0: loss 9.9364, time 53614.57ms, mfu -100.00%\n",
            "iter 10: loss 8.3524, time 80.05ms, mfu 6.44%\n",
            "iter 20: loss 7.3786, time 81.01ms, mfu 6.43%\n",
            "iter 30: loss 6.4343, time 80.59ms, mfu 6.43%\n",
            "iter 40: loss 5.8000, time 82.99ms, mfu 6.41%\n",
            "iter 50: loss 5.7333, time 81.11ms, mfu 6.40%\n",
            "iter 60: loss 5.4813, time 80.52ms, mfu 6.40%\n",
            "iter 70: loss 5.2254, time 80.40ms, mfu 6.40%\n",
            "iter 80: loss 5.1006, time 82.27ms, mfu 6.39%\n",
            "iter 90: loss 4.9886, time 82.81ms, mfu 6.37%\n",
            "iter 100: loss 4.6134, time 81.06ms, mfu 6.37%\n",
            "iter 110: loss 4.6059, time 80.93ms, mfu 6.37%\n",
            "iter 120: loss 4.5445, time 80.85ms, mfu 6.37%\n",
            "iter 130: loss 4.5266, time 81.48ms, mfu 6.37%\n",
            "iter 140: loss 4.4523, time 81.60ms, mfu 6.36%\n",
            "iter 150: loss 4.4700, time 82.03ms, mfu 6.35%\n",
            "iter 160: loss 4.4381, time 81.78ms, mfu 6.35%\n",
            "iter 170: loss 4.3046, time 81.24ms, mfu 6.35%\n",
            "iter 180: loss 4.4187, time 81.31ms, mfu 6.35%\n",
            "iter 190: loss 4.2507, time 81.24ms, mfu 6.35%\n",
            "iter 200: loss 4.2191, time 81.66ms, mfu 6.34%\n",
            "iter 210: loss 4.2526, time 81.41ms, mfu 6.34%\n",
            "iter 220: loss 4.2558, time 81.21ms, mfu 6.34%\n",
            "iter 230: loss 4.2027, time 81.70ms, mfu 6.34%\n",
            "iter 240: loss 4.0951, time 82.22ms, mfu 6.33%\n",
            "step 250: train loss 3.9741, val loss 4.9423\n",
            "saving checkpoint to out\n",
            "iter 250: loss 4.0227, time 9179.15ms, mfu 5.70%\n",
            "iter 260: loss 4.0718, time 81.53ms, mfu 5.77%\n",
            "iter 270: loss 3.8701, time 82.31ms, mfu 5.82%\n",
            "iter 280: loss 4.0008, time 82.88ms, mfu 5.86%\n",
            "iter 290: loss 3.8651, time 82.68ms, mfu 5.89%\n",
            "iter 300: loss 3.8872, time 82.74ms, mfu 5.93%\n",
            "iter 310: loss 3.9772, time 82.67ms, mfu 5.96%\n",
            "iter 320: loss 3.7900, time 82.61ms, mfu 5.99%\n",
            "iter 330: loss 3.8061, time 82.60ms, mfu 6.01%\n",
            "iter 340: loss 3.9023, time 83.40ms, mfu 6.03%\n",
            "iter 350: loss 3.7281, time 82.82ms, mfu 6.05%\n",
            "iter 360: loss 3.6269, time 83.38ms, mfu 6.06%\n",
            "iter 370: loss 3.7402, time 82.50ms, mfu 6.08%\n",
            "iter 380: loss 3.7060, time 82.64ms, mfu 6.10%\n",
            "iter 390: loss 3.6848, time 83.13ms, mfu 6.11%\n",
            "iter 400: loss 3.4884, time 83.36ms, mfu 6.11%\n",
            "iter 410: loss 3.6630, time 82.70ms, mfu 6.13%\n",
            "iter 420: loss 3.5259, time 83.04ms, mfu 6.13%\n",
            "iter 430: loss 3.6214, time 83.43ms, mfu 6.14%\n",
            "iter 440: loss 3.5301, time 82.68ms, mfu 6.15%\n",
            "iter 450: loss 3.5459, time 83.26ms, mfu 6.15%\n",
            "iter 460: loss 3.5012, time 83.77ms, mfu 6.15%\n",
            "iter 470: loss 3.5692, time 83.11ms, mfu 6.16%\n",
            "iter 480: loss 3.3643, time 83.44ms, mfu 6.16%\n",
            "iter 490: loss 3.4891, time 83.18ms, mfu 6.16%\n",
            "step 500: train loss 3.3472, val loss 5.1834\n",
            "saving checkpoint to out\n",
            "iter 500: loss 3.3770, time 9648.01ms, mfu 5.55%\n",
            "iter 510: loss 3.5584, time 82.49ms, mfu 5.62%\n",
            "iter 520: loss 3.4349, time 82.85ms, mfu 5.68%\n",
            "iter 530: loss 3.2923, time 83.32ms, mfu 5.73%\n",
            "iter 540: loss 3.4523, time 83.90ms, mfu 5.77%\n",
            "iter 550: loss 3.2899, time 85.95ms, mfu 5.80%\n",
            "iter 560: loss 3.2029, time 85.25ms, mfu 5.82%\n",
            "iter 570: loss 3.1203, time 83.39ms, mfu 5.86%\n",
            "iter 580: loss 3.2445, time 83.71ms, mfu 5.89%\n",
            "iter 590: loss 3.1629, time 84.08ms, mfu 5.91%\n",
            "iter 600: loss 3.1845, time 83.65ms, mfu 5.94%\n",
            "iter 610: loss 3.1560, time 84.30ms, mfu 5.95%\n",
            "iter 620: loss 3.1272, time 84.00ms, mfu 5.97%\n",
            "iter 630: loss 3.2094, time 83.81ms, mfu 5.99%\n",
            "iter 640: loss 3.1760, time 83.70ms, mfu 6.01%\n",
            "iter 650: loss 3.1241, time 83.93ms, mfu 6.02%\n",
            "iter 660: loss 3.1054, time 84.04ms, mfu 6.03%\n",
            "iter 670: loss 3.1122, time 84.00ms, mfu 6.04%\n",
            "iter 680: loss 2.9601, time 85.06ms, mfu 6.04%\n",
            "iter 690: loss 3.0470, time 83.70ms, mfu 6.06%\n",
            "iter 700: loss 2.9280, time 83.39ms, mfu 6.07%\n",
            "iter 710: loss 3.0271, time 83.53ms, mfu 6.08%\n",
            "iter 720: loss 2.7817, time 83.72ms, mfu 6.09%\n",
            "iter 730: loss 2.9472, time 83.57ms, mfu 6.09%\n",
            "iter 740: loss 2.8712, time 84.01ms, mfu 6.10%\n",
            "step 750: train loss 2.6746, val loss 5.5042\n",
            "saving checkpoint to out\n",
            "iter 750: loss 2.8118, time 9519.20ms, mfu 5.49%\n",
            "iter 760: loss 2.8176, time 81.80ms, mfu 5.57%\n",
            "iter 770: loss 2.7164, time 82.70ms, mfu 5.64%\n",
            "iter 780: loss 2.7605, time 82.89ms, mfu 5.70%\n",
            "iter 790: loss 2.6811, time 84.81ms, mfu 5.74%\n",
            "iter 800: loss 2.9365, time 83.56ms, mfu 5.78%\n",
            "iter 810: loss 2.7463, time 83.48ms, mfu 5.82%\n",
            "iter 820: loss 2.7081, time 82.44ms, mfu 5.86%\n",
            "iter 830: loss 2.5945, time 82.82ms, mfu 5.90%\n",
            "iter 840: loss 2.6910, time 83.30ms, mfu 5.93%\n",
            "iter 850: loss 2.8498, time 89.80ms, mfu 5.91%\n",
            "iter 860: loss 2.6224, time 83.56ms, mfu 5.93%\n",
            "iter 870: loss 2.4935, time 83.26ms, mfu 5.96%\n",
            "iter 880: loss 2.5080, time 82.97ms, mfu 5.99%\n",
            "iter 890: loss 2.5122, time 82.37ms, mfu 6.01%\n",
            "iter 900: loss 2.3901, time 83.37ms, mfu 6.03%\n",
            "iter 910: loss 2.5345, time 82.96ms, mfu 6.05%\n",
            "iter 920: loss 2.4420, time 82.52ms, mfu 6.07%\n",
            "iter 930: loss 2.3574, time 82.80ms, mfu 6.08%\n",
            "iter 940: loss 2.4878, time 82.81ms, mfu 6.10%\n",
            "iter 950: loss 2.4302, time 82.40ms, mfu 6.11%\n",
            "iter 960: loss 2.1073, time 82.99ms, mfu 6.12%\n",
            "iter 970: loss 2.2080, time 82.72ms, mfu 6.13%\n",
            "iter 980: loss 2.2807, time 82.91ms, mfu 6.14%\n",
            "iter 990: loss 2.1204, time 82.65ms, mfu 6.15%\n",
            "step 1000: train loss 1.9160, val loss 5.8955\n",
            "saving checkpoint to out\n",
            "iter 1000: loss 2.4240, time 9595.79ms, mfu 5.54%\n",
            "iter 1010: loss 2.1726, time 81.77ms, mfu 5.62%\n",
            "iter 1020: loss 2.2638, time 81.48ms, mfu 5.69%\n",
            "iter 1030: loss 2.3357, time 83.55ms, mfu 5.74%\n",
            "iter 1040: loss 1.9610, time 84.82ms, mfu 5.77%\n",
            "iter 1050: loss 1.9788, time 83.64ms, mfu 5.81%\n",
            "iter 1060: loss 2.0857, time 83.97ms, mfu 5.84%\n",
            "iter 1070: loss 1.9608, time 82.26ms, mfu 5.89%\n",
            "iter 1080: loss 2.1539, time 82.55ms, mfu 5.92%\n",
            "iter 1090: loss 1.8106, time 83.39ms, mfu 5.95%\n",
            "iter 1100: loss 1.9787, time 83.51ms, mfu 5.97%\n",
            "iter 1110: loss 1.9831, time 83.59ms, mfu 5.99%\n",
            "iter 1120: loss 1.9085, time 83.61ms, mfu 6.01%\n",
            "iter 1130: loss 1.8058, time 83.37ms, mfu 6.02%\n",
            "iter 1140: loss 1.8245, time 82.75ms, mfu 6.04%\n",
            "iter 1150: loss 1.8682, time 83.33ms, mfu 6.06%\n",
            "iter 1160: loss 1.9459, time 83.48ms, mfu 6.07%\n",
            "iter 1170: loss 1.9195, time 83.53ms, mfu 6.08%\n",
            "iter 1180: loss 1.7062, time 83.17ms, mfu 6.09%\n",
            "iter 1190: loss 1.8716, time 83.77ms, mfu 6.10%\n",
            "iter 1200: loss 1.8251, time 83.89ms, mfu 6.10%\n",
            "iter 1210: loss 1.8868, time 83.57ms, mfu 6.11%\n",
            "iter 1220: loss 1.7875, time 83.40ms, mfu 6.12%\n",
            "iter 1230: loss 1.7445, time 83.60ms, mfu 6.12%\n",
            "iter 1240: loss 1.6386, time 83.63ms, mfu 6.13%\n",
            "step 1250: train loss 1.2943, val loss 6.3647\n",
            "saving checkpoint to out\n",
            "iter 1250: loss 1.6822, time 9649.77ms, mfu 5.52%\n",
            "iter 1260: loss 1.6370, time 81.76ms, mfu 5.60%\n",
            "iter 1270: loss 1.7877, time 82.23ms, mfu 5.66%\n",
            "iter 1280: loss 1.5519, time 83.20ms, mfu 5.72%\n",
            "iter 1290: loss 1.5962, time 83.69ms, mfu 5.76%\n",
            "iter 1300: loss 1.6450, time 84.21ms, mfu 5.80%\n",
            "iter 1310: loss 1.6449, time 83.84ms, mfu 5.83%\n",
            "iter 1320: loss 1.5386, time 83.84ms, mfu 5.86%\n",
            "iter 1330: loss 1.4867, time 83.69ms, mfu 5.89%\n",
            "iter 1340: loss 1.4604, time 83.52ms, mfu 5.92%\n",
            "iter 1350: loss 1.5904, time 84.11ms, mfu 5.94%\n",
            "iter 1360: loss 1.5929, time 83.82ms, mfu 5.96%\n",
            "iter 1370: loss 1.4449, time 83.77ms, mfu 5.98%\n",
            "iter 1380: loss 1.5829, time 83.68ms, mfu 6.00%\n",
            "iter 1390: loss 1.5169, time 83.11ms, mfu 6.02%\n",
            "iter 1400: loss 1.4379, time 83.24ms, mfu 6.04%\n",
            "iter 1410: loss 1.3940, time 83.92ms, mfu 6.05%\n",
            "iter 1420: loss 1.4960, time 83.70ms, mfu 6.06%\n",
            "iter 1430: loss 1.4375, time 84.29ms, mfu 6.06%\n",
            "iter 1440: loss 1.3734, time 84.02ms, mfu 6.07%\n",
            "iter 1450: loss 1.5756, time 84.01ms, mfu 6.08%\n",
            "iter 1460: loss 1.3066, time 84.00ms, mfu 6.08%\n",
            "iter 1470: loss 1.3961, time 83.75ms, mfu 6.09%\n",
            "iter 1480: loss 1.3089, time 83.87ms, mfu 6.10%\n",
            "iter 1490: loss 1.4488, time 83.80ms, mfu 6.10%\n",
            "step 1500: train loss 0.8999, val loss 6.8685\n",
            "saving checkpoint to out\n",
            "iter 1500: loss 1.3489, time 9687.84ms, mfu 5.50%\n",
            "iter 1510: loss 1.2922, time 82.10ms, mfu 5.57%\n",
            "iter 1520: loss 1.3764, time 83.04ms, mfu 5.64%\n",
            "iter 1530: loss 1.2059, time 83.21ms, mfu 5.69%\n",
            "iter 1540: loss 1.2381, time 84.91ms, mfu 5.73%\n",
            "iter 1550: loss 1.2319, time 85.22ms, mfu 5.76%\n",
            "iter 1560: loss 1.2164, time 83.89ms, mfu 5.80%\n",
            "iter 1570: loss 1.2159, time 83.10ms, mfu 5.84%\n",
            "iter 1580: loss 1.2052, time 83.14ms, mfu 5.88%\n",
            "iter 1590: loss 1.1849, time 83.52ms, mfu 5.91%\n",
            "iter 1600: loss 1.2075, time 83.96ms, mfu 5.93%\n",
            "iter 1610: loss 1.2534, time 84.29ms, mfu 5.95%\n",
            "iter 1620: loss 1.2164, time 83.20ms, mfu 5.97%\n",
            "iter 1630: loss 1.1932, time 83.43ms, mfu 5.99%\n",
            "iter 1640: loss 1.0847, time 83.42ms, mfu 6.01%\n",
            "iter 1650: loss 1.1350, time 83.44ms, mfu 6.03%\n",
            "iter 1660: loss 1.1385, time 83.88ms, mfu 6.04%\n",
            "iter 1670: loss 1.1101, time 83.16ms, mfu 6.06%\n",
            "iter 1680: loss 1.1245, time 83.50ms, mfu 6.07%\n",
            "iter 1690: loss 1.2342, time 83.73ms, mfu 6.08%\n",
            "iter 1700: loss 1.1183, time 83.54ms, mfu 6.09%\n",
            "iter 1710: loss 0.9988, time 83.88ms, mfu 6.09%\n",
            "iter 1720: loss 1.0673, time 83.56ms, mfu 6.10%\n",
            "iter 1730: loss 1.0905, time 83.32ms, mfu 6.11%\n",
            "iter 1740: loss 1.0423, time 83.13ms, mfu 6.12%\n",
            "step 1750: train loss 0.6675, val loss 7.2028\n",
            "saving checkpoint to out\n",
            "iter 1750: loss 1.0396, time 9509.61ms, mfu 5.51%\n",
            "iter 1760: loss 1.0687, time 82.56ms, mfu 5.58%\n",
            "iter 1770: loss 1.0728, time 82.29ms, mfu 5.65%\n",
            "iter 1780: loss 0.9996, time 83.48ms, mfu 5.70%\n",
            "iter 1790: loss 1.0884, time 84.00ms, mfu 5.75%\n",
            "iter 1800: loss 1.0742, time 83.93ms, mfu 5.79%\n",
            "iter 1810: loss 0.9745, time 83.86ms, mfu 5.82%\n",
            "iter 1820: loss 1.0856, time 83.56ms, mfu 5.86%\n",
            "iter 1830: loss 0.9854, time 82.62ms, mfu 5.90%\n",
            "iter 1840: loss 1.0393, time 83.51ms, mfu 5.92%\n",
            "iter 1850: loss 0.9904, time 83.37ms, mfu 5.95%\n",
            "iter 1860: loss 0.9986, time 83.87ms, mfu 5.97%\n",
            "iter 1870: loss 0.8981, time 83.35ms, mfu 5.99%\n",
            "iter 1880: loss 0.9681, time 83.65ms, mfu 6.01%\n",
            "iter 1890: loss 0.9389, time 83.33ms, mfu 6.03%\n",
            "iter 1900: loss 0.9734, time 83.38ms, mfu 6.04%\n",
            "iter 1910: loss 0.9256, time 83.90ms, mfu 6.05%\n",
            "iter 1920: loss 0.9599, time 83.48ms, mfu 6.06%\n",
            "iter 1930: loss 0.9236, time 83.60ms, mfu 6.07%\n",
            "iter 1940: loss 0.9810, time 83.27ms, mfu 6.09%\n",
            "iter 1950: loss 0.9010, time 83.18ms, mfu 6.10%\n",
            "iter 1960: loss 0.9375, time 83.67ms, mfu 6.10%\n",
            "iter 1970: loss 0.9002, time 83.65ms, mfu 6.11%\n",
            "iter 1980: loss 0.8990, time 83.74ms, mfu 6.11%\n",
            "iter 1990: loss 0.8952, time 83.25ms, mfu 6.12%\n",
            "step 2000: train loss 0.4979, val loss 7.6217\n",
            "saving checkpoint to out\n",
            "iter 2000: loss 0.8769, time 9705.25ms, mfu 5.51%\n",
            "iter 2010: loss 0.8844, time 81.72ms, mfu 5.59%\n",
            "iter 2020: loss 0.8907, time 82.27ms, mfu 5.66%\n",
            "iter 2030: loss 0.8259, time 83.37ms, mfu 5.71%\n",
            "iter 2040: loss 0.7842, time 84.60ms, mfu 5.75%\n",
            "iter 2050: loss 0.8156, time 83.90ms, mfu 5.79%\n",
            "iter 2060: loss 0.8257, time 84.10ms, mfu 5.82%\n",
            "iter 2070: loss 0.7809, time 83.50ms, mfu 5.86%\n",
            "iter 2080: loss 0.8171, time 82.93ms, mfu 5.89%\n",
            "iter 2090: loss 0.8022, time 83.35ms, mfu 5.92%\n",
            "iter 2100: loss 0.8214, time 83.54ms, mfu 5.95%\n",
            "iter 2110: loss 0.8370, time 83.15ms, mfu 5.97%\n",
            "iter 2120: loss 0.8420, time 83.86ms, mfu 5.99%\n",
            "iter 2130: loss 0.8190, time 83.85ms, mfu 6.01%\n",
            "iter 2140: loss 0.8551, time 83.13ms, mfu 6.03%\n",
            "iter 2150: loss 0.7380, time 83.33ms, mfu 6.04%\n",
            "iter 2160: loss 0.7554, time 83.31ms, mfu 6.06%\n",
            "iter 2170: loss 0.8577, time 82.94ms, mfu 6.07%\n",
            "iter 2180: loss 0.8411, time 83.50ms, mfu 6.08%\n",
            "iter 2190: loss 0.7797, time 83.17ms, mfu 6.09%\n",
            "iter 2200: loss 0.7257, time 83.59ms, mfu 6.10%\n",
            "iter 2210: loss 0.7441, time 83.55ms, mfu 6.11%\n",
            "iter 2220: loss 0.7511, time 83.45ms, mfu 6.11%\n",
            "iter 2230: loss 0.7240, time 83.61ms, mfu 6.12%\n",
            "iter 2240: loss 0.7280, time 83.23ms, mfu 6.13%\n",
            "step 2250: train loss 0.3712, val loss 7.8820\n",
            "saving checkpoint to out\n",
            "iter 2250: loss 0.7327, time 9475.43ms, mfu 5.52%\n",
            "iter 2260: loss 0.7483, time 82.37ms, mfu 5.59%\n",
            "iter 2270: loss 0.6993, time 82.64ms, mfu 5.66%\n",
            "iter 2280: loss 0.6624, time 83.38ms, mfu 5.71%\n",
            "iter 2290: loss 0.7003, time 84.68ms, mfu 5.75%\n",
            "iter 2300: loss 0.7570, time 84.41ms, mfu 5.78%\n",
            "iter 2310: loss 0.6984, time 83.61ms, mfu 5.82%\n",
            "iter 2320: loss 0.6849, time 83.02ms, mfu 5.86%\n",
            "iter 2330: loss 0.6745, time 83.06ms, mfu 5.90%\n",
            "iter 2340: loss 0.7014, time 83.96ms, mfu 5.92%\n",
            "iter 2350: loss 0.7071, time 83.62ms, mfu 5.94%\n",
            "iter 2360: loss 0.6994, time 83.95ms, mfu 5.96%\n",
            "iter 2370: loss 0.6892, time 83.48ms, mfu 5.98%\n",
            "iter 2380: loss 0.6480, time 83.39ms, mfu 6.00%\n",
            "iter 2390: loss 0.7045, time 83.40ms, mfu 6.02%\n",
            "iter 2400: loss 0.6491, time 83.25ms, mfu 6.04%\n",
            "iter 2410: loss 0.7761, time 83.85ms, mfu 6.05%\n",
            "iter 2420: loss 0.6329, time 83.13ms, mfu 6.06%\n",
            "iter 2430: loss 0.6297, time 84.13ms, mfu 6.07%\n",
            "iter 2440: loss 0.6542, time 83.34ms, mfu 6.08%\n",
            "iter 2450: loss 0.6208, time 83.39ms, mfu 6.09%\n",
            "iter 2460: loss 0.5622, time 83.21ms, mfu 6.10%\n",
            "iter 2470: loss 0.6306, time 83.85ms, mfu 6.11%\n",
            "iter 2480: loss 0.6656, time 83.72ms, mfu 6.11%\n",
            "iter 2490: loss 0.6169, time 82.67ms, mfu 6.12%\n",
            "step 2500: train loss 0.2956, val loss 8.1788\n",
            "saving checkpoint to out\n",
            "iter 2500: loss 0.6255, time 9683.82ms, mfu 5.52%\n",
            "iter 2510: loss 0.5802, time 81.91ms, mfu 5.59%\n",
            "iter 2520: loss 0.5910, time 78.38ms, mfu 5.69%\n",
            "iter 2530: loss 0.5419, time 83.10ms, mfu 5.74%\n",
            "iter 2540: loss 0.6475, time 84.53ms, mfu 5.78%\n",
            "iter 2550: loss 0.5793, time 84.70ms, mfu 5.81%\n",
            "iter 2560: loss 0.5728, time 82.85ms, mfu 5.85%\n",
            "iter 2570: loss 0.6273, time 82.61ms, mfu 5.89%\n",
            "iter 2580: loss 0.6132, time 83.37ms, mfu 5.92%\n",
            "iter 2590: loss 0.5882, time 83.70ms, mfu 5.94%\n",
            "iter 2600: loss 0.5676, time 83.55ms, mfu 5.97%\n",
            "iter 2610: loss 0.5808, time 84.37ms, mfu 5.98%\n",
            "iter 2620: loss 0.5460, time 83.57ms, mfu 6.00%\n",
            "iter 2630: loss 0.5600, time 83.31ms, mfu 6.02%\n",
            "iter 2640: loss 0.5401, time 83.32ms, mfu 6.03%\n",
            "iter 2650: loss 0.5450, time 83.68ms, mfu 6.05%\n",
            "iter 2660: loss 0.5804, time 83.55ms, mfu 6.06%\n",
            "iter 2670: loss 0.5394, time 83.78ms, mfu 6.07%\n",
            "iter 2680: loss 0.5755, time 83.96ms, mfu 6.08%\n",
            "iter 2690: loss 0.5682, time 83.51ms, mfu 6.09%\n",
            "iter 2700: loss 0.5362, time 83.50ms, mfu 6.09%\n",
            "iter 2710: loss 0.5350, time 84.20ms, mfu 6.10%\n",
            "iter 2720: loss 0.5224, time 83.54ms, mfu 6.10%\n",
            "iter 2730: loss 0.4923, time 83.07ms, mfu 6.11%\n",
            "iter 2740: loss 0.4815, time 83.65ms, mfu 6.12%\n",
            "step 2750: train loss 0.2236, val loss 8.3082\n",
            "saving checkpoint to out\n",
            "iter 2750: loss 0.5356, time 9418.40ms, mfu 5.51%\n",
            "iter 2760: loss 0.5551, time 82.21ms, mfu 5.59%\n",
            "iter 2770: loss 0.4808, time 82.71ms, mfu 5.65%\n",
            "iter 2780: loss 0.5308, time 82.42ms, mfu 5.71%\n",
            "iter 2790: loss 0.4793, time 84.39ms, mfu 5.75%\n",
            "iter 2800: loss 0.4938, time 84.11ms, mfu 5.79%\n",
            "iter 2810: loss 0.5508, time 83.56ms, mfu 5.83%\n",
            "iter 2820: loss 0.4571, time 82.84ms, mfu 5.87%\n",
            "iter 2830: loss 0.4455, time 83.17ms, mfu 5.90%\n",
            "iter 2840: loss 0.5013, time 83.14ms, mfu 5.93%\n",
            "iter 2850: loss 0.4482, time 84.02ms, mfu 5.95%\n",
            "iter 2860: loss 0.4406, time 83.94ms, mfu 5.97%\n",
            "iter 2870: loss 0.4619, time 83.66ms, mfu 5.99%\n",
            "iter 2880: loss 0.4563, time 83.06ms, mfu 6.01%\n",
            "iter 2890: loss 0.4512, time 83.73ms, mfu 6.03%\n",
            "iter 2900: loss 0.4474, time 83.71ms, mfu 6.04%\n",
            "iter 2910: loss 0.4294, time 83.15ms, mfu 6.05%\n",
            "iter 2920: loss 0.4337, time 83.34ms, mfu 6.07%\n",
            "iter 2930: loss 0.4702, time 83.69ms, mfu 6.08%\n",
            "iter 2940: loss 0.4412, time 83.19ms, mfu 6.09%\n",
            "iter 2950: loss 0.4618, time 83.39ms, mfu 6.10%\n",
            "iter 2960: loss 0.4192, time 83.61ms, mfu 6.10%\n",
            "iter 2970: loss 0.4372, time 83.57ms, mfu 6.11%\n",
            "iter 2980: loss 0.4299, time 83.51ms, mfu 6.12%\n",
            "iter 2990: loss 0.4230, time 84.21ms, mfu 6.12%\n",
            "step 3000: train loss 0.1794, val loss 8.4446\n",
            "saving checkpoint to out\n",
            "iter 3000: loss 0.4316, time 9592.32ms, mfu 5.51%\n",
            "iter 3010: loss 0.4538, time 82.35ms, mfu 5.59%\n",
            "iter 3020: loss 0.4177, time 82.47ms, mfu 5.65%\n",
            "iter 3030: loss 0.4132, time 83.04ms, mfu 5.71%\n",
            "iter 3040: loss 0.4586, time 84.06ms, mfu 5.75%\n",
            "iter 3050: loss 0.4258, time 84.99ms, mfu 5.78%\n",
            "iter 3060: loss 0.4119, time 83.76ms, mfu 5.82%\n",
            "iter 3070: loss 0.4024, time 83.62ms, mfu 5.85%\n",
            "iter 3080: loss 0.4063, time 83.24ms, mfu 5.89%\n",
            "iter 3090: loss 0.3787, time 83.66ms, mfu 5.91%\n",
            "iter 3100: loss 0.4143, time 82.99ms, mfu 5.94%\n",
            "iter 3110: loss 0.4240, time 83.45ms, mfu 5.97%\n",
            "iter 3120: loss 0.4292, time 83.52ms, mfu 5.99%\n",
            "iter 3130: loss 0.4241, time 83.51ms, mfu 6.01%\n",
            "iter 3140: loss 0.4298, time 83.41ms, mfu 6.02%\n",
            "iter 3150: loss 0.4174, time 83.61ms, mfu 6.04%\n",
            "iter 3160: loss 0.3841, time 84.09ms, mfu 6.05%\n",
            "iter 3170: loss 0.4123, time 83.82ms, mfu 6.06%\n",
            "iter 3180: loss 0.3594, time 84.99ms, mfu 6.06%\n",
            "iter 3190: loss 0.3956, time 83.97ms, mfu 6.07%\n",
            "iter 3200: loss 0.3848, time 83.74ms, mfu 6.07%\n",
            "iter 3210: loss 0.4276, time 83.57ms, mfu 6.08%\n",
            "iter 3220: loss 0.3302, time 83.29ms, mfu 6.09%\n",
            "iter 3230: loss 0.4145, time 81.75ms, mfu 6.12%\n",
            "iter 3240: loss 0.3648, time 83.31ms, mfu 6.12%\n",
            "step 3250: train loss 0.1468, val loss 8.6951\n",
            "saving checkpoint to out\n",
            "iter 3250: loss 0.4020, time 9606.08ms, mfu 5.52%\n",
            "iter 3260: loss 0.3461, time 82.36ms, mfu 5.59%\n",
            "iter 3270: loss 0.3450, time 82.54ms, mfu 5.66%\n",
            "iter 3280: loss 0.3736, time 83.42ms, mfu 5.71%\n",
            "iter 3290: loss 0.3624, time 83.77ms, mfu 5.75%\n",
            "iter 3300: loss 0.3886, time 84.17ms, mfu 5.79%\n",
            "iter 3310: loss 0.3618, time 83.62ms, mfu 5.83%\n",
            "iter 3320: loss 0.3683, time 83.31ms, mfu 5.86%\n",
            "iter 3330: loss 0.3653, time 83.20ms, mfu 5.90%\n",
            "iter 3340: loss 0.3698, time 83.02ms, mfu 5.93%\n",
            "iter 3350: loss 0.3565, time 83.58ms, mfu 5.95%\n",
            "iter 3360: loss 0.3444, time 83.87ms, mfu 5.97%\n",
            "iter 3370: loss 0.3608, time 83.11ms, mfu 5.99%\n",
            "iter 3380: loss 0.3654, time 84.30ms, mfu 6.01%\n",
            "iter 3390: loss 0.3400, time 83.25ms, mfu 6.02%\n",
            "iter 3400: loss 0.3202, time 83.72ms, mfu 6.04%\n",
            "iter 3410: loss 0.3643, time 83.53ms, mfu 6.05%\n",
            "iter 3420: loss 0.3451, time 83.22ms, mfu 6.07%\n",
            "iter 3430: loss 0.3300, time 83.51ms, mfu 6.08%\n",
            "iter 3440: loss 0.3038, time 83.25ms, mfu 6.09%\n",
            "iter 3450: loss 0.3264, time 83.66ms, mfu 6.09%\n",
            "iter 3460: loss 0.3497, time 83.59ms, mfu 6.10%\n",
            "iter 3470: loss 0.3087, time 83.52ms, mfu 6.11%\n",
            "iter 3480: loss 0.3089, time 83.48ms, mfu 6.12%\n",
            "iter 3490: loss 0.3229, time 83.53ms, mfu 6.12%\n",
            "step 3500: train loss 0.1221, val loss 8.8094\n",
            "saving checkpoint to out\n",
            "iter 3500: loss 0.3011, time 9504.86ms, mfu 5.51%\n",
            "iter 3510: loss 0.3194, time 81.94ms, mfu 5.59%\n",
            "iter 3520: loss 0.3472, time 82.49ms, mfu 5.66%\n",
            "iter 3530: loss 0.2866, time 83.45ms, mfu 5.71%\n",
            "iter 3540: loss 0.3131, time 84.12ms, mfu 5.75%\n",
            "iter 3550: loss 0.3067, time 83.96ms, mfu 5.79%\n",
            "iter 3560: loss 0.3165, time 83.28ms, mfu 5.83%\n",
            "iter 3570: loss 0.3217, time 83.06ms, mfu 5.87%\n",
            "iter 3580: loss 0.3327, time 83.63ms, mfu 5.90%\n",
            "iter 3590: loss 0.2896, time 83.50ms, mfu 5.92%\n",
            "iter 3600: loss 0.3066, time 83.74ms, mfu 5.95%\n",
            "iter 3610: loss 0.2725, time 83.62ms, mfu 5.97%\n",
            "iter 3620: loss 0.2975, time 83.42ms, mfu 5.99%\n",
            "iter 3630: loss 0.3040, time 83.35ms, mfu 6.01%\n",
            "iter 3640: loss 0.2812, time 83.69ms, mfu 6.02%\n",
            "iter 3650: loss 0.2963, time 83.88ms, mfu 6.04%\n",
            "iter 3660: loss 0.2743, time 83.46ms, mfu 6.05%\n",
            "iter 3670: loss 0.2848, time 83.27ms, mfu 6.06%\n",
            "iter 3680: loss 0.3012, time 84.05ms, mfu 6.07%\n",
            "iter 3690: loss 0.2862, time 83.69ms, mfu 6.08%\n",
            "iter 3700: loss 0.2899, time 83.78ms, mfu 6.09%\n",
            "iter 3710: loss 0.2733, time 83.90ms, mfu 6.09%\n",
            "iter 3720: loss 0.2568, time 82.87ms, mfu 6.11%\n",
            "iter 3730: loss 0.2728, time 83.60ms, mfu 6.11%\n",
            "iter 3740: loss 0.2810, time 83.91ms, mfu 6.11%\n",
            "step 3750: train loss 0.1048, val loss 8.9234\n",
            "saving checkpoint to out\n",
            "iter 3750: loss 0.2959, time 9593.85ms, mfu 5.51%\n",
            "iter 3760: loss 0.3028, time 82.12ms, mfu 5.59%\n",
            "iter 3770: loss 0.2816, time 82.20ms, mfu 5.65%\n",
            "iter 3780: loss 0.2690, time 83.17ms, mfu 5.71%\n",
            "iter 3790: loss 0.2708, time 84.09ms, mfu 5.75%\n",
            "iter 3800: loss 0.2727, time 84.96ms, mfu 5.78%\n",
            "iter 3810: loss 0.2876, time 83.86ms, mfu 5.82%\n",
            "iter 3820: loss 0.2601, time 83.16ms, mfu 5.86%\n",
            "iter 3830: loss 0.2932, time 83.40ms, mfu 5.89%\n",
            "iter 3840: loss 0.2582, time 82.95ms, mfu 5.92%\n",
            "iter 3850: loss 0.2702, time 83.84ms, mfu 5.94%\n",
            "iter 3860: loss 0.2381, time 83.38ms, mfu 5.97%\n",
            "iter 3870: loss 0.2613, time 83.28ms, mfu 5.99%\n",
            "iter 3880: loss 0.2585, time 83.90ms, mfu 6.01%\n",
            "iter 3890: loss 0.2549, time 83.58ms, mfu 6.02%\n",
            "iter 3900: loss 0.2549, time 83.24ms, mfu 6.04%\n",
            "iter 3910: loss 0.2433, time 83.77ms, mfu 6.05%\n",
            "iter 3920: loss 0.2628, time 83.76ms, mfu 6.06%\n",
            "iter 3930: loss 0.2350, time 83.12ms, mfu 6.07%\n",
            "iter 3940: loss 0.2464, time 83.59ms, mfu 6.08%\n",
            "iter 3950: loss 0.2509, time 83.62ms, mfu 6.09%\n",
            "iter 3960: loss 0.2559, time 83.99ms, mfu 6.10%\n",
            "iter 3970: loss 0.2494, time 83.83ms, mfu 6.10%\n",
            "iter 3980: loss 0.2318, time 83.10ms, mfu 6.11%\n",
            "iter 3990: loss 0.2265, time 83.36ms, mfu 6.12%\n",
            "step 4000: train loss 0.0950, val loss 8.9588\n",
            "saving checkpoint to out\n",
            "iter 4000: loss 0.2466, time 9588.43ms, mfu 5.51%\n",
            "iter 4010: loss 0.2467, time 82.36ms, mfu 5.59%\n",
            "iter 4020: loss 0.2598, time 82.36ms, mfu 5.65%\n",
            "iter 4030: loss 0.2572, time 83.55ms, mfu 5.71%\n",
            "iter 4040: loss 0.2335, time 83.73ms, mfu 5.75%\n",
            "iter 4050: loss 0.2565, time 84.71ms, mfu 5.78%\n",
            "iter 4060: loss 0.2533, time 83.42ms, mfu 5.82%\n",
            "iter 4070: loss 0.2476, time 83.17ms, mfu 5.86%\n",
            "iter 4080: loss 0.2472, time 82.82ms, mfu 5.90%\n",
            "iter 4090: loss 0.2514, time 83.97ms, mfu 5.92%\n",
            "iter 4100: loss 0.2530, time 83.36ms, mfu 5.95%\n",
            "iter 4110: loss 0.2264, time 83.88ms, mfu 5.97%\n",
            "iter 4120: loss 0.2519, time 83.46ms, mfu 5.99%\n",
            "iter 4130: loss 0.2488, time 83.48ms, mfu 6.01%\n",
            "iter 4140: loss 0.2484, time 83.61ms, mfu 6.02%\n",
            "iter 4150: loss 0.2505, time 83.09ms, mfu 6.04%\n",
            "iter 4160: loss 0.2357, time 82.96ms, mfu 6.06%\n",
            "iter 4170: loss 0.2102, time 83.57ms, mfu 6.07%\n",
            "iter 4180: loss 0.2364, time 83.65ms, mfu 6.08%\n",
            "iter 4190: loss 0.2311, time 83.68ms, mfu 6.09%\n",
            "iter 4200: loss 0.2554, time 83.62ms, mfu 6.09%\n",
            "iter 4210: loss 0.2388, time 83.69ms, mfu 6.10%\n",
            "iter 4220: loss 0.2488, time 83.97ms, mfu 6.10%\n",
            "iter 4230: loss 0.2126, time 83.44ms, mfu 6.11%\n",
            "iter 4240: loss 0.2319, time 83.77ms, mfu 6.12%\n",
            "step 4250: train loss 0.0867, val loss 9.1420\n",
            "saving checkpoint to out\n",
            "iter 4250: loss 0.2389, time 9594.79ms, mfu 5.51%\n",
            "iter 4260: loss 0.2311, time 82.84ms, mfu 5.58%\n",
            "iter 4270: loss 0.2304, time 82.96ms, mfu 5.64%\n",
            "iter 4280: loss 0.2358, time 83.29ms, mfu 5.70%\n",
            "iter 4290: loss 0.2067, time 84.05ms, mfu 5.74%\n",
            "iter 4300: loss 0.2233, time 84.12ms, mfu 5.78%\n",
            "iter 4310: loss 0.2113, time 83.38ms, mfu 5.82%\n",
            "iter 4320: loss 0.2279, time 83.67ms, mfu 5.85%\n",
            "iter 4330: loss 0.2326, time 83.05ms, mfu 5.89%\n",
            "iter 4340: loss 0.2430, time 83.53ms, mfu 5.92%\n",
            "iter 4350: loss 0.2151, time 83.73ms, mfu 5.94%\n",
            "iter 4360: loss 0.2287, time 84.31ms, mfu 5.96%\n",
            "iter 4370: loss 0.2159, time 84.05ms, mfu 5.98%\n",
            "iter 4380: loss 0.1925, time 83.10ms, mfu 6.00%\n",
            "iter 4390: loss 0.2194, time 83.16ms, mfu 6.02%\n",
            "iter 4400: loss 0.1996, time 83.44ms, mfu 6.03%\n",
            "iter 4410: loss 0.1976, time 83.85ms, mfu 6.05%\n",
            "iter 4420: loss 0.2164, time 82.98ms, mfu 6.06%\n",
            "iter 4430: loss 0.2184, time 84.20ms, mfu 6.07%\n",
            "iter 4440: loss 0.2082, time 83.57ms, mfu 6.08%\n",
            "iter 4450: loss 0.2082, time 83.25ms, mfu 6.09%\n",
            "iter 4460: loss 0.1995, time 83.31ms, mfu 6.10%\n",
            "iter 4470: loss 0.2175, time 83.61ms, mfu 6.11%\n",
            "iter 4480: loss 0.1857, time 83.99ms, mfu 6.11%\n",
            "iter 4490: loss 0.1915, time 84.52ms, mfu 6.11%\n",
            "step 4500: train loss 0.0795, val loss 9.1217\n",
            "saving checkpoint to out\n",
            "iter 4500: loss 0.2139, time 9582.91ms, mfu 5.50%\n",
            "iter 4510: loss 0.2104, time 82.45ms, mfu 5.58%\n",
            "iter 4520: loss 0.2163, time 83.01ms, mfu 5.64%\n",
            "iter 4530: loss 0.2052, time 83.23ms, mfu 5.70%\n",
            "iter 4540: loss 0.2023, time 84.80ms, mfu 5.73%\n",
            "iter 4550: loss 0.1952, time 84.19ms, mfu 5.77%\n",
            "iter 4560: loss 0.2034, time 82.98ms, mfu 5.82%\n",
            "iter 4570: loss 0.2038, time 83.03ms, mfu 5.86%\n",
            "iter 4580: loss 0.2044, time 83.65ms, mfu 5.89%\n",
            "iter 4590: loss 0.2212, time 83.46ms, mfu 5.92%\n",
            "iter 4600: loss 0.2147, time 83.84ms, mfu 5.94%\n",
            "iter 4610: loss 0.1954, time 83.63ms, mfu 5.96%\n",
            "iter 4620: loss 0.1965, time 84.03ms, mfu 5.98%\n",
            "iter 4630: loss 0.2113, time 83.13ms, mfu 6.00%\n",
            "iter 4640: loss 0.1842, time 83.12ms, mfu 6.02%\n",
            "iter 4650: loss 0.2034, time 83.58ms, mfu 6.04%\n",
            "iter 4660: loss 0.1935, time 83.67ms, mfu 6.05%\n",
            "iter 4670: loss 0.2162, time 84.05ms, mfu 6.06%\n",
            "iter 4680: loss 0.2049, time 83.20ms, mfu 6.07%\n",
            "iter 4690: loss 0.2136, time 83.73ms, mfu 6.08%\n",
            "iter 4700: loss 0.1887, time 83.24ms, mfu 6.09%\n",
            "iter 4710: loss 0.1910, time 83.21ms, mfu 6.10%\n",
            "iter 4720: loss 0.1790, time 83.31ms, mfu 6.11%\n",
            "iter 4730: loss 0.2138, time 83.84ms, mfu 6.11%\n",
            "iter 4740: loss 0.2185, time 83.39ms, mfu 6.12%\n",
            "step 4750: train loss 0.0768, val loss 9.2059\n",
            "saving checkpoint to out\n",
            "iter 4750: loss 0.1945, time 9621.57ms, mfu 5.51%\n",
            "iter 4760: loss 0.2166, time 82.36ms, mfu 5.59%\n",
            "iter 4770: loss 0.2041, time 82.26ms, mfu 5.66%\n",
            "iter 4780: loss 0.1753, time 83.73ms, mfu 5.71%\n",
            "iter 4790: loss 0.1724, time 85.07ms, mfu 5.74%\n",
            "iter 4800: loss 0.1897, time 84.21ms, mfu 5.78%\n",
            "iter 4810: loss 0.1875, time 83.23ms, mfu 5.82%\n",
            "iter 4820: loss 0.1732, time 83.15ms, mfu 5.86%\n",
            "iter 4830: loss 0.1870, time 83.42ms, mfu 5.89%\n",
            "iter 4840: loss 0.1983, time 83.04ms, mfu 5.92%\n",
            "iter 4850: loss 0.1788, time 83.84ms, mfu 5.94%\n",
            "iter 4860: loss 0.1850, time 83.45ms, mfu 5.97%\n",
            "iter 4870: loss 0.1826, time 84.07ms, mfu 5.98%\n",
            "iter 4880: loss 0.1671, time 83.25ms, mfu 6.00%\n",
            "iter 4890: loss 0.1911, time 83.17ms, mfu 6.02%\n",
            "iter 4900: loss 0.1857, time 83.87ms, mfu 6.04%\n",
            "iter 4910: loss 0.1879, time 83.59ms, mfu 6.05%\n",
            "iter 4920: loss 0.1930, time 83.60ms, mfu 6.06%\n",
            "iter 4930: loss 0.1701, time 83.69ms, mfu 6.07%\n",
            "iter 4940: loss 0.1880, time 83.51ms, mfu 6.08%\n",
            "iter 4950: loss 0.1811, time 83.67ms, mfu 6.09%\n",
            "iter 4960: loss 0.1862, time 83.90ms, mfu 6.09%\n",
            "iter 4970: loss 0.1746, time 83.36ms, mfu 6.10%\n",
            "iter 4980: loss 0.2055, time 83.79ms, mfu 6.11%\n",
            "iter 4990: loss 0.1868, time 84.08ms, mfu 6.11%\n",
            "step 5000: train loss 0.0731, val loss 9.2729\n",
            "saving checkpoint to out\n",
            "iter 5000: loss 0.1779, time 9544.70ms, mfu 5.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Outputs with our New Model\n",
        "\n",
        "Now we can leverage the `sample.py` file to generate outputs from our model!"
      ],
      "metadata": {
        "id": "L2J5JlRxFJOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation Set Up and Model Loading"
      ],
      "metadata": {
        "id": "eo_QP1ITFfX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import torch\n",
        "import tiktoken\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "-vftqU9LheEK"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "if init_from == 'resume':\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQRB3j7iiNkl",
        "outputId": "289537da-d88d-4f59-e8d2-182438f9ea7b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 29.55M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
      ],
      "metadata": {
        "id": "N1YAy8DriVZZ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tokenizer\n",
        "encode = lambda s: enc.encode(s)\n",
        "decode = lambda l: enc.decode(l)"
      ],
      "metadata": {
        "id": "KoB-5ZuLicAT"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation!"
      ],
      "metadata": {
        "id": "mkTQ9wo7FjYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmTcaHCjii5l",
        "outputId": "a3c65810-b179-4efe-fccd-4b36ba72c908"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "To the swift ambassador,\n",
            "Where you may pass your father's ghost:\n",
            "Mattering the reason of marriage fit to make you shall be an everlasting leiger:\n",
            "Therefore your best appointment make with speed;\n",
            "To-morrow you set on.\n",
            "\n",
            "JULIET:\n",
            "Is there no such needful ornaments\n",
            "But I shall have such needful ornaments\n",
            "As you till Thursday to-morrow?\n",
            "\n",
            "CAPULET:\n",
            "Go, nurse, go tell him what Thursday?\n",
            "\n",
            "LADY  CAPULET:\n",
            "No, not to; tell the tide,\n",
            "And there any well, I think our provision:\n",
            "Therefore then near night.\n",
            "\n",
            "CAPULET:\n",
            "Tush, I will stir about, I warrant thee, wife:\n",
            "And all things shall be well, I warrant thee, wife:\n",
            "Therefore accept things shall be well deserves a means, I warrant thee,\n",
            "I'll pardon you.\n",
            "\n",
            "TYou, nurse, my friend; this is a shrewd ill-night;\n",
            "And I do not endure him.\n",
            "\n",
            "PARIS:\n",
            "Monday, then.\n",
            "\n",
            "CAPULET:\n",
            "Monday! ha, ha, ha! Well, Wednesday is too soon,\n",
            "O' Thursday let it be: o' Thursday, tell her, tell her,\n",
            "She shall be married to this noble earl.\n",
            "Will you be ready? do you like this haste?\n",
            "We'll keep no great ado,--a friend or two;\n",
            "For, hark you, Tybalt being slain so late,\n",
            "It may be thought we held him carelessly,\n",
            "Being our kinsman, if we revel much:\n",
            "Therefore we'll have some half a dozen friends,\n",
            "And there an end. But what say you to Thursday?\n",
            "\n",
            "PARIS:\n",
            "My lord, I would that Thursday were to-morrow.\n",
            "\n",
            "CAPULET:\n",
            "Well get you gone: o' Thursday be it, then.\n",
            "Go you to Juliet ere you go to bed,\n",
            "Prepare her, wife, wife, against this wedding-day.\n",
            "Farewell, my lord. Light to my chamber, ho!\n",
            "Afore me! it is so very very very late,\n",
            "That we may call it early by and by.\n",
            "Good night.\n",
            "\n",
            "JULIET:\n",
            "Wilt thou be gone? it is not yet\n",
            "---------------\n",
            "\n",
            "JULIET:\n",
            "Do not well, I have been my faith;\n",
            "Or shall ever meet the battlements of yonder tower;\n",
            "Or walk in thievish ways; or bid me be ta'en out of yonder tower;\n",
            "Or shut me nightly in a charnel-house,\n",
            "O'en to-cover'd quite with dead men's rattling bones,\n",
            "With reeky shanks and yellow chapless skulls;\n",
            "Or bid me go into a new-made grave\n",
            "And hide me with a dead man in his shroud;\n",
            "Things that, to hear them told, have made me tremble;\n",
            "And I will do it without fear or doubt,\n",
            "To live an unstain'd wife to my sweet love.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Hold, then; go home, give consent\n",
            "To marry Paris: Wednesday is to-morrow:\n",
            "To-morrow night look that thou lie alone;\n",
            "Let not thy nurse lie with thee in thy chamber:\n",
            "Take thou this vial, being then in bed,\n",
            "And this distilled liquor drink thou off;\n",
            "When presently through all thy veins shall run\n",
            "A cold and drowsy humour, for no pulse\n",
            "Shall keep his native progress, but surcease:\n",
            "No warmth, no breath, no breath, shall testify thou livest;\n",
            "The roses in thy lips and cheeks shall fade\n",
            "To paly ashes, thy eyes' windows fall,\n",
            "Like death, when he shuts up the day of life;\n",
            "Each part, deprived of supple government,\n",
            "Shall, stiff and stark and cold, appear like death:\n",
            "And in this borrow'd likeness of shrunk death\n",
            "Thou shalt continue two and forty hours,\n",
            "And then awake as from a pleasant sleep.\n",
            "Now, when the bridegroom in the morning comes\n",
            "To rouse thee from thy bed, there art thou dead:\n",
            "Then, as the manner of our country is,\n",
            "In thy best robes uncover'd on the bier\n",
            "Thou shalt be borne to that same ancient vault\n",
            "Where all the kindred of the Capulets lie.\n",
            "In the mean time, against thou shalt awake,\n",
            "Shall Romeo by my letters know our drift,\n",
            "And hither shall he come: and he and I\n",
            "Will watch thy waking, and that very night\n",
            "Shall Romeo bear thee hence to Mantua.\n",
            "And this shall free thee from this present shame;\n",
            "If no inconstant toy, nor womanish fear,\n",
            "---------------\n",
            "\n",
            "ISABELLA:\n",
            "Ay, but such gifts that know your honour.\n",
            "\n",
            "ANGELO:\n",
            "Hark how I'll bribe you: good my lord, turn back.\n",
            "\n",
            "ANGELO:\n",
            "How! bribe me?\n",
            "\n",
            "ISABELLA:\n",
            "Ay, with such gifts that heaven shall share with you.\n",
            "\n",
            "LUCIO:\n",
            "\n",
            "ISABELLA:\n",
            "Not with fond shekels of the tested gold,\n",
            "Or stones whose rates are either rich or poor\n",
            "As fancy values them; but with true prayers\n",
            "That shall be up at heaven and enter there\n",
            "Ere sun-rise, prayers from preserved souls,\n",
            "From fasting maids whose minds are dedicate\n",
            "To nothing temporal.\n",
            "\n",
            "ANGELO:\n",
            "Well; come to-morrow.\n",
            "\n",
            "LUCIO:\n",
            "\n",
            "ISABELLA:\n",
            "Heaven keep your honour safe!\n",
            "\n",
            "ANGELO:\n",
            "\n",
            "ISABELLA:\n",
            "At what hour to-morrow\n",
            "Shall I attend your lordship?\n",
            "\n",
            "ANGELO:\n",
            "At any time 'fore noon.\n",
            "\n",
            "ISABELLA:\n",
            "'Save your honour!\n",
            "\n",
            "ANGELO:\n",
            "From thee, even from thy virtue!\n",
            "What's this, what's this? Is this her fault or mine?\n",
            "The tempter or the tempted, who sins most?\n",
            "Ha!\n",
            "Not she: but it is I\n",
            "That, lying by the violet in the sun,\n",
            "Do as the carrion does, not as the flower,\n",
            "Corrupt with virtuous season. Can it be\n",
            "That modesty may more betray our sense\n",
            "Than woman's lightness? Having waste ground enough,\n",
            "Shall we desire to raze the sanctuary\n",
            "And pitch our evils there? O, fie, fie, fie!\n",
            "What dost thou, or what art thou, Angelo?\n",
            "Dost thou desire her foully for those things\n",
            "That make her good? O, let her brother live!\n",
            "Thieves for their robbery have authority\n",
            "When judges steal themselves. What, do I love her,\n",
            "That I desire to hear her speak again,\n",
            "That I desire to hear her speak again,\n",
            "And feast upon her eyes? What is't I dream on?\n",
            "O cunning enemy, that, to catch a saint,\n",
            "With saints dost bait thy hook! Most dangerous\n",
            "Is that temptation that doth goad us on\n",
            "To sin in\n",
            "---------------\n",
            "\n",
            "\n",
            "CORIOLANUS:\n",
            "Pray you, I'll leave.\n",
            "\n",
            "COMINIUS:\n",
            "Now, what then?\n",
            "\n",
            "CORIOLANUS:\n",
            "I would they were barbarians--as they are,\n",
            "Though in Rome litter'd--not Romans--as they are not,\n",
            "Though calved i' the porch o' the Capitol--\n",
            "\n",
            "MENENIUS:\n",
            "Be gone;\n",
            "Put not your worthy rage into your tongue;\n",
            "One time will owe another.\n",
            "\n",
            "CORIOLANUS:\n",
            "On fair ground\n",
            "I could beat forty of them.\n",
            "\n",
            "COMINIUS:\n",
            "I could myself\n",
            "Take up a brace o' the best of them; yea, the\n",
            "two tribunes:\n",
            "But now 'tis odds beyond arithmetic;\n",
            "And manhood is call'd foolery, when it stands\n",
            "Against a falling fabric. Will you hence,\n",
            "Before the tag return? whose rage doth rend\n",
            "Like interrupted waters and o'erbear\n",
            "What they are used to bear.\n",
            "\n",
            "MENENIUS:\n",
            "Pray you, be gone:\n",
            "I'll try whether my old wit be in request\n",
            "With those that have but little: this must be patch'd\n",
            "With cloth of any colour.\n",
            "\n",
            "COMINIUS:\n",
            "Nay, come away.\n",
            "\n",
            "A Patrician:\n",
            "This man has marr'd his fortune.\n",
            "\n",
            "MENENIUS:\n",
            "His nature is too noble for the world:\n",
            "He would not flatter Neptune for his trident,\n",
            "Or Jove for's power to thunder. His heart's his mouth:\n",
            "What his breast forges, that his tongue must vent;\n",
            "And, being angry, does forget that ever\n",
            "He heard the name of death.\n",
            "Here's goodly work!\n",
            "\n",
            "Second Patrician:\n",
            "I would they were abed!\n",
            "\n",
            "MENENIUS:\n",
            "I would they were in Tiber! What the vengeance!\n",
            "Could he not speak 'em fair?\n",
            "\n",
            "SICINIUS:\n",
            "Where is this viper\n",
            "That would depopulate the city and\n",
            "Be every man himself?\n",
            "\n",
            "MENENIUS:\n",
            "You worthy tribunes,--\n",
            "\n",
            "SICINIUS:\n",
            "He shall be thrown down the Tarpeian rock\n",
            "With rigorous hands: he hath resisted law,\n",
            "And therefore law shall scorn him further trial\n",
            "Than the severity of the public power\n",
            "Which he so sets at nought.\n",
            "\n",
            "First Citizen:\n",
            "He shall\n",
            "---------------\n",
            "\n",
            "\n",
            "MAMILLIUS:\n",
            "Nay, my lord.\n",
            "\n",
            "LEONTES:\n",
            "Dwelt by a churchyard: I will tell it softly;\n",
            "Yond crickets shall not hear it.\n",
            "\n",
            "HERMIONE:\n",
            "Come on, then,\n",
            "And give't me in mine ear.\n",
            "\n",
            "LEONTES:\n",
            "Was he met there? his train? Camillo with him?\n",
            "\n",
            "First Lord:\n",
            "Behind the tuft of pines I met them; never\n",
            "Saw I men scour so on their way: I eyed them\n",
            "Even to their ships.\n",
            "\n",
            "LEONTES:\n",
            "How blest am I\n",
            "In my just censure, in my true opinion!\n",
            "Alack, for lesser knowledge! how accursed\n",
            "In being so blest! There may be in the cup\n",
            "A spider steep'd, and one may drink, depart,\n",
            "And yet partake no venom, for his knowledge\n",
            "Is not infected: but if one present\n",
            "The abhorr'd ingredient to his eye, make known\n",
            "How he hath drunk, he cracks his gorge,\n",
            "With violent hefts. I have drunk,\n",
            "and seen the spider.\n",
            "Camillo was his help in this, his pander:\n",
            "There is a plot against my life, my crown;\n",
            "All's true that is mistrusted: that false villain\n",
            "Whom I employ'd was pre-employ'd by him:\n",
            "He has discover'd my design, and I\n",
            "Remain a pinch'd thing; yea, a very trick\n",
            "For them to play at will. How came the posterns\n",
            "So easily open?\n",
            "\n",
            "First Lord:\n",
            "By his great authority;\n",
            "Which often hath no less prevail'd than so\n",
            "On your command.\n",
            "\n",
            "LEONTES:\n",
            "I know't too well.\n",
            "Give me the boy: I am glad you did not nurse him:\n",
            "Though he does bear some signs of me, yet you\n",
            "Have too much blood in him.\n",
            "\n",
            "HERMIONE:\n",
            "What is this? sport?\n",
            "\n",
            "LEONTES:\n",
            "Bear the boy hence; he shall not come about her;\n",
            "Away with him! and let her sport herself\n",
            "With that she's big with; for 'tis Polixenes\n",
            "Has made thee swell thus.\n",
            "\n",
            "HERMIONE:\n",
            "But I'ld say he had not,\n",
            "And I'll be sworn you would believe my saying,\n",
            "Howe'er you\n",
            "---------------\n",
            "\n",
            "If you are yours, and yours; that you are well.\n",
            "\n",
            "CORIOLANUS:\n",
            "I sometime lay here in Corioli\n",
            "At a beggar, that I saw him:\n",
            "' the earth he used me. Boy!\n",
            "\n",
            "AUFIDIUS:\n",
            "Why, lords, 'tis the noble lords,\n",
            "Which was forced to scold. Your judgments, my grave lords,\n",
            "Must give this cur the lie: and his own notion--\n",
            "Who wears my stripes impress'd upon him; that\n",
            "Must bear my beating to his grave--shall join\n",
            "To thrust the lie unto him.\n",
            "\n",
            "First Lord:\n",
            "Peace, both, and hear me speak.\n",
            "\n",
            "CORIOLANUS:\n",
            "Cut me to pieces, Volsces; men and lads,\n",
            "Stain all your edges on me. Boy! false hound!\n",
            "If you have writ your annals true, 'tis there,\n",
            "That, like an eagle in a dove-cote, I\n",
            "Flutter'd your Volscians in Corioli:\n",
            "Alone I did it. Boy!\n",
            "\n",
            "AUFIDIUS:\n",
            "Why, noble lords,\n",
            "Will you be put in mind of his blind fortune,\n",
            "Which was your shame, by this unholy braggart,\n",
            "'Fore your own eyes and ears?\n",
            "\n",
            "All Conspirators:\n",
            "Let him die for't.\n",
            "\n",
            "All The People:\n",
            "'Tear him to pieces.' 'Do it presently.' 'He kill'd\n",
            "my son.' 'My daughter.' 'He killed my cousin\n",
            "Marcus.' 'He killed my father.'\n",
            "\n",
            "Second Lord:\n",
            "Peace, ho! no outrage: peace!\n",
            "The man is noble and his fame folds-in\n",
            "This orb o' the earth. His last offences to us\n",
            "Shall have judicious hearing. Stand, Aufidius,\n",
            "And trouble not the peace.\n",
            "\n",
            "CORIOLANUS:\n",
            "O that I had him,\n",
            "With six Aufidiuses, or more, his tribe,\n",
            "To use my lawful sword!\n",
            "\n",
            "AUFIDIUS:\n",
            "Insolent villain!\n",
            "\n",
            "All Conspirators:\n",
            "Kill, kill, kill, kill him!\n",
            "\n",
            "Lords:\n",
            "Hold, hold, hold, hold!\n",
            "\n",
            "AUFIDIUS:\n",
            "My noble masters, hear me speak.\n",
            "\n",
            "First Lord:\n",
            "O Tullus,--\n",
            "\n",
            "Second Lord:\n",
            "Thou\n",
            "---------------\n",
            "\n",
            "The slave, and the point.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Here's Romeo, Aumerle, be done.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "What is that he?\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "As near is that my lord, I come to see the writing.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "I do beseech your grace to pardon me:\n",
            "It is a matter of small consequence,\n",
            "Which for some reasons I would not have seen.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Which for some reasons, sir, I mean to see.\n",
            "I fear, I fear, I fear,--\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "What should you fear?\n",
            "'Tis nothing but some bond, that he is enter'd into\n",
            "For gay apparel 'gainst the triumph day.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Bound to himself! what doth he with a bond\n",
            "That he is bound to? Wife, thou art a fool.\n",
            "Boy, let me see the writing.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "I do beseech you, pardon me; I may not show it.\n",
            "\n",
            "DUKE OF YORK:\n",
            "I will be satisfied; let me see it, I say.\n",
            "Treason! foul treason! Villain! slave!\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "What is the matter, my lord?\n",
            "\n",
            "DUKE OF YORK:\n",
            "Ho! who is within there?\n",
            "Saddle my horse.\n",
            "God for his mercy, what treachery is here!\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Why, what is it, my lord?\n",
            "\n",
            "DUKE OF YORK:\n",
            "Give me my boots, I say; saddle my horse.\n",
            "Now, by mine honour, by my life, by my troth,\n",
            "I will appeach the villain.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "What is the matter?\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Peace, foolish woman.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "I will not peace. What is the matter, Aumerle.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Good mother, be content; it is no more\n",
            "Than my poor life must answer.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Thy life answer!\n",
            "\n",
            "DUKE OF YORK:\n",
            "Bring me my boots: I will unto the king.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Strike him, Aumerle\n",
            "---------------\n",
            "\n",
            "I could make her that her die her long.\n",
            "\n",
            "LADY GREY:\n",
            "Why, then.\n",
            "\n",
            "GLOUCESTER:\n",
            "That would be ten days' wonder at the least.\n",
            "\n",
            "CLARENCE:\n",
            "That's a day longer than a wonder lasts.\n",
            "\n",
            "GLOUCESTER:\n",
            "By so much is the wonder in extremes.\n",
            "\n",
            "LADY ANNE:\n",
            "I'll leave that would I tell you both\n",
            "the dog for her husband's lands.\n",
            "\n",
            "GLOUCESTER:\n",
            "Well, leave that toucheth you and I will keep you.\n",
            "\n",
            "LADY ANNE:\n",
            "I beseech you, my leave of me.\n",
            "\n",
            "GLOUCESTER:\n",
            "So will I came unto the Tower:\n",
            "I do beseech you.\n",
            "\n",
            "LADY ANNE:\n",
            "No, then, my lord protector.\n",
            "\n",
            "GLOUCESTER:\n",
            "That is no more than I lie in me.\n",
            "\n",
            "LADY ANNE:\n",
            "Why, shall you fear?\n",
            "\n",
            "GLOUCESTER:\n",
            "That would I lie.\n",
            "\n",
            "LADY ANNE:\n",
            "Why, then, this keen encounter of our wits,\n",
            "And fall somewhat into a slower method,\n",
            "Is not the causer of the timeless deaths\n",
            "Of these Plantagenets, Henry and Edward,\n",
            "As blameful as the executioner?\n",
            "\n",
            "LADY ANNE:\n",
            "Thou art the cause, and most accursed effect.\n",
            "\n",
            "GLOUCESTER:\n",
            "Your beauty was the cause of that effect;\n",
            "Your beauty: which did haunt me in my sleep\n",
            "To undertake the death of all the world,\n",
            "So I might live one hour in your sweet bosom.\n",
            "\n",
            "LADY ANNE:\n",
            "If I thought that, I tell thee, homicide,\n",
            "These nails should rend that beauty from my cheeks.\n",
            "\n",
            "GLOUCESTER:\n",
            "These eyes could never endure sweet beauty's wreck;\n",
            "You should not blemish it, if I stood by:\n",
            "As all the world is cheered by the sun,\n",
            "So I by that; it is my day, my life.\n",
            "\n",
            "LADY ANNE:\n",
            "Black night o'ershade thy day, and death thy life!\n",
            "\n",
            "GLOUCESTER:\n",
            "Curse not thyself, fair creature thou art both.\n",
            "\n",
            "LADY ANNE:\n",
            "I would I were, to be revenged on thee.\n",
            "\n",
            "G\n",
            "---------------\n",
            "\n",
            "LEONTES:\n",
            "O course I am a cup\n",
            "Of this gentleman! I have too much believed,\n",
            "And see thee!\n",
            "\n",
            "ANTIGONUS:\n",
            "I'll leave me, I'll be so.\n",
            "\n",
            "PAULINA:\n",
            "It is an heretic that makes the fire,\n",
            "Not she which burns in't. I'll not call you tyrant;\n",
            "But this most cruel usage of your queen,\n",
            "Not able to produce more accusation\n",
            "Than your own weak-hinged fancy, something savours\n",
            "Of tyranny and will ignoble make you,\n",
            "Yea, scandalous to the world.\n",
            "\n",
            "LEONTES:\n",
            "On your allegiance,\n",
            "Out of the chamber with her! Were I a tyrant,\n",
            "Where were her life? she durst not call me so,\n",
            "If she did know me so, did know me one. Away with her!\n",
            "\n",
            "PAULINA:\n",
            "I pray you, do not push me; I'll be gone.\n",
            "Look to your babe, my lord; 'tis yours:\n",
            "Jove send her\n",
            "A better guiding spirit! What needs these hands?\n",
            "You, that are thus so tender o'er his follies,\n",
            "Will never do him good, not one of you.\n",
            "So, so: farewell; we are gone.\n",
            "\n",
            "LEONTES:\n",
            "Thou, traitor, hast set on thy wife to this.\n",
            "My child? away with't! Even thou, that hast\n",
            "A heart so tender o'er it, take it hence\n",
            "And see it instantly consumed with fire;\n",
            "Even thou and none but thou. Take it up straight:\n",
            "Within this hour bring me word 'tis done,\n",
            "Within this hour bring me word 'tis done,\n",
            "And by good testimony, or I'll seize thy life,\n",
            "With what thou else call'st thine. If thou refuse\n",
            "And wilt encounter with my wrath, say so;\n",
            "The bastard brains with these my proper hands\n",
            "Shall I dash out. Go, take it to the fire;\n",
            "For thou set'st on thy wife.\n",
            "\n",
            "ANTIGONUS:\n",
            "I did not, sir:\n",
            "These lords, my noble fellows, if they please,\n",
            "Can clear me in't.\n",
            "\n",
            "Lords:\n",
            "We can: my royal liege,\n",
            "He is not guilty of her coming hither.\n",
            "\n",
            "LEONTES:\n",
            "You\n",
            "---------------\n",
            "\n",
            "Not she's best.\n",
            "\n",
            "LEONTES:\n",
            "What, Camillo,\n",
            "That I am satisfied and that's an honest,\n",
            "But once she's a gross lout, a mindless slave,\n",
            "Or else a hovering temporizer, that\n",
            "Canst with thine eyes at once see good and evil,\n",
            "Inclining to them both: were my wife's liver\n",
            "Infected as her life, she would not live\n",
            "The running of one glass.\n",
            "\n",
            "CAMILLO:\n",
            "Who does infect her?\n",
            "\n",
            "LEONTES:\n",
            "Why, he that wears her like a medal, hanging\n",
            "About his neck, Bohemia: who, if I\n",
            "Had servants true about me, that bare eyes\n",
            "To see alike mine honour as their profits,\n",
            "Their own particular thrifts, they would do that\n",
            "Which should undo more doing: ay, and thou,\n",
            "His cupbearer,--whom I from meaner form\n",
            "Have benched and reared to worship, who mayst see\n",
            "Plainly as heaven sees earth and earth sees heaven,\n",
            "How I am galled,--mightst bespice a cup,\n",
            "To give mine enemy a lasting wink;\n",
            "Which draught to me were cordial.\n",
            "\n",
            "CAMILLO:\n",
            "Sir, my lord,\n",
            "I could do this, and that with no rash potion,\n",
            "But with a lingering dram that should not work\n",
            "Maliciously like poison: but I cannot\n",
            "Believe this crack to be in my dread mistress,\n",
            "So sovereignly being honourable.\n",
            "I have loved thee,--\n",
            "\n",
            "LEONTES:\n",
            "Make that thy question, and go rot!\n",
            "Dost think I am so muddy, so unsettled,\n",
            "To appoint myself in this vexation, sully\n",
            "The purity and whiteness of my sheets,\n",
            "Which to preserve is sleep, which being spotted\n",
            "Is goads, nettles, tails of wasps,\n",
            "Give scandal to the blood o' the prince my son,\n",
            "Who I do think is mine and love as mine,\n",
            "Without ripe moving to't? Would I do this?\n",
            "Could man so blench?\n",
            "\n",
            "CAMILLO:\n",
            "I must believe you, sir:\n",
            "I do; and will fetch off Bohemia for't;\n",
            "Provided that, when he's removed, your highness\n",
            "Will take again your queen as yours at first,\n",
            "Even for your son's sake; and thereby for sealing\n",
            "The injury of\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}