video_id: 5h-JBkySK34
Hello everyone. Today I want to talk about Langraph, a new library that we're releasing. So Langgraph builds on top of Langchain and makes it really easy to create agents and agent runtimes. So what exactly is an agent and an agent runtime? So in Langchain we define an agent as a system powered by a language model that decides what action to take. There's then an agent runtime that basically runs that agent in a loop, calls that agent, decides what action to take, then takes that action and then records the observation and then passes that back in and starts to loop over again and continues going through that loop until the agent decides that it is finished. So we've made it easy over the past few months to customize agents in LangChain with LangChain expression language and I'd recommend checking those out if you haven't. What we're doing with LangGraph is we're making it easy to customize the agent runtime. So previously the agent runtime was always the agent executor class and this was basically something that ran in the loop and called tools in a specific way and handled errors in a specific way. And that was great, but it was just one way to create this runtime. And so we want to create more ways to create this runtime and ways to create them more flexibly and dynamically. And a key part of this runtime is the ability to add cycles. So the whole point of the agent runtime is running this LLM-powered agent in a loop. And so you need to be able to have cycles. And language chain expression language and other DAG-like frameworks are non-cyclical. And so that's why we're introducing Langraph for. It's a way to create these cyclical agent runtimes. So that's hopefully an overview of what LaneGraph is and why we created it. The rest of the videos in this series are gonna focus on two of the main agent runtimes that we've added to start. So first we've added agent executor, very similar to the agent executor in lane chain. So we've recreated that agent executor with lane graph. And then we've added a chat agent executor. The chat agent executor takes in a list of messages and then represents the agent state just as a list of messages. And so it returns a list of messages as well. The reason we did this is that a lot of the newer models are chat-based models and they intrinsically represent function calling as basically parameters as part of a message, and then they also have function responses being a separate type of message. And so representing this agent state as a list of messages is very natural for these types of models, and so we've added a separate agent executor, particularly for that. Then I'm gonna highlight a bunch of different ways that you can modify the base agent executors to do things like add human in the loops, force a specific tool to be called first, and other exciting things like that. Hope you enjoy.


video_id: 9dXp5q3OFdQ
Alright, so in this video we're going to go over how to build the equivalent of the current Lang chain agent executor from scratch in Lang Graph. And we'll see how easy it is. So in order to do this, we're first going to set everything up. We need to install a few packages. We need to install Lang chain, so we'll use this to use the existing agent classes in Lang chain. We can still easily use those agent classes in Lang graph. We'll also install Lang chain open AI so that we can use the open AI package. And we'll use that to power our agent. And then we'll also install Tavili Python. This will power the search tool that we'll use as one of our tools for the agent. After we do that, we'll set the open AI API key. We'll set the Tavilli API key, and then we'll also set the LangSmith API key. And so here, these two variables, LangChainTracingV2 and LangChain API key, if you set those, those will set things up so things start getting logged to LangSmith, which is our observability platform. And so you can see the instructions here. And you'll also need to grab an API key from here. If you don't have access to LangSmith yet, it's currently in a private beta, just DM me on Twitter or LinkedIn and we can get you that. Going back to this notebook, the first thing we're going to do is create the LangChain agent. And so this is the exact same code that we used in LangChain. So I'm not going go over in too much detail if you want more information on that, check out the Langchain documentation. But basically we'll create a tool, which is this Tavili search tool. We'll get our prompt, which we're pulling from the hub. We'll choose the LLM that we want to use, which is this OpenAI LLM, and then we'll create this OpenAI Functions Agent, which is a particular type of agent. We will then define the graph state. So this is the state that's going to be tracked throughout the graph over time. And basically the reason that this is important is that once we establish the state, each node can basically push updates to that state. So we don't have to pass around this state from node to node all the time. Rather, what we can do is we can just pass updates to that state. So we don't have to pass around this state from node to node all the time. Rather, what we can do is we can just pass updates to that state. As part of that, we need to specify what type of updates that we're pushing to that state. So by default, the updates will basically override the existing attribute for that state. That's useful in some cases, but in other cases, you want to actually add to the existing state that state. That's useful in some cases, but in other cases you want to actually add to the existing state that exists, and we'll see an example of that here. And so again default will override, but oftentimes you want to add to the attribute for that state. And so if we look at the existing agent state, we can see here that the first two things are basically inputs. So the input message to the conversation and then the chat history if there happens to be any chat history. And so those will be things that we'll pass in as we see later on. The next two are things that the graph will add over time. So agent outcome will be set by a few nodes or by one node in particular when after the agent is called and then that will be basically the basically tool that it should call or the final result that it should pass and so we have agent action and agent finish to basically represent that tool and then the final result we also have none because that's what this is will default to when it starts so this will be none to start finally we have a list of the steps that the agent has taken thus far. So this is one of the ones that we don't wanna override this, but rather we want to append to this over time and keep on growing this. And so here we've annotated this with the add operator. And so this means that anytime a node writes to this attribute, it will basically add it to the existing value rather than override it. And we type this as a list of tuples of agent actions and strings. These are how intermediate steps are represented in the current lane chain agents. So we can run that. Then what we need to do is define the nodes. So here we define the nodes and define the edges. So first we really have a need for two nodes. First the agent node, which uses the agent to determine what action to take. And then basically the node that invokes the tools. So takes in the agent decision, calls that tool, and then does something with that. Besides those nodes, we also need to add some edges. So there's two types of edges, basically a conditional edge, which will kind of, it's basically one node leads to a fork in the road. And there's two different directions or three different directions or four different directions. But basically there's different edges that it could take conditional on kind of like the result of the previous node. And so we'll see an example of adding that. The example that we'll add is basically based on the agent outcome. We either want to call a tool or we want to return to the user. And so that'll be the forking decision that we have to make. Then there's a normal edge, and this is where this always happens. And so an example of that is after we call a tool, we always want to return back to the agent to let it decide what to do next. So let's take a look at the nodes that we have here. First, we have this run agent node, which calls the agent. It takes in the data, calls agent runnable.invoke, and then assigns this to the agent outcome. So this will override the existing value of agent outcome and put this new output there. Then we have this execute tools function, which takes in the data, gets the current agent outcome, executes it with this tool executor, which is a nice little helper function we've made to just easily basically say, you know, to run this tool with this function. And then it will return intermediate steps. Remember, intermediate steps is the one that we're adding to. So here we define a list of this agent action and then the output cast to a string, and this list will get appended to the existing list. And then finally we add this function, shouldContinue, and this is basically going to be used to create that conditional node or conditional edge. So we look at the agent outcome and if it's a finish, if it's agent finish, then we return end, and if otherwise we return continue and we'll see how we use these later on when we construct the graph which we'll do right now. So first we define a new graph so we import state graph from langgraph.graph and we pass in agent state which is the state that we defined above. We then add the two nodes so we add them by specifying the name of the node as a string, and then the function, and this can be a function or a runnable, either or, and so you can specify that here, and the reason that we do this is so then that we can refer to this node down below. We set the entry point as agent, so this is using the same string that we set here, and this is basically just saying this will be the first node that is called when input gets in. We then add a conditional edge. So first we define the start. So this is saying after the agent is, after the agent node is finished running, what we'll do is we'll call this function, the should continue function. So this should continue function gets the output of anything after the agent node is called and then it will look at the data and return end or continue. And so if we look down here, the last thing we pass into this is this mapping of string to string. And so basically we have this key, this key should match the output of should continue. And so if should continue returns continue, then we call the action node, which we defined above. If it returns end, then we call this special end node, which is a built-in node that denotes that we should end and return to the user. We then add a normal edge from action back to agent. So this is after the tool is called. We then return back to the agent. And then finally, we compile the graph, and this basically converts this graph structure into a lane chain runnable that we can then use. So we can use invoke, we can use stream, and everything with it. Taking a look at what this looks like, we can now call, we can run this, and then we can call it with some inputs. So remember, we need the input key and we need chat history as the two inputs. And we'll use the stream method after this, and this will print out the results of each node. So we can see here that we get the agent outcome. And so this is the first, the output of the agent node. We get back this thing that says use tool to really search results JSON with this tool input query, whether in San Francisco, we then get back intermediate steps. And so the intermediate steps are a tuple of this tool with the results of calling that tool. And then we get back a new agent outcome. And this is calling agent finished. So this is saying we're done with the agent. And then this is calling agent finished. So this is saying we're done with the agent, and then this is the final thing that's returned, which is just basically the whole state. So it includes the input, the chat history, the current agent outcome, and then any intermediate state steps. We can see what this looks like in LangSmith for perhaps a better example. So if we click into LangGraph, we can see that it starts. It then calls the agent and under the under the hood the agent's calling OpenAI and so we can see the exact prompt that goes in. And so we can see we get back this function call. We can then see the function call in this action. And here we get back this result and we can see it goes back into agent after that. It calls OpenAI again. It's now got this stuff in it and it says that it gets some response back. So that's basically how to use the, or how to create an agent executor very similar to the existing agent executor in link chain from scratch with sling graph. We'll go over a few additional things in future videos, like one we'll do a deeper dive on the interface that this state graph exposes, and then two, we'll cover streaming more and different ways that you can stream results back.


video_id: Un-88uJKdiU
In this video, we're going to go over the chat agent executor that we've added to LaneGraph. So the chat agent executor is basically an agent executor that works solely on a list of input messages and then basically adds messages to that list to keep track of the agent's state over time. This is useful and interesting because a lot of the newer models are chat-based models that represent function calling and function responses as messages, and so we can just append it to this list of messages and keep track of it that way. Compared to the other video on using link chain agents, this will actually use less link chain concepts. So we'll just use the OpenAI model. It can use any model that supports function calling, but we'll use the OpenAI model for this video. And it will use tools from Langchain, but it won't use the Langchain agent abstractions. It will be more kind of like bare bones. So let's take a look at setting it up. We're going to require Langchain package. We're going to require Langchain OpenAI package to use the OpenAI model. And we're going to require Tavili package, which will be the search tool that we'll use. We'll set the API keys for that and then we'll also set up a Lang chain tracing from LangSmith. So this isn't required but this is a really good way to observe what's going on under the hood with your agents. So first we're going to set up the tools. We can do that pretty easily. We're going to be using Tav search here, and then we're just gonna set up the tool executor, which is a helper method to basically invoke these tools. After that, we're gonna set up the model. So what we're gonna do is we're gonna import the chat open AI model from the lane chain integration, but we're gonna initialize it. We're gonna set streaming equals to true, and we'll see why this is important later on, but basically we can stream back tokens this way. And then what we're gonna do is we're going to basically attach the functions that we want the model to have the ability to call to this object. So we can call this format tool to OpenAI function method which takes in the lane chain tools and converts it to the format that the OpenAI functions calling expects. Once we do that we can define the agent state. So the agent state is the thing that is passed around and all nodes in the graph will basically update this state over time. So here the state's really simple it's just gonna require it's gonna be a dictionary and there's gonna be one key in it it's gonna be this list of messages. This list of messages we want to append to over time so there's two different ways that you can update the state of, you can update the state over time. You can either overwrite the individual attributes or you can add to it. Here we want to add to it. So we're going to use annotated and then this operator.add to note that any updates from nodes to this messages thing are going to add to it over time. And this is nice so we just, so we can just return only the new messages and not have to worry about returning the old messages plus the new messages. After this we're going to define the nodes and the edges. So the nodes are things that do work and then edges are basically things that connect them. So we're going to want a few things here. We're going to want an agent node that basically calls the language model and gets back a response. And then we're going to want the action node, which will take the response or the list of messages, see if there's any tools that should be called, call those tools, and then append them to the list of messages. We're also going to need a way to determine whether we want to go from the agent to the tool calling thing or to finish. So remember the agent doesn't always have to call tool when it's finished, when it's called as many tools as it wants to, and it wants to finish, it can just return directly. And so we need to have a function that determines which of those paths to go down. And that's called a conditional. We'll see that later on. So here we'll first define this function that determines which branch to go down. It's gonna look at the at the current list of messages. If there's a function call in the last message, then we're going to, or sorry, if there's not a function call in the last message, then we're gonna end. If there is a function call, then we're gonna continue. And so we'll use this to create that conditional edge later on. Then we have this function that calls the model. Here we just get the messages, pass it into the model, and then append that response. This response will be a single message. And we'll use a list here because this is getting added to the current messages value. So we need it to be something that can be added with a list. And then finally, we'll have this call tool node. And this will take in the messages. It will get the last message. It will create this tool invocation, which basically just has the tool and then the tool input. And we'll load those from the function calling that's returned from OpenAI function calling. We will then call the tool executor with this tool invocation. We'll create a function message. And then we'll append this function message to the list of messages by returning this function message as its own list. Once that's done, we can now define the graph. So we create a graph by passing in the agent state we defined above. We then create two nodes, the agent node and the action node. We then set the entry point to be the agent node. And so this is basically saying as soon as the input gets in, the first node that we're going to send it to is the agent node. We're then going to add a conditional edge after the agent is called. So after the agent is called, we either want to go to the action node or we want to finish. And we want to do that based on the logic that's up here in the should continue function. So we're going to add conditional edges from the agent node. It's going to use shouldContinue as the function. And then basically we're going to pass in a mapping from string to some other node name. So continue and end, these are the two values that are returned from the shouldContinue function. And basically if continue is called, then we're going to go to the action node that we defined above. If end is called, then we're going to go to this end node. And this is a special node that denotes the end of agent. Now we can add an edge from the action node to the agent node. And we're going to do this because we always want to go to the agent node after we call an action. And then we can compile this graph into something that we can use, like a lane chain runnable. And so we'll expose a lot of similar interfaces as lane chain runnables do, and we'll use these below. So in order to use it, we need to create our input. Our input's going to be a dictionary. It's going to have a messages key, and that messages key is just going to be a list of messages. Here I just have one human message, but I can easily add a system message, a system message, a human message, an AI message, and another human message, anything there. So I can have full control over the list of inputs. It's just a list of messages. Once I call it, I need to run the above cell. Once I call it, it will take a little bit because it's doing a bunch of calls under the hood, but it will eventually return an answer and it will return an updated thing with this messages and this messages will be the list it has in the human message that we passed in and then the AI message, which is the first call that it made, a function message, which is the result, and then an AI message, which is the final result. If we wanna see what's going on under the hood, one way that we can do that is with LangSmith. So we can go in here and we can see that there's a few things that are happening under the hood. First, we're calling OpenAI. The inputs, this list of messages, which is just this human message, gets back this function call thing. We then call this, we then go to the action node and we call to Villy search. And so we get back this list of results. And then we have another call to the agent where we have this list of messages as input and we have this output. So this shows what's going on under the hood. As you may have noticed it took a little bit to get this and so one of the things that's really nice about LaneGraph is it also has a few different streaming capabilities and that's what we'll cover in the next video.


video_id: 9H4gwJGgvfg
Here we're going to go over how to modify the chat agent executor to add a human in the loop component, where it validates that you approve a tool action before it takes that action. So it's going to build off the base notebook. So if you haven't already run that notebook, I'd highly recommend going over that notebook, as we're largely going to focus on just any modifications from that notebook in this video. So we're going to set it up as before, there's no extra installs that we need to do. We're going to create our tool, we're going to create our tool executor, we're going to set up our model, we're going to bind tools to that model, we're going to define the agent state, all of this is the same as before. And the first thing that's different is when we start to define the nodes. The should continue logic and the call model logic is the same, but that's different is when we start to define the nodes. The shouldContinue logic and the callModel logic is the same. But what's different is this callTool function. So here we're now adding this logic where here we're just going to prompt the user in this interactive IDE should they continue with this action. And if the response is no, then we're going to throw an error and basically stop. So this is the only modification that we make, the graph's going to be the exact same, and now we can use it. And so when we call it, what we can see is happen is when, before we call this tool, it gives this. And if we say yes, then it will continue, it will call the tool, and we'll get back a response as normal. What we can also do though is we can see what happens if I say no. So if I say no then it will raise the value error and it will stop. So this is a really simple example. You know you'd probably want to do something besides raise a value error and you'd probably want this not in a Jupyter notebook and in some other UI but this hopefully shows how you can add a really simple human in the loop component to LaneGraph agents.


video_id: rabXcLaAlqE
The base chat agent executor that we created goes through this loop of calling a language model to determine either to exit or to call a tool. If it determines to call a tool, then it calls that tool, it gets back the response, and then it goes back to the language model. Here we're going to do a slight modification where we give the agent the ability to basically say, go call that tool and then pass that tool response as the final thing to the user. And so this is useful when we have some tools that we know can sometimes return valid responses and we just want to return those. We don't want to have the agent do any additional summarization or interpretation of the tool responses. And importantly, this is useful when only sometimes do you want that to happen. If you always want that to happen, you can set a property on the lane chain tools called return direct, where it always returns directly to the user. Here, we will let the agent dynamically determine whether it wants to do that. So this is a modification from the base chat agent executor notebook. If you haven't already done that notebook, please go do that first. We're going to build off that notebook, we are only going to change, um, or we're only going to talk about things that we change. So to start, we're going to change the tool specifically. We're going to add this, uh, method to the tool schema called return direct, which is going to be a bullying is going to default to false. And this is basically what the agent will set if it wants to dynamically return this tool to the user. So this will not actually get used by the tool. And so we'll see how we deal with that later on. But basically we're adding it to the schema of the tool. So we're defining it here and we're then passing it in as arg schema. We're defining it here so that the agent knows that it has this option and that it can specify sometimes that it should return direct equals true and return to the user. We can then create the tool executor, same as before. We can then create the model, same as before. Bind the tools to it, same as before. Create the agent state, same as before. And now we can define the nodes. So here, we're going to change a few things. First, we're going to change the shouldContinue function. So the previous logic was that if there was no function call in the additional quarks, then we'd end. Now we also want to end, or now we want to go to this final step when there is this return direct value set in the argument. So we're going to add this final step. And then if the return direct is not true, then we're going to pass, then we're going to say continue. The call model node is going to be the same as before, but now this call tool thing is going to be slightly different because remember we have this value return direct that's not actually being used in the tool, but the agent is returning it. So what we want to do is we don't actually want to pass that into the tool. And so we can see that that's what we're doing here. We're extracting the tool name and the arguments. And then if this tool name is the name of the tool, which we know has this return direct parameter, and if return direct is in arguments, we're deleting it. And so that means that when it gets past the tool and tool executor here, this return direct won't be present. If it was present, it would cause some errors, because again, that tool doesn't do anything with the return direct won't be present. If it was present, it would cause some errors. Because again, that tool doesn't do anything with the return direct. That's just for the agent to know that it should return directly. So now we're defining the graph. And so one modification that we're going to make is we have this final node. And so this is going to be the final tool call. And the reason that we have this as a separate node from the action, which also uses the same function is that basically we want the responses from this to be handled in different ways. And so we'll see that when we create the edges later on. So we set the entry point, we create this additional thing, but now we have. These, uh, or we have these two different ages, uh, we have these two different edges that we add here. So for the first action node, this is when return direct is not true. We always, we're gonna return to the agent. For this final node, when, and this is when return direct is true, we're gonna go to end, which is this, at the end of the agent. So that's the reason that we have these two separate nodes is because we have these two separate outbound edges from them. We can create the graph that way, and now can use it if we use it normally we can see that it will call the language model it'll make a tool call it'll make another call to the language model and then it will finish and so if we go see what that looks like in Langsmith we can see that we have this call to open AI call to Tavilli call to open AI and it's done. Now we're this call to OpenAI, call to Tavilli, call to OpenAI, and it's done. Now we're going to try to make it return directly. So we're going to ask the same question, but we're going to say return this result directly by setting return direct equal to this is a little bit, you know, ideally the agent would recognize when it wants to do this dynamically. You'd probably do that through some form of prompt engineering, but for demonstration purposes, we're just going to put this really simple here. And so we can see in the streaming that we get back this output from the agent node, we get back the output from this final node, so it's going down a different path, and then it ends. And so if we go see, remember, previously it was open AI to Vili, open AI. If we go to the other trace now, we can see it's just OpenAI to VILLA, because it goes to this final node instead, and it ends. So this covers how we can make agents sometimes return the results of tools directly to the user. Again, if you always want a tool to return directly, then you don't need to do this. You can just set return direct equals true on the tool type.


video_id: P1aDaYkZzNg
Here we're going to cover how we can get a chat agent executor to respond in a specific format. And so this is often useful when you know that you want your agent to respond in a specific format and you want to enforce that via function calling. And so we're going to cover how to do that here. This is building off of the base chat agent executor notebook. So if you haven't already gone and looked at that notebook, please do that before you get started. Because in this notebook we're just going to cover the things that are different. So we're going to start by creating the tool and then creating the tool executor. This is all the same as before. We're going to create the model. This is the same as before, but now we have this modification to it. So what we're doing is previously we just binded only the tools that it could call as functions to the model, but now we also want to bind this other function definition to the model and this is the response schema that we want the the agents responded. So here we're making one up we're going to say we want it to respond with a temperature and then other notes which is a string and so what we're going to do is first we create functions which are from the tools. So this takes in the tools and it converts them to OpenAI function calling. Then we're also adding in this response schema. And so this response schema is this converted to OpenAI function calling. And we're binding that as functions to the model. Now the rest of it here we're defining the agent state that's the same as before and now we're defining the nodes and there's some differences here. So first in the should continue state so previously this is so this is the same as before right here if there is no function call in the last message then we want to end. If there is a function call and the function call name is response, then we also want to end. And then if there is a function call and the function call name is not response, then we continue. And other than that, the other two nodes are the same. So we'll define this graph. The graph is the same as before and now we can use it. So here let's call it. We can start to see some responses streamed back. We get back an AI message first which calls this function. We get back the function message which has the result. We get back another AI message which calls this function, but this function is response. And so then we end. And so this is slightly different from previously. Previously the response was in content, which was great, it's a string. Here we have a structured response and it still ends. And so if we take a look at what's going on under the hood in LengSmith, we can see that we have a call to OpenAI, this TavilliSearch, and then a call to ChatOpenAI. If we click on here, we can see that we now have two functions and tools. We have this Tavilli search results JSON, which it's using as a tool, but then we also have this response, which it's using to respond to the user. So this is how you can use an agent executor in Lengraph to respond in a specific format with an agent call.


video_id: xJ6ipunO16E
One of the common questions that we get is how can I modify the existing agent executor to do different things with the internal state? Previously that wasn't super possible because it was always in this Lang chain agent executor class, but with Lang graph where it's all exposed there, you can easily modify it to do anything with the agent steps that happen as the agent is progressing. And so in this notebook, we'll go over how to do that. It builds off the base chat agent executor notebook. So if you haven't already done that notebook, please go and do that. We're only gonna cover the modifications we make, which are very, very small ones. So we're gonna set up our tools. This is all the same as before. We're gonna set up the model. This is all the same as before. We're gonna define the agent state. This is all the same as before. We're going to define the agent state. This is all the same as before. And now we're going to define the nodes. The should continue edge logic is the same, but the modification that we're making now is we're just adding some logic in here to filter the messages that we end up passing to the model. So if we want to only pass the five most recent messages, we could put it here. If we want to add some different logic where we take the system message and then the five most recent messages, we could put that here. If we want to do some summarization of any of the messages that are older than five, we could put that here. And so this is where you can add in logic to handle how the intermediate steps of the agent are handled. So we'll do that here. And then everything else is the same. So it's a very minor modification, which we can then use. There's actually gonna be no difference here because I only have one message that's passed in and the intermediate steps only get up to length two. But any logic, the important part is that any logic that we wanna put to modify how the agent steps are represented can go here. And so this is for modifying a chat agent executor, although the same exact stuff holds if we're doing this with a normal agent executor.


video_id: o5Hw9V5ntxw
In this video, we're going to make a really simple modification to the chat agent executor so that we always call a tool first. So if you haven't already watched the chat agent executor video, please do that. We will work off of that notebook that we have there and we'll only make a few modifications. And in this video, we're only going to cover those modifications. So if you want more context on what's going on in the notebook, please check out the video in the notebook for the base chat executor. All right, so we're going to set this up. We're force calling tool first. Most of the setup is the same. We're going to create our tools. We're going to create this tool executor to column. We're going to create our model. We're going to bind the tools to the model, we're going to define an agent state, this is all exactly the same and we're going to actually define the same exact states that in our nodes and edges that we had before, but we're also going to define an additional node. So this is we're going to call this the first model node and this is basically this is going to be the first node that gets called and we want this to return a message that says to call a specific tool. So we have this Tavili search results JSON tool, that's the name of the tool that we defined above. If you want it to call a different tool, you have to pass in a different name here and then we're going to pass in the inputs that we want it to call with. So we're going to basically call it with query and we're going to get the most recent message and get the content from that message and pass that in as a query. So we're going to create this function and now we're going to modify the graph. So compared to the previous graph we now have this new entry point which is called first agent. We have the same two other entry points as before, agent and action, And we're gonna set the entry point to be first agent. So this is a change from before as well. So now we're setting that this first agent node is always called first. We then have a conditional node from agent to action or end. This is the same as before. We now have this node from action to agent. This is the same as before, but we have this new node from first agent to action, which we're gonna set. And this is basically saying after we call the first agent, we always wanna call an action. And we know that we always wanna call an action because we are forcing it to call a AI message with this function call. So we always wanted to call this tool. So we can create this graph. And now if we use it, we can see that we get the first result back really, really fast, because it's not actually calling a language model, it's just passing this message in as the argument, and then we can see the other things as well. If we take a look at what's going on under the hood in Langsmith, we can see this even better. So we have this result here, we can see that the first agent, we pass in this message, and we get back this other message, but there's actually no language model call going on under the hood. And the tool is really the first thing that's ever invoked. And then there is a language model call at the end, but we can see that we skip this entirely because we are forcing the agent to basically always call this tool first.


video_id: hvAPnpSfSGo
Hey everyone, this is Harrison from LangChain here. I wanted to do a video on LangGraph, which is a new library we released pretty recently, and how you can use LangGraph to create some multi-agent workflows. So for those of you who aren't caught up on LangGraph, we have a whole series of videos going through it. And it's basically a way to dynamically create agent-like workflows as graphs. And so what do I mean by that? By an agent-like workflow, I generally mean running a language model in a loop in a variety of ways. And there's different structure in how you might wanna run that language model in a loop. And so Langraph is useful for defining that structure, and specifically it allows for the definition of cycles, because this is a loop after all. Another way to think about this is thinking about it as a way to define kind of like state machines. And so a state machine and a labeled directed graph are pretty similar in that you have different nodes. And if you think of this as a state machine, then a node would be a state. And if you think of it as kind of like in this multi-agent workflow, then that node state is an agent. And then you have these edges between them. And so in a graph, that's an edge, in a state machine, that is transition probabilities. And when you think of these multi-agent workflows, then those are basically how do these agents communicate to each other. So we'll think about building multi-agent workflows using LandGraph. The nodes are the agents, the edges are how they communicate. If you haven't already checked out LandGraph, again, highly recommend that you check out the video that we have. At a high level, the syntax looks like this. It's very similar to network X. You define a graph that tracks some state over time. You then add nodes into that graph. You then define edges between those nodes and then you can use it like you would any other Lang chain chain. So today I wanna talk about three different variants of multi-agent workflows that we're going to use Lang Graph to create. So the first one is what we're calling multi-agent collaboration. And so this is largely defined as you have multiple agents working on the same state of messages. So when you think about multiple agents collaborating, they can either kind of like share state between them or they can be kind of like independent and work on their own and then maybe pass like final responses to the other. And so in a multi-agent collaboration, they share the state. And so specifically this example that we've set up, it has two different agents and these are basically prompts plus LLMs. So there's a prompt plus an LLM for a researcher, and there's a prompt plus an LLM for a chart generator. They each have a few different tools that they can call, but basically the way that it works is we first call the researcher node or the researcher agent, we get a message. And then from there, there's three different ways that it can go. One, if the researcher says to call function, then we call that tool, and then we go back to the researcher. Two, if the researcher says a message that says final answer, so in the prompt we say if you're done, say final answer. So if the researcher says final answer, then it returns to the user. And then three, if it just sends a normal message, there's no tool calls and there's no final answer, then we let the chart generator take a look at the state that's accumulated and kind of respond after that. So we're gonna walk through what this looks like. We're first gonna set up everything we need. I've already done this. Importantly, we're gonna be using Langsmith here to kind of like track the multi-agents and see what's going on for a really good debugging experience. LangSmith right now is in private beta. If you don't have access, shoot me a DM on LinkedIn or Twitter and we can get you access. So the first thing we're gonna do is we're gonna define this helper function that creates an agent. So basically, we're gonna create these two kind of like agents up here. Um, and these agents are parameterized by an LLM, the tools they have, and a system message. And so then we're taking kind of like, uh, we're basically creating this mini chain, this very simple mini chain. That is a prompt, which has the system message in it and then it's a call to the LLM and it has access to some tools. So let's define this helper function. We're then going to find the tools that we want the agents to be able to call. So we'll have one search tool we'll use to really search for that and then we'll have one Python tool that can run Python code. Now we can start to create our graph. So first we're going to define the state of what we want to track. So we want to track the messages and so again like each agent will add a message to this messages property and then we also want to track who the most recent sender is and the reason we need this info is because after we call this tool we're going to check who the sender was and we're going to return to them. So this is just some state that's useful to track over time. So let's define this state. We're now going to define some agent nodes. So here we've created a little helper function to create an agent node. And so specifically this agent node is going to take in state, agent name, it's going to call the agent on the state, this agent node is going to take in state agent name. It's going to call the agent on the state. And it's then going to look at the result. We need to do a little bit of kind of like converting here. So the agent is going to respond with an AI message. But when we add that to the messages states that's accumulated over time, we want to represent that as a human message, actually, because we basically want the next AI that sees this to kind of like work with that. And so we're gonna, if it's a function message, then we're gonna represent it as a function message because it will just stay a function message. Otherwise, we're gonna cast it to a human message and we're gonna give it a specific name. We're gonna give it the name of the agent so that it knows basically is this message that it sees. Again, because we're keeping track of this global state. And so in the global state, when it sees a human message, is it from this researcher or is it from this chart generator? And so we're going to do that. And we're going to create two nodes. We're going to create this research agent node by basically partially now the agent node function. And then we're going to create two nodes. We're going to create this research agent node by basically partially now the agent node function. And then we're going to create this chart generator node. We're now going to define the tool node. This is the node that just runs the tools. It basically looks at the most recent message. It loads the tool arguments. It kind of, it will call it. It will call the tool executor. The tool executor is just a simple wrapper around tools that makes it easy to call them. And then it will call the tool executor. The tool executor is just a simple wrapper around tools that makes it easy to call them. And then it will convert the response into a function message and append that to the messages property. And now we need the edge logic. So this is the big router logic here, which basically defines some of the logic of where to go. So we look at the messages, we look at the last message. If there's a function call in the last message, then we go to the call tool or then we return call tool and we'll see what that leads to later on. If there's final answer in the last message, then we return end, otherwise we return continue. And so let's see how we use this in the graph. So first we create this graph. Let me run this miss, this L first. First we create this graph with the agent state. Then we add the three nodes that we have, the research node, the chart node, and the tool node. We then add this conditional edge. So the researcher, after we call the researcher, it uses the router logic. If the router returns continue, then we go to chart generator. So if the researcher returns basically something that doesn't have a tool and something that isn't final answer, then we return continue, in which case we go to the chart generator. If the router returns call tool, then we call the tool, and if it returns end, then we end. Similar with the chart generator. And then we add this conditional edge after call tool. So after we call the tool we basically look at who the sender is and if the sender is the researcher then we go to the researcher, if the sender is the chart generator then we go to the chart generator and we set the entry point to be the researcher node. Now we can invoke it. We'll use the stream method so that we can see things being printed out. We'll pass in this human message asking it to fetch the UK's GDP over the past five years, then draw a line graph of it, and then after we code it, finish it. And we'll run this. It'll take a little bit, so I'll probably pause the video here. I'll kick it off, pause it, come back when it's finished, and then we can see what it looks like in LangSmith as well. All right, so it's finished. So it was streaming out, so we can take a look at the whole stream of things in here, if I can figure out how to expand it. So we can see the researcher. You know, the first thing it does is it queries UK GDP data for the past five years, and then calls a tool, and then it kind of keeps on going. Um, at some point there's this chart. So, you know, it plot plots it over the past three years, it looks like. So it didn't quite get all the, all the data it needs. So you can see, uh, yeah, you can see it says that the data for years, 2018, 2019 is missing. Um, and so it plots that. And then this is just the final response. So that's why it shows up so big. If we want to see this in a little bit nicer view, because this is a lot to digest, we can go to Langsmith. And so Langsmith, at the top of the notebook, I set it so that it would log to this specific project, the multi agent collaboration, I can see that I've got one run, see some basic stats on it, like it took over a minute, it's got some tokens on it. If I click into it, I can now see exactly what was going on under the hood. So first it called the researcher and we can click into here and we can see the exact call that it made to open AI. And so there was the system prompt and then this was the user's first message and then output this. It then called to Tavilli, which is the search tool. We can see the results there. It then called the researcher again. We can see that there's now more in it because we start to build up this message bank. We can see it calls tovilly again. Okay, so now the researcher, and it doesn't call tovilly. Okay, interesting. Let's see what's going on here. So at the bottom, it basically says, okay, so now it's responding what the values are. And there's no tool call and there's no thing saying that it's finished so that means that it goes to the next one which is the chart generator. So it calls the chart generator under the hood that calls openai. We can see that down here we get a function call and this is running python code so it prints this out. Calls the python REPL. We can see that it runs this. The chart generator then goes, and the chart generator says, here's the line graphs, see the chart above, the researcher then goes, and it says, final answer. So now that there's final answer, it's finished, and that's basically what happened. So that's the multi-agent collaboration bit. The kind of like specific thing about this that's interesting is it has this global state of messages that each LLM sees and appends to. So it keeps on adding to this over time. And so on one hand, this is good because it allows each agent to see kind of like exactly the other steps that the other one is doing. And that's why we called it collaboration. It's very collaborative. The other version of this is that you have agents that have their own independent scratch pads and so we'll take a look at the next multi-agent example that we have which is this agent supervisor. So here we have the supervisor which basically routes between different independent agents. So these agents will be lane chain agents so they'll basically be an LLM that's run with the agent executor in a loop. The LLM decides what to do. The agent executor then calls the tool, goes back to the LLM, calls another tool, goes back to the LLM, and then then finishes. And then there's basically the supervisor. The supervisor has its own list of messages and essentially what it will do is it will call agent one. Agent one will go do its work but it will return only the final answer. Only that final answer is then added. The supervisor only sees this final answer and then based on that it can call another agent or it can call another agent. Another way to think about this is that this is essentially, you know, the supervisor's an agent and it's got a bunch of tools, and these tools themselves are agents. So it's a slightly different framing than before. It's not as collaborative. They're not working on kind of like the same sections. So let's get this started. Again, we're going to be, I'm gonna, let me restart my kernel. We're gonna be using LangSmith. So again, hit me up if you don't have access to that. First thing we're going to do is we're going to create some tools. We'll use the same two tools as before, the Tavilli tool and the Python tool. We'll next define some helpers again to create these agents as the individual nodes. So we have this create agent function similar to last time. The difference is now that it's actually doing a little bit more work under the hood. So it's not just a prompt plus a language model. It's actually creating an OpenAI tool agent, which is an agent class that we have in Lang chain. From there, it's creating an agent executor, which is also in Lang chain and we're returning that. That's the final agent. So let's run that. Then what we're doing is we're also creating, similar to last time, this agent node thing which does this human message conversion, just like we did last time. It's taking the agent result, which is an AI message and converting it into a human message with a name so we know kind of like where it's coming from. Now we create the agent supervisor. So this is gonna be the system that's responsible for looking at the different agents that it has and deciding which one to pass it on to. So here, you can see that we're defining a bunch of stuff. We're defining system prompt, your supervisor task with managing conversation between the following workers. And it's basically using this function definition to select the next role to send things to and it's or it's selecting finish finishes another option we're creating this prompt template we're creating this LLM and we're creating this supervisor chain which basically has a prompt it's got an LLM with some functions that it can call specifically we're forcing it to call this route function where it selects the next best role. And then we're parsing the response in JSON and we're using that to determine where to go to next. Now that we've got that, we can create the graph. So we've got this agent state, which is messages and then the next field. We've got this research agent. So we create the agent, we then create the node for it. So we partial out agent node. We then do the same for the code agent. And then we start creating the graph. So we create the graph with the agent state, and we add the three nodes. We now connect the edges in the graph. Okay, so for all the members, and again if we look at where members is defined, members is defined up here, so it's the researcher encoder, it's two, it's the two basically subnodes or subagents. So for those, after we call those agents, we go back to the supervisor. Then we also have this conditional edge. So from the supervisor, after the supervisor is run, we look at what's in next, and we look in the conditional map. And the conditional map is basically a mapping between the members and finish. So if it says researcher, then we go to research node. If it says coder, then we go to coder node. If it says finish, then we finish. We set the entry point to the supervisor, and then we run the graph. Now we can do the same thing. We can invoke the team. We can see what happens and see how it performs. Alright, so it runs it. You can see that it calls the coder. So here we're asking it to print out or to say hello world and printed to the terminal. So it calls the coder and then it finishes. Here we can write a brief research report on PICAs and it will do that. While that's running, we can maybe look in Langsmith to see the previous ones. So here we have the coding one. We can see that we first called the supervisor agent. It's got this call to OpenAI. Again, it's got this system prompt and then the human and then it decides what to do next. And it's using this function call. We can see it has this function definition of route here. Based on that, it goes to the coder. So now there's the coder agent. The coder agent does several things under the hood. It calls OpenAI first, it does a function call, it then calls the Python REPL, and then it calls OpenAI again. And so when we go back to the supervisor, if we look up what the supervisor says, it just sees the response from the Coder. So there's a human message with the NameCoder, and it gets this response. It doesn't see any of these calls though, or any of the calls to the Python REPL and then we get a response and it says to finish. So this is an example of where you know there's many calls but they're kind of hidden within one agent state. We can also see that it finished writing the research report on PICAs so if we go back here we can see that it's finished running and if we take a look it now calls the researcher agent under the hood and we can see that it's finished running. And if we take a look, it now calls the researcher agent under the hood. And we can see here, this is interesting because this took a pretty long time, 45 seconds. We can quickly drill in and see that the longest call was this call to OpenAI inside the researcher agent, where basically it has all these information from the internet and it writes this long research report. So that's pretty much it for the agent supervisor. Now I want to talk about the third and final type of agent that we have an example of, which is the hierarchical agent teams. So this is very similar to the previous concept. It's just changing what's going on in the sub things. So now each agent node is itself a different kind of like supervisor agent setup. And so what this means is that it's going to be a lot more kind of like a VLANG graph and a little bit less of VLANG chain. We can get started by setting up the environment, similar to last time. We now create the tools. So there's two different sets of tools that we need to create. One's for the research team, this is searcher and web scraper. The other is for this document authoring team, and it's these three tools here. So we can do that, we can load to Vili, we can create our scrape web pages tool, and then we can start creating these other tools as well. And these are for the writer team. So we create one that creates an outline, reads documents, writes documents, edit documents, and then a Python REPL tool as well. We're going to create a few helper utilities that make it easier to basically construct this pretty complicated graph. So we have one that creates an agent, we then have an agent node, and we then have this team supervisor idea. And so these are kind of like taking a lot of the concepts that we had in the last notebook and extending them. Now we can define these agent teams, they're hierarchical teams. So we have this research team. This research team has a few different attributes on it. The main things here are this messages. We initialize an LLM. We create the search agent and then the search node, the research agent and then the research node, and then we have the supervisor agent as well. We now put this all together. I'm not going to go over all the edges because it's very similar to some of the notebooks that we did before, but the end result is that we have this graph that we create. In order for this graph to be used in as a sub graph and another component, we're gonna add this enter chain function, which basically is just a converter to make it easier to use. We can use this directly. So we can try out, and again, this is just one of the sub teams, but we can try asking it a question. And we can see that it does a search. It has a search tool, so we'll get a response from there and it will give us an answer. We can see that we get back a response here. And if we look at that in Langsmith, we can see that it's doing a pretty similar thing to before. It's got the supervisor agent, it's calling search, and then it responds with the supervisor. We're now going to put together another agent or another graph of agents. So it's this document writing team. So here we are defining a bunch of stuff. We're defining the state that we're tracking. It's largely these messages. There's then some logic that's run before each worker agent begins. So that's more aware of the current state of everything that exists. This is basically with populating relevant context. We then create a bunch of agents and a bunch of agent nodes. And then we create the doc writing supervisor. Now that that's done, we can create the graph. So we're adding a bunch of nodes. We're adding a bunch of edges. we've got some conditional routing edges, we set the entry point and now that we have this we can do something simple like write an outline for a poem and write the poem to disk. We can see that this is finished and if we look at this in Langsmith we can see that it also looks fairly similar as to what we were doing before. We've got the supervisor, we do a bunch of note taking, and then the supervisor finishes. It doesn't see any of the internal things, it just sees the final response. And we can now put this together. So we now have a layer on top of this. We've got this team supervisor and now we have this top level graph state. It's just a list of messages. We have some helper functions. We start creating the graph. We add the research team as its own node. It's got a little bit of, uh, it's got a little bit of, uh, wrappers around the previous chain to like get the previous message and then pat, and then at the beginning, pass it in that into the chain and then call join graph to join it with the rest of the stuff. Same, uh, uh, with the paper writing team. We then add the supervisor. We then add some nodes. We compile it. All right. And so now we can do a more complex thing where we call, ask it to write this brief research report on the North American Sturgeon, include a chart. So let's kick this off and I'll come back when this is done. All right, so it's done. Took a while. If we go to Lang Smith, we can see that we have this most recent trace. Took 400 seconds. If we click inside it, we can see that there is a lot going on. So a lot of calls down the hood. It looks like there's this big call to the paper writing team at one point that took a while There was also this big call to the research team to start that took a while I can click into any of these I can see like what exactly is going on at any point in time What exactly was happening? So this is pretty helpful for understanding what's going on Yeah, wow That's a lot of tokens Anyways, yeah, that's pretty much it for the multi-agent stuff. Hope this was helpful. I think, uh, you know, the, the benefits of thinking about things as a multi-agent way are really that it, I think it helps compartmentalize what you expect different functions to be happening inside this system. So, you know, each agent basically has its own prompt, its own tools, even its own LLM at times. And so you can really focus it on just doing specific things, as opposed to having one general autonomous agent with lots of tools. And so even if you, you know, even if you have all those same tools and you just organize it a bit more hierarchically, that can often be helpful for doing kind of like complex tasks as well as just a good mental model to how to think about things. So hopefully this has been a good introduction to that. These are again, probably only a few of the ways that you can create multi-agent workflows with Lengraph. Really excited to see what other ones emerge.


video_id: YE6A5d8kNp4
In this video, we'll cover how to add persistence to LandGraph agents. So we've added a method in LandGraph where when you create the graph, you can specify a check pointer. And this check pointer will save the state of the graph at each iteration. So if you want to resume from that same state, you easily can. One really obvious use case of this is giving the agent's memory. So you can basically resume previous conversations with the agents and it will have the exact memory of everything that they've done up until that point. Or more precisely, they will have memory of what the state was when they last interacted with that checkpoint. So let's take a look at an example of how to do this. So in this notebook, we can set up our OpenAI, Tavilli, and Langsmith keys. We can start, and this is just creating a simple message graph agent. If you haven't watched the videos on how to create these types of Lang graph agents before, go check those out. There's much deeper dives on them. We'll just create a pretty straightforward one. It will have access to one tool, a Tavili search tool. We'll create the tool executor. We'll be using OpenAI for this demo, although you can use any model you want. We'll create, we'll bind these functions to the models, to the model. And now we'll create our graph. This is just going to be a really simple, so we're going to use a really simple message graph down here. So we're going to have one function that basically takes in a list of messages and determines whether it needs to call a function or whether it should continue. When it, when we call the model, we're just going to invoke it on the model and then return back the response. And then we're also going to have this node that just calls tools. So these are standard. These are standard nodes that we're going to create. We'll create the graph. We have this agent node. We have this action node. We set the entry point to be agent. After the agent, we see whether we should continue. We either finish or we go to the action. And after the action, we go back to the agent. This is all standard kind of like agent graph stuff that we've done in other videos. What's now new is that when we compile this graph, we're going to pass in a check pointer. Specifically we're going to pass in the SQLite check pointer. So it's called SQLite Saver. From connection string we're connecting to it in memory, although we could easily connect to a database that's in disk or anywhere else. So we're going to create this. We're going to pass this into a workflow.compile and now we have our app. So now we can interact with this agent. So let's create our first message and let's say, hi, I'm Bob. We'll pass this in and notice that we're passing in this thing right here, which has a thread ID. So this is, so configurable is basically just ways to configure specific attributes on it. And thread ID is a way to configure the check pointer. So what will happen is we'll send the inputs in and they'll be associated with this thread ID equals two. And so if we want to resume from that, then we can just use the same thread ID. If we wanna start again, we can use a separate thread ID. So here we'll pass this in. We'll print out the results. Here is just, hello Bob, how may I assist you? Here we'll pass in another one. And so notice that we're only passing in what is my name. And so if we did this and we didn't have this persistence layer here, then it wouldn't know what the name is. But since we added this and we didn't have this persistence layer here, then it wouldn't know what the name is. But since we added this persistence layer, we can see that it remembers that your name is Bob. And this is because we're using the same thread ID. If we switch thread IDs and set thread ID equals a 3 and pass in the same question, what is my name? It doesn't remember it. And that's because there's nothing in that state before. This video hopefully showed you how to add state or add persistence to your land graph agent. We showed how to do it with the message graph. So it's just remembering this list of messages, although this works generically for the state graph as well. So if you want to save other aspects of state, it can absolutely do that. And with this thread ID configuration, you can easily resume conversations from where you last left off. Thanks.


video_id: ylrew7qb8sQ
Ever wondered how to build an AI agent that can see? In this tutorial, you will learn how to create an autonomous agent that can surf the web and assist you in performing complex tasks solely using vision. Hi all, it's Will from LangChain. Today I'm going to be teaching you how to build a vision-enabled web browsing agent using LangGraph. LangGraph is an open source framework from the Lang chain team that makes it easy to balance expressivity and control when building cyclic type LLM workflows such as those commonly used for AI agents. If you're not familiar with Lengraph, we've got a nice introductory series of videos on YouTube that you can check out as well as a number of examples in this repository here that you can see to see more basic examples of how to build with it. Hopefully through the course of building this vision enabled that you can see to see more basic examples of how to build with it. Hopefully through the course of building this vision enabled agent, you'll see a little bit more and understand a little bit more about what types of workflows that it really empowers you to build. For our example, we'll be building this based on the Web Voyager paper by He Erdao of Zhejiang University. They haven't released the code yet, so this is my best impression or understanding of how the agent operates based on the description in the paper. The agent itself is a basic reasoning and action loop type of agent. If you're familiar with React style agents, what it does is it generates a chain of thought saying why it's gonna make the next action to improve its accuracy there, and then it'll give the action for the tool, and then the arguments is going to be passing that, and then that will then call an API, and then we return and decide to continue to loop until the LLM decides it has the answer, has accomplished the goal that it was set out to accomplish. Once that state is reflected, then it returns that output or response to the user there. When building a web browser agent, there's a number of considerations that you're gonna have. For one, if you're just feeding it raw text, it's going to have a lot of useless information that is all going into this machine code that actually isn't helpful in getting it to the end state. In general, when building agents, you want to reduce the complexity as much as possible, reduce distractions as much as possible so that you can make it as resilient as possible and give it a better likelihood of actually accomplishing its end goal. So the web was designed for human eyes. But even then, if you pass in a big screenshot like this, it's kind of hard for it to get that level of precision just from generating text. So they have a nice approach that uses this set of marks style bounding box annotation that will generate these bounding boxes around the different things. And then it can then select these elements by number and then use the mouse in order to make these clicks and other types of actions. This is a nice way of creating affordances for the UI affordances for the agent so that it can actually interact in a more successful way. If we look at this diagram I made, the basic structure is as follows. You have the state of the graph which is basically the web browser that we annotate with this bounding boxes. You have the question, the trajectory, which is just the past series of actions that it's taken so that it knows not to repeat itself or it knows what it's tried before. You have the LLM imprompt here encoded by the brain. And then you have the tools. Tools in general are just the API between the LLM and the outside world. In our case, the outside world is the web browser. So we have things like scrolling, typing, clicking, waiting, going back and going to search engine. I put this together pretty quickly. I haven't optimized it at all, so a lot of these examples could probably be made more resilient by either giving it more tools, increasing the parameter size or improving the prompts, etc. And so a lot of this type of building, you can probably optimize more for your use case. One thing that makes it really easy to do that is Lengsmith. So you can click here and you can sign up for an account, I already do, and this gives us a nice set of traces that you can look through and help debug and develop your application regularly. We'll get back to this a little bit later once we actually start running code. So once you've signed up for that, again, it's optional, but highly recommended. And if you need to get an account or get off the wait list, you can feel free to follow up with us offline. You can email us at support at linechain.dev. So we'll set up the API key here, and then we're gonna be sending all of these to this little project here, to a web voyager here, and then we're going to be sending all of these to this little project here to a web voyage related, and then installing the requirements. We required the playing graph, this package, and then Langsmith for tracing, and the Langtain OpenAI wrapper, which gives a runnable API to the OpenAI SDK. Then if you haven't already, you can install Playwright. Playwright is the browser technology that allows the LLM to be giving control to that. You could also use Selenium or other things like that. It's not super important for this case. And then this is just a step that allows the async browser to be operating in a Jupyter Notebook. So wouldn't need that in production, but for the sake of the Notebook, I could do that. All right, on staging. Basically, when you're creating a graph, this is a stateful production, but for the sake of the notebook, I could do that. All right, on staging. Basically when you're creating a graph, it is a stateful graph and you can think of it as a state machine. So then what dictates the next step is both the state and the series of connections within the graph to determine what we're gonna be doing next. In our case, we've got a number of elements here. You've got the web page itself. So this is a page within Playwright. You've got the user input, and then you've got some things like the image and the bounding boxes, as well as its prediction, and then you have some other things like the observation of the tool, once it actually does and clicks or types on things. It's just a simple string. And then the list of messages, so if it's going to be saving additional information in the messages, we can keep track of it there. You can try tweaking this to see if you can get better performance than what I've already configured here. Once you define that state, we'll start creating the tools, which again are the API between the LLM and the browser. You see in land graph and in the stateful graph, we accept this state into each of these tools and see to these functions, which are nodes in the graph. So in this case, the click operation, where we get the page and then the arguments that are passed by LLM, which in this case are the bounding box that we have, as well as the, that's actually it. The bounding box then we can look up in this list of B boxes above, and we can get the x yy coordinates which are the pixel level information about where we're going to click. The other functions are defined similarly. So for typing text, again, you have a bounding box and then you also have the text content that you're going to be typing into it. So if it needs to search, if it needs to be filling out forms, you can use this. We've got some information to overwrite here and all that other stuff. We've got this information to overwrite here and all that other stuff. We've got this scroll operation. Right now I'm keeping it simple and not even letting it tell the tool how far to scroll. This could definitely be improved, but it lets it go and navigate about whenever you can't see all of it in a single browser view. It can wait for things to load, it can go back a page as it traverses, and it can jump back to Google, which is kind of like a get out of jail free card here in this case. I'll run this cell, and I forgot to run the Faridah cell. Here we go. Now that we have the tools, we can actually define the agent, and this is kind of the fun part. The agent, as we mentioned before, is really composed of a couple things. There's the annotation part, so the thing that actually goes and takes the screenshots and generates the bounding boxes, and then that image will be passed to the LLM, and then there is the prompt and the LLM. Here I have a JavaScript file that's also in this repository. It just generates bounding boxes so that we can screenshot it. And we'll just unmark the page afterwards so that we don't have those bounding boxes follow us along as we go and navigate the web. This would be the annotate function. And then we have this parsing function here. So when the LLM generates raw text, we take that and then we look for this action prefix and then whatever arguments are semi colon separated and return it there. Again, you can try playing around with different formats and other types of things to make this more robust if you need to. We are storing the prompt on the Langsmith hub. You can see what it looks like here. I'll go have this prompt ready, then you can compose it with the LLM and the AGM. This pipe syntax, if you're not familiar with it, is basically a part of root-linked and expression language. All it does is it creates a new object by composing these together. So the output of prompt goes into LLM, which then goes into this output parser thing that takes the text from the overall output message and then this function that we just defined above, which is parse. This is a single unit and we're assigning it to this variable prediction in the output and then composing that with annotate here. So this annotate function again, here is what takes the browser and generates the bounding boxes. Once you define this, if you wanna see in a more visual format, so scroll that. You can even get graphed, ASCII, and now I'll need to print that. And you can see basically what I was saying. So again, it's all piping, similar to bash piping. The data will flow through these different pipes and then the output of this will be assigned to the prediction value here. And then the output of this, whatever keys are going into here will then be joined in the output. My Jupyter was messing up there for a second. If you want a quick view of what the bounding boxes look like, I pulled up this screenshot here so you can see we just go and look for things with Ahrefs basically and then we'll draw these things around it. The LLM thing is to say, I'm going to click on number 18 here. And so then we can resolve those using the bounding boxes to these things here. Makes it easy again to help the agent interact with this world. Once we go here, we are trying to start creating the actual graph. We've got one more function that we need to define in order to finish our workflow here, which is the update scratch pad. Basically anytime this agent will loop, we want to be in order to finish our workflow here, which is the update scratchpad. Basically, anytime this agent will loop, we want to be able to format the observations, the tools into a structure that we can then put back into the prompt eventually. So what this does is it looks at the string output of those functions that we defined above as tools. And we then will be pressing a little bit and then just putting it into this list of previous actions and we'll list number of steps And putting down this ScrapShop, scratchpad variable. When we investigate the traces of Langsmith light, you can get a better sense of how this is working Now it's time to actually compose the graph. So if you're not as familiar with Lang graph I'll take a little bit longer to describe these two these operations for you If you are very familiar with it, then you can probably skip forward a few seconds. The basic steps here are going to create this graph builder object from the StreetGraph, which then takes in that agent state above this type that we defined above, and that is basically the state that is part of the state machine. So anytime a node returns, it updates the state. We then start adding nodes and defining edges. Nodes again do the work. Edge define how the work is then routed between each of the different units. So we add the agent that we did above. We're going to set it as the entry point. So when the user calls it, it first goes to the agent. Next, we're going to add this update scratch pad function because we just created it. And we're gonna say every time the update scratch pad function returns, next pass that state over to the agent. Again, this is because we're completing the loop there from when anytime the tool is called and it goes back to the agent to make the decision what's due next. After that, we've got this list of tools here that we're gonna give each a name. And we're going to add them to the graph. And then we're going to add an edge that says anytime this tool completes, go back to that update scratch pad node we defined here. And then finally, before I get too far, let's see, is again that piping expression language syntax. All this does is it says the tool function that defined above first do that and then wrap that in a key called observation. So it takes the string, puts that in, and then says this is the observation key that you turn from it. And again this is because we defined the observation in the Landgraf state. It'll then update that after this node is called. We finally will create this single conditional edge that will determine the whole structure of how logic is routed. Basically after the agent runs, this function will be called and then it will take the output of the agent and then decide which of the other nodes to go to. So if the action is answer, then we will end. And so the user can see the final result that the agent returned. If the action is retry, which if you look back above, is just output if you have a parsing error or something like that. This lets the agent handle issues if it outputs in the wrong structure. Otherwise, return the actual action itself. Since we named the action the same value that we put in the prompt for the LLM, we can just directly have it output and it'll route it right back to that node. So do this. The compile step just takes this builder and creates the graph itself. And then it's time to play around with it. Let's see how we did. First, we're going to create the actual browser object. And then there's a helper function which will just print all the things out to the notebook so we can watch it running in real time. So notice it opened the browser and now I get it to Google. I'll go back here. First, all right, for our first question we're going to ask, can you explain the Web Voyager paper on archives? So I'll give it a little guidance here. As you remember, LMS is today, they haven't been trained on data that's very recent, so it certainly doesn't have baked in knowledge within its parameters of this paper. So it needs to use the web browser to see. So you can see first it sees, it says, I'm gonna type in Web Voyager Paper Archive, it knows how to Google good. And then it clicks number 27, so this little paper here. And then once it actually decides to move on from there, you can see it opens it up. And you can see follow along also in the browser here. You can see it's opening there. And then it responds to the answer. So it presents a study on building engine and web agent with large multimodal models. It's an advancement and WebPBT models, detailed architecture training, okay, so it doesn't it's a little bit lazy It just likely details this it doesn't actually go and scroll through but you know, they got the gist, it was able to get there So let's give it another question. Maybe explain today's xkcd comic for me. Why is it funny? So I'm not sure how much of a sense of humor these models have but we'll see if I can explain It's starting from the same point that we left off before because, as we mentioned, the browser is just there. So it knows to go back to the start and then it searches today's xkcd comic and it goes and goes to the search bar there, finds it. Oh, you see it has these things here and then it gets all these nice pop- ups, but it's able to stay true and ignore them for now. And we'll see if it can click. Number 24, which one is that? Actually takes me a while. Oh yeah, it just clicks the link there. This is on a slight delay. So you can see it actually did open it up. And what I like about this little display is it shows what it looks like to the agent as you go through. So it gets the final response and says, shows an image, seems to be curious about the greenhouse effect. It actually has a timeline with two significant historical events. It talks about James Watt, who kicked off the Industrial Revolution, and then some of it. So the humor is age exclusion, the early recognition of the seriousness. It's been a while. So we haven't made that much progress. A little bit of a depressing comic, but a classic for XKCD. We can see if it knows how to find what the latest blog posts are from Langchain. Again, it may or may not know what Langchain is, but it knows how to use these tools because we've given it instructions. So it starts at XKCD, looks up Google, and then we'll see what it does after that. These first couple of questions follow a similar pattern where it goes to Google, it does a search, does a couple of clicks, relatively straightforward. So maybe we can see if it can do a more complicated task next. It's going through, opens this up, and then it's able to read that it has open GBTs and multi-agent workflows and lane graphs. So look at that, it seems like we're actually getting the right stuff. Again, this model is only taking in the image and then some of the instructions that we're having. So it's kind of impressive it's able to read all of this from the browser. Let's give it a more challenging one. Let's tell it to search a one-way flight from New York to Reykjavik for one adult and analyze the price graph for the next two months. So lots of steps. We implicitly, if you remember above, give it only 150 steps to complete so we can see if it can complete on time. So it's starting from the Lang chain blog and see it goes back to Google there to start out. Okay agent, we'll see if it can use Google Maps. This is a little bit more challenging since it's less of a strict old fashioned web page. You should have to navigate different things within it. We'll see what time it needs or I should leave to get to SFO by 7 o'clock, starting from SF downtown. And then let's see what it does. It'll take a little bit of time. So again, it still is to Google, looks at Google Maps. We can even follow it in real time on the browser. It looks like it's opened it up. What is it going to do next? All right, for our last one, we'll ask it something a little bit different. We'll see if it can navigate Google Maps. So what time should I leave to get to SFO by 7 o'clock? Starting from SF downtown. And this one will probably take a little bit of time, and we'll see how it does. Starts by searching for, let's have downtown. So it doesn't even start by going to Google Maps. 10, not sure where it's going actually. Oh, there it goes, Google Maps. It's got there. Looks like it is thinking at seven a.m. because I haven't passed it the current time. So that's another thing that I can probably improve. at seven a.m. because I haven't passed it the current time. So that's another thing that I can probably improve. X zero. It's X out of there. A little interesting. Looks like this is what it's saying here. So it's going through and clicking a few things. Looks like the driver had an error here, but it's gonna try to get through some things. And it's starting a little bit. One thing that we can do now is as good a time as any to check out is you can look at all these operations within Langsmith. And then you have this whole record that you can share with your team to go and debug exactly the steps and inputs that you had that led to each situation. So you can look at all these LLM calls, which again corresponds to those agent calls to see the series of steps it had. So again, we asked it, so if you check here, and then this is what it was fed in. And then you can say it thinks, so fine, I need to search for this information on Google Maps. First, it's going to type and see how it's done Tom. It goes through and makes some decisions. You can check SF here. You can look at click 10. These things. You notice that we're only passing the most recent screenshot. We're not passing previous ones to show these different observations or anything like that. Maybe it's something you could try out later on to see if it improves. You can go to some of these decisions. And it looks like it's set here. And it says, excuse me for this, I should initiate a search for directions on the map. And this is click 0. And so that's a little bit interesting. One of the things that I like to do here is you can open up in the playground and then say maybe if I updated the instructions here, I can actually test it out. So it's not actually trying to click zero, it's trying to type zero. So maybe I can go in and change some of these instructions here or say web browsing and say, remember to type when needed as this will also select and submit the request for you or something. Again, I am not an absolute prompt engineer, so we'll see how that goes. And then once you have that, I've already added my API key and you can hit start. That's GPT-4-V, so it takes a little bit of time, but then you can start seeing what it will take. So yes, oh, this is the search query here, it's already entered and so it's good. So you can make some updates, you can keep seeing what it will take. So yes, oh, this is the search query here, it's already entered. And so it says, so you can make some updates, you can keep on changing around things. Maybe we'll say it's clicked here, et cetera. I think one other issue with this is it seems like the bounding boxes aren't correct, so you can improve that actual affordance layer. But again, all of this lets you go and see the exact steps you needed to take and then go and play around with it to improve in the UI without having to change any of your code initially. You can go and step through each of these steps and then see exactly what has been done. We'll go back here. It seems like it's getting a little bit closer, but it ran out of steps. It's a little bit too challenging for this agent using the existing setup. And so we will have to go in and make some improvements if we wanted it to be able to do something like this. Anyways, I think that's enough for today. Hopefully, I've shown you a couple of things. You've learned how to build something that looks like this Web Voyager paper, which I think is a pretty cool set of ideas. I've taught you, and you've been able to build something with line graph, which lets you go and write regular Python code or join the JavaScript code as well. It's just compose everything together in a nice little graph that you can then build and improve to balance some of this control with some of that more magical AI type of experience. And you've also gotten something that can go and browse the web for you. So I guess, again, if you want to get access to LangSmith or you want to try building this out, you can check out the LangGraph repo to see these examples that we've shared, or you can sign up for LangSmith at smith.langchain.com. And finally, don't forget to subscribe to the LangChain channel. This will help you stay on top of all the most recent AI advancements and give you nice tutorials such as this on how to build different types of agents, RAG, evaluations, and all those types of things. We'll make sure that we keep the content fresh and that you stay up to date. So thanks again and until next time.


video_id: pbAd8O1Lvm4
Hi, this is Lance from LangChain. I'm going to be talking about using LandGraph to build diverse and sophisticated RAG flows. So just to set the stage, the basic RAG flow you can see here starts with a question, retrieval of relevant documents from an index, which are passed into the context window of an LLM for generation and answer grounded in the retrieved documents. That's kind of the basic outline. And we can see it's a very linear path. In practice, though, you often encounter a few different types of questions, like when do we actually want to retrieve based upon the context of the question? Are the retrieved documents actually good or not? And if they're not good, should we discard them? And then how do we loop back and retry retrieval with, for example, an improved question? So these types of questions motivate an idea of ActiveRAG, which is a process where an LLM actually decides when and where to retrieve based upon existing retrievals or existing generations. Now, when you think about this, there's a few different levels of control that you have over an LLM in a RAG application. The base case, like we saw with our chain, is you just use an LLM to choose a single steps output. So for example, in traditional RAG, you feed it documents and it decides to generation. So it's just kind of one step. Now, a lot of RAG workflows will use the idea of routing. So like given a question, should I route it to a vector store or a graph DB? And we have seen this quite a bit. Now, this newer idea that I want to introduce is how do we build more sophisticated logical flows in a rag pipeline pipeline that you let the LLM choose between different steps, but specify all the transitions that are available. And this is known as what we call a state machine. Now, there's a few different architectures that have emerged to build different types of RAG chains. And of course, chains are traditionally used just for like very basic RAG, but this notion of a state machine is a bit newer. And Langraph, which we recently released, provides a really nice way to build state machines for RAG and for other things. And the general idea here is that you can lay out more diverse and complicated RAG flows and then implement them as graphs. And it kind of motivates this more broad idea of like flow engineering and thinking through the actual like workflow that you want and then implementing it. And we're going to actually do that right now. So I'm going to pay a recent paper called CRAG, corrective rag, which is really a nice method, for active rag that incorporates a few different ideas. So first you retrieve documents, and then you grade them. Now, if at least one document exceeds the threshold for relevance, you go to generation to generate your answer. And it does this knowledge refinement stage after that, but let's not worry about that for right now. It's kind of not essential for understanding the basic flow here. So again, you do a grade for relevance for every document. If any is relevant, you generate. Now, if they're all ambiguous or incorrect based upon your grader, you retrieve from an external source. They use web search, and then they pass that as their context for answer generation. So it's a really neat workflow where you're doing retrieval, just like with basic RAG, but then you're reasoning about the documents. If they're relevant, go ahead and at least one is relevant, go ahead and generate. If they're not, retrieve from alternative source and then pack that into the context and generate your answer. So let's see how we would implement this as a state machine using LandGraph. We'll make a few simplifications. We'll make a few simplifications. We're going to first decide if any documents irrelevant, we'll go ahead and do the web search to supplement the output. So that's just like kind of one minor modification. We'll use tabularly search for web search. We'll use query writing to optimize the search for, to optimize the web search. We use query writing to optimize the search to optimize the web search. But it follows a lot of the intuitions of the main paper. Small note here, we set the Tabela API key. And another small note, I've already set my Langsmith API key, which we'll see is useful a bit later for observing the resulting traces. Now, I'm going to index three blog posts that I like. I'm going to use ChromaDB. I'm going to use OpenEye Embeddings. I'm going to run this right now. This will create a vector store for me from these three blog posts. And then what I'm going gonna do is define a state. Now this is kind of the core object that's gonna be passed around my graph that I'm gonna modify. And right here is where I define it. And the key point to note right now is it's just a dictionary. And it's gonna contain things that are relevant for rag, like question, documents, generation. And we'll see how we update that in a little bit. But the first thing to note is we define our state and this is what's going to be modified in every node of our graph. Now here's really the crux of it. And this is the thing I want to zoom in on a little bit. Here's really the crux of it. And this is the thing I want to zoom in on a little bit. So when you move from just thinking about prompts to thinking about overall flows, it's like a fun and interesting exercise. I think about this as it's been mentioned on Twitter a little bit, more like flow engineering. So let's think through what was actually done in the paper and what modifications to our state are going to happen in each stage. So we start with a question. You can see that on the far left. And this kind of state is represented as a dictionary like we have. We start with a question. We perform retrieval from our vector store, which we just created. That's going to give us documents. So that's one node. We made an adjustment to our state by adding documents. That's step one. Now we have a second node where we're going to grade the documents. And in this node we might filter some out. So we are making a modification to state, which is why it's a node. So we're going to have a grader. Then we're going to have what we're going to have a grader. Then we're going to have what we're going to call a conditional edge. So we saw we went from question to retrieval. Retrieval always goes to grading. And now we have a decision. If any document is irrelevant, we're going to go ahead and do web search to supplement. And if they're all relevant, we'll go to generation. It's a minor kind of logical decision that we're going to make. If any are not relevant, we'll transform the query, and we'll do web search, and we'll use that for generation. So that's really it. And that's how we can think about our flow and how our state's gonna be modified throughout this flow. Now, all we then need to do, and I kind of found spending 10 minutes thinking carefully through your flow engineering is really valuable because from here, it's really just implementation details. And it's pretty easy, as you'll see. So basically, I'm going to run this code block, but then we can walk through some of it. I won't show you everything, so it'll get a little bit boring. But really all we're doing is we're finding functions for every node that take in the state and modify it in some way. That's all that's going on. So think about retrieval. We run retrieval, we take in state, and modify it in some way. That's all that's going on. So think about retrieval. We run retrieval, we take in state, remember it's a dict, we get our state dict like this. We extract one key question from our dict. We pass that to our retriever, we get documents, and we write back out state, now with documents key added, that's all. Generate's going to be similar. We take in state, now we have our question and documents, we pull in a prompt, we define an LLM, we do minor post-processing on documents, we set up a chain for retrieval, or sorry for generation, which is just going to be take our prompt, plumb that to an LLM, parse the output to a string. And we run it right here, invoking our documents and our question to get our answer. We write that back to state. That's it. And you can follow here for every node, we just define a function that performs the state modification that we want to do on that node. Grading documents is going to be the same. In this case, I do a little thing extra here because I actually define a Pydantic data model for my grader so that the output of that particular grading chain is a binary yes or no. You can look at the code. I'll make sure it's all shared. And that just makes sure that our output is very deterministic so that we then can, down here, perform logical filtering. So what you can see here is we define this search value, no. And we iterate through our documents. We grade them. If any document is graded as not relevant, we flag this search thing to yes. That means we're gonna perform web search. We then add that to our state dict at the end, so it run web search, now that value is true, that's it. And you can kind of see we go through some other nodes here as a web search node. Now here is where our one conditional edge we define right here. This is where we decide to generate or not based on that search key. So we again, get our state, let's extract the various values. So we have this search value now. If search is yes, we return the next node that we want to go to. So in this case, it'll be transform query, which will then go to web search. Else we go to generate. So what we can see is we laid out our graph, which you can kind of see up here. And now we define functions for all those nodes, as well as the conditional edge. And now we scroll down. All we have to do is just lay that out here, again, as our flow. And this is kind of what you might think of as like kind of flow engineering, where you're just laying out the graph as you drew it, where we have set our entry point as retrieve, we're adding an edge between retrieve and grade documents. So we went retrieval, grade documents, we add our conditional edge, depending on the grade, either transform the query, go to web search, or just go to generate. We create an edge between transform the query and web search, then web search to generate, and then we also have an edge generate to end. And that's a whole graph. That's it. So we can just run this. And now I'm going to ask a question. So let's just say, how does agent memory work, for example? Let's just try that. And what this is going to do is going to print out what's going on as we run through this graph. So first we can see output from retrieve. This is going to be all of our documents that we retrieved. So that's fine. This is from our retriever. Then you can see that we're doing a relevance check across our documents. And this is kind of interesting, right? You can see we are grading them here. One is great as not relevant. And okay, you can see the documents are now filtered because we removed the one that's not relevant. And because one is not relevant, we decide, okay, we're gonna just transform the query and run web search. And you can see after query transformation, we rewrite the question slightly. We then run web search. And you can see from web search, it searched from some additional sources, which you can actually see here. It's appended as a... So here it is. So here it's a new document appended from web search, which is from memory knowledge requirements. So it basically looked up some AI architecture related memory web results. So that's fine. That's exactly what we want to do. And then we generate a response. So that's great. And this is just showing you everything in kind of gory detail. But I'm going to show you one other thing that's really nice about this. If I go to Langsmith, I have my API key set, so all my generations are just logged to Langsmith. And I can see my Lang graph run here. Now what's really cool is this shows me all of my nodes. So remember we had retrieve grade, we evaluated the grade because one was irrelevant. We then went ahead and transformed the query. We did a web search. We've tended that to our context. You can see all those steps are laid out here. In fact, you can even look at every single grader and its output. I will move this up slightly. So you can see the different scores for grades. Okay, so this particular retrieval was graded as not relevant. So that's fine, that can happen in some cases. And because of that, we did a query transformation. So we modified the question slightly. How does the memory system and artificial agents function? So it's just a minor rephrasing of the question. We did this heavily web search. This is where it queried from this particular blog post or medium. So it's like a same web query. We can like sanity check it. And then what's neat is we can go to our generate step, look at open AI and here's our full prompt. How does the memory system in our official agents function? And then here's all of our documents. So this is the web search, as well as we still have the relevant chunks that were retrieved from our blog posts. And then here's our answer. So that's really it. You can see how really moving from the notion of just like, I'll actually go back to the original, moving from... I will try to open this up a little bit. Yeah, I can see my face still. The transition from laying out simple chains to flows is a really interesting and helpful way of thinking about why graphs are really interesting because you can encode more sophisticated logical reasoning workflows, but in a very clean and well-engineered way where you can specify all the transitions that you actually want to have executed. And I actually find this way of thinking and building logical workflows really intuitive. We have a blog post coming out tomorrow that discusses both implementing self-RAG as well as C-RAG for two different active RAG approaches using this idea of state machines and LandGraph. So I encourage you to play with it. I found it really intuitive to work with. I also found inspection of traces to be quite intuitive using LandGraph because every node is enumerated pretty clearly for you, which is not always the case when you're using other types of more complex reasoning approaches, for example, like agents. So in any case, I hope this was helpful and I definitely encourage you to check out this notion of flow engineering using LandGraph and in the context of RAG it can be really powerful, hopefully as you've seen here. Thank you.


video_id: uRya4zRrRx4
Hi all, this is Will from LangChain. Today I'm going to walk through how to create, plan, and execute style agents in LangGraph. For those of you not familiar with LangGraph, we've got a great set of videos on YouTube that you can go through to get a little more of an introduction. But a short summary of it would be that it's a framework built on top of LangChain Core that provides a graph-based syntax that gives you a great balance between expressivity and control when building things like agents and state machines that require loop type workflows. You can go and you can install it here, check out the repo in the Lineshane AI organization in GitHub. I'm particularly excited about this video because plan and execute style agents bring agents a step closer to being production ready in general. They can help you have faster execution time, lower token costs, and overall have the promise to have better reliability relative to previous generations of agents. I guess before we jump too far ahead, we'll go give a brief background on some of LLM agents by using React as an example. So a little over a year and a half ago, I guess now, Shen Yu Yao from Princeton proposed the React paper, which stands for Reasoning in Action, because of the way it prompts language models to reason about the types of things that it needs to do, and then output an action that will then be parsed and put into other software in order to take actions in the real world. And so, in this way, the language model can power an agent in a situated environment such as a web browser or another application and take those actions. So this is an example of the prompt of a React agent. It has a place for it to be updating its observations from previous steps. You have different tools it's provided and it'll output the next action. So it'll action for search and then it will pick the highest scoring offensive player in the NFL, for instance, given my input question. And then after a time, it decides that it has enough information to respond and will respond directly to the final user. This works often, and it exposes a really powerful capability of LLMs that wasn't originally thought of. However, it has some limitations. For one, it requires an LLM for each tool invocation, and so the serial execution can take a lot of time. Second, it's only doing one step at a time, and so it's possible that the LLM could be short-sighted and pick a next step that makes some sense but that doesn't actually give us all the way to the output in a very strategic or efficient manner. And then there's some other potential limitations in that it can't do parallel calls and stuff like that. So it works in some cases, but isn't super fast relative to what we have today. To address this, a design pattern for agents has been proposed by a number of people over the past year or so for a plan and execute style. And so it breaks down agents into a couple of main modules. One is the planner. So the first step is to take the user input and then whatever additional environmental cues you might have and then generate a plan of the steps it needs to take in order to attain the information required or in order to take actions to bring the environment to the desired state. Once it generates those tasks, you'll have particular like executors or other types of functionality in order to execute that task, and then optionally you'll have a step that can choose to replan. And this is the looping logic that it often characterizes in agent. This paper, this example that we have here, the plan and execute example, is based largely on the plan and solve paper here by Wang et al. And it's also based a little bit on the Baby AGI project from Yohei Nakajima. It's pretty simple. In this example, we'll run through you first populate with your OpenAI key, your Cavalier API key, which is the search engine, and we'll go and set up tracing with LangSmith, and that helps you really debug and observe all the choices that the agent makes over the course of its trajectory as it's trying to solve the tasks. So you can really drill down and see how efficient it's being over time. And you'll just create the tools and then you'll pull from the prompt and create the agent itself. So in our case, the main components again are the planner, replanner, and the tool executors. This first bit is the agent that we're going to be creating. And this is the actual execution agent. So after the planner has created the task list, it subdivides it so it can divide and conquer. And then for each task, this execution agent will then be taking that and try to use whatever tool it's provided in order to answer the question. So you create that, you got the agent executer, and you can invoke it there and do whatever. So this is an example, who's winner of the US Open. We're just going to give it, let's see, we gave it the search engine so we can see if it can come and answer it there. While that's running, you can show the the line graph state. So again, if you're familiar with line graph or the state graph within line graph, each node represents a module. So you can have one for the planner, one for the executor, one for the solver. Each node receives the state whenever it's invoked and the response of that node then is used to update the state. And this is how the graph then proceeds to its computation. As you can see, it's a state machine. In our case, we have a few different things. We have the input. We have the plan that the vendor generates. We have the past states, which are populated. This annotated means that any responses that's appended to this list here, we take the annotated syntax as a way to define how the state is updated. And there's a final response, which is populated by the solver there. We will create a structured output runnable, which again, we'll use function calling or tool calling from OpenAI to populate a pedantic object directly from the user input. So we've got this prompt saying it needs to come up with a simple plan. We parse it as steps. So there's just a list of steps. Again, this is just a way of connecting with a fine-tuned model that OpenAI has fine-tuned to generate code as structured output. And then we have this parser, which we'll then put in there. It's a pretty reliable way of parsing the output. So you can generate the plan there. And then the replan step. So if it isn't able to completely accomplish the intended goal in the first step, we will prompt a replanner which has the objective, the original plan, past steps, and like all the results from those, and then it is tasked with updating the plan or responding. So you can do that. We'll populate it with this LLM, and again, the prompt there, and it's two choices of tools. So it can either respond or it can plan. And again, since this model that OpenAI has developed is fine tuned on code generation, it's pretty good at outputting structured syntax, and we parse them into these pedantic objects, which makes it easier to maintain within your existing software stack. Finally, now that we've defined all these primitives, we can create the graph. So we'll have one node that executes the steps, one that plans, and one that replans, and then this conditional edge that decides whether it should end or not. So all of these, and these could be asynchronous or it could be synchronous as well, kind of depends on the performance characteristics you want. Yeah, so we first, we create this workflow and we define it with this plan execute state that we defined above, and then we add each node, which as I mentioned before, is the different modules. And then we define how those modules are connected. So after each node is called, we can either say that there is a specific, like an edge from that node to another one, which means it will always progress and the state will always pass the next node, or we can add this conditional edge, which means that whether this should end function is called, basically after node replan is called, should end is called, and then that will determine whether the output should end or whether it should go back to the agent here. So this is just a way of having conditional logic of passing the competition between nodes. Finally, we can actually call this. So in this case, we have this configuration that sets a recursion limit. This is basically a cap on the number of steps we let the agent take. You could set this to an arbitrarily large number, but in general, we'd recommend that you keep it within some reason since most use cases are relatively time-sensitive and you don't want to be continuously calling it all along if it gets stuck to a loop. And then here's the input to it. So we can stream the outputs and then we'll say if it is the end event we'll actually print out the full end object here. So here's like a plan and we'll watch it progress there. Since I turned on streaming, we can actually then go to this project here, so we can look up in Langsmith, and you can see as it continues to execute all these series of steps it's making. So we're printing it out in the notebook. You can also see all these nodes in the graph, and you can see all of them. So you can see there's a lot of steps that LameGraph obfuscates for you, but you can see the most important ones here, and you can see, for instance, here's the planner. You can see that it called this tool, which returns a bunch of information about the 2024 Australian Open. You can see that it is going to be choosing, or it's able to summarize that, and then it goes here, and this is a response. And so it outputs a new plan, so it's looking more about the hometown and all these types of things. So it's giving more background into it. And so if you think that this is being too verbose, you can then look into the different prompts that these are using and modify those using the playground if you want. I won't get too far into that, so we have plenty of examples there. And I want to get onto other examples, but into that. So we have plenty of examples there. And I wanna get on to other examples, but I think it's a great way of just tracking how the agent's progressing through laying graphs there. So it looks like it actually hit the recursion in this case. And so I could update the prompts to try to make sure that it's not going to verbose in its research. We can say that keep it as efficient as possible and that'll make it be a little bit more effective there. I'm going to go on to the next one now, which is Reasoning Without Observation paper by Xu et al. So if you recall in the React paper, the limitation was that it called an LLM on every step. And that is pretty slow and it's expensive since you're evolving an LLM call. And sending this redundant prefix, you have the system prompt and everything at the beginning. Then in the plan and execute paper, the plan and solve paper, we solved a bit of that by saying, let's generate a task list here, and then we can execute those things accordingly. The problem is that task list was also just a list of strings, and there wasn't any sort of variable substitution or anything there. So it just took it and had the tools called in sequence, and then had this response, and the replanner was tasked with deciding, oh, now do I need to generate more information now that I've actually received answers to my first questions there in the plan. Reasoning Without Observation goes a step further and allows variable substitution. So the initial plan can actually include a search for the first step and then include information from that search in the second step without actually having to consult the planner again. This, of course, saves you a lot of time because you don't have to involve an LLM in every step and lets you actually go all the way to completion without having to replan in many cases, so long as the tool responses result in a reasonable output. This is a diagram of the overall graph that we're going to create. So you can see the main components. Again, our planner, you have this worker, which again, executes the tasks in sequence, and a solver, which takes those results and then responds to the user directly. You can see, we'll see like a trace later about this. For all these examples, we'll give it the similar tools. We'll have this Tavoli search engine tool. You could replace with any search engine. It's more of a demonstration of how the agent works that we're going for. And we'll also set up length chain tracing just so we can get a nice in-depth observation and trace of how the agent's working so that we can debug different things if they do arise. In this case, our state for the graph includes the task, the planning string. So these are for the task execution node, the steps that were proposed, any results from the task execution. So again, since we have variable substitution, this will be populated with those results so that you can then use it to replace those variables in later values, and the final result. Maybe I'll dive a little bit more into the format of the prompt here. You can see here the format for the planner, it outputs a plan which is similar to the reasoning step to the react agent. And this says like search for information about Hinchada. So it's an ability to do some chain of thought reasoning and improve the likelihood that its plan is going to be performed better. It then assigns variables to each of these tool calls. So say like the pound, E1 for search Wikipedia for the Hinchata. It also is always given this LLM call. And I find this to be an interesting design decision. It basically is used to get around the fact that the outputs of a lot of these tools need to be additionally formatted for subsequent calls, or you might want to be including some additional reasoning steps in there without having to go back to the planner. While this is still an additional LLM call, since it's more scoped, presumably you can use a faster or cheaper LLM than what you would use for the planner. So you might use GPT-4 or whatever next version of that and the planner and then with each LLM call, you might use 3.5 or an open source model or something like that. So you can still get some performance gains there. I'll skip over the worker and solver for now since we'll talk about that later, but suffice to say the planner here takes this prompt and then outputs a plan using this format, pretty straightforward. The executor then takes the tasks and will execute the tool directly. So you can see basically, we'll get the current tasks from the state. And so this basically looks at the results and then returns the information there. It then looks at the steps, which again was that reg ex output from above and goes through the results that we have so far. And if we have an instance of that variable, we'll replace it with the actual output. So if I did a search originally and then you have pound E2, hashtag E2, you'll then replace that within the string so that that later tool call can use that information from previous steps in its output. And then you can see we're doing this lookup for our search engine and LLM, and you can add other tools here if you want. I believe in the paper they tested up to 20 tools and said that it worked okay, but as we know from this and other studies, if you have too many tools, the quality, it often serves to distract the LLM, it uses more tokens and can often have negative consequences. So yeah, you usually want to pick a proper number of tools and reduce the number of extraneous ones if they're not gonna be used very often, or if they're not high value for most of your use cases. And we have other videos on how to balance that and how to use LandGraph actually to be selecting tools. With most other videos, and we have examples in the repo. All right, next comes the solver. So this is similar to the previous one. It executes the, it decides the final response based on the tool responses. So, we can run that. And finally, the graph itself. Again, you have each node, this plan, tool, and solve. And then we say every time that we plan, we then go to the tool node. This tool node again, just goes through all of the plan in sequence and executes them. We then have the solve node, which is always traversed from tool to solve. And then finally, we have this conditional edge, which says that if, again, so if there's the next step, or if there's no more steps left to execute, we'll go to the solve one, so we'll finish the loop. Otherwise, we'll continue to stay at this tool loop. So again, in picture form, it first goes to the planner node. This generates a task list that goes to the worker. And then the conditional edge defines whether it's going to continue to loop or whether it goes to here. And then it responds to the user there. All right, so we'll finally prompt our Reboot agent with the task that we provided, which is to look up the hometown of the 2024 Australian Open Winter. And while this is running, I'll pull up LangSmith just to show what it will look like as you're debugging this. So you can see it's streaming right now. And then you can look at the first LLM call, which is the planner call. So it has the prompt here, and you see the tools that we've provided it, and you see the task, where is the hometown of the 2024 Australian Open Winner. It generates this plan, which seems to make a little bit of sense. So at first it's going to search for the winner, and then we'll use the LLM as an extraction function essentially to take the winner out from the results. Again, the results are a little JSON list. It's pretty long. So this is actually an improvement that we'll give it just a couple words for the next function. Next one is the search for the hometown and then give it this information. And then it says, retrieve the hometown from the results. So again, extract from this search function. It seems to have that pattern down. We'll do a brief refresh just to look at what other functions is done. Looks like it did call the search engine there and then called it again with hometown given the content equals this. So we'll show the output. The hometown is Italy.. Doesn't necessarily make the most sense. It looks like we're using 3.5 turbo. So we could probably get improvement with a better model or with better prompting here. Very focused on the hometown there. But you can see that it at least found some relevant information based on this. And using Lengspeth, we were able to see some things that we could improve. So without wanting to take too much more time on this, a recap of this review paper is that it improves on the plan and execute approach, the naive plan and execute approach by allowing variable substitution. It was able to go and use that sequence of steps with search and then extract without having to call the planner LLM again with this redundant step of agent scratch pad and all that kind of stuff. So in theory it's able to save a lot of time and tokens in doing so. The problem this still has is that it still relies on a sequential series of tasks. We don't explicitly track the dependencies of each tool, you can add that. And then all the tools are invoked only after the LLM itself completes its response. So you wait until the full generation is done, and then you execute the tools in sequence, and then you call the solver. So all of this adds time in your execution. If a user's waiting for the response, it's not the best user experience. The LLM compiler paper by Kim et al. Sets out to address some of these limitations that we saw in the reWu paper. It's designed to speed up the execution of the agent by two main approaches. One, it streams tasks in the form of a DAG. So each task it has has the variable substitution there, along with an explicit list of dependencies. So in those later search steps, it marks as it's dependent on the first one, for instance, and then we'll go there. So you can be having some of these tools executing in parallel rather than all in sequence. That's going to save us a decent amount of time if each of these search operations is gonna add up. Second, we're actually streaming the task output itself. So as each token comes out, we check to see if we can then parse a new task from that streaming output. And as soon as that is available, we pass that on to the task fetching unit. This again saves us a lot of time because while the LLM, especially for a Blonk plan, while the LLM is outputting more tokens, we can already start executing these earlier tasks and it's saving the time overall. So this can get us above some of the performance of, for instance, some of the naive parallel tool execution of OpenAI and the previous plan and execute examples we reviewed today. This task fetching unit will go into further detail later, but it basically uses multi-threading and will schedule a task as soon as its dependencies are met. Once all of these tasks are finished, the rest of this is basically the same as the other examples we saw. We update the state with the task results, and then we have this joiner, which essentially is a dynamic replanner that takes the results, the input, and all of that information, and decides will it go respond to the user or is it going to generate a replan operation. And then that will then go back to this task fetching unit there. So again, we're going to set up Langsmith and install our dependencies here. I've already done so. I wanted to add my API key for Langsmith so it can go to this separate project, the LLM compiler project, so we can really group our traces like that. And then without further ado, we can define all of the different components of our agent. We'll start by defining the tools. In this case, we're gonna have both the search engine, which we're doing this Tavoli engine again, and a math tool, which is an ASD parser and a calculator there. We use an LLM within the math tool since the outputs of search results may be unstructured and we want to make sure that we can structure it into specific floats. This is akin to what we saw in Rewu, where having a separate LLM tool that it can call that would extract those results for us. So we'll define those, and you can see, for instance, this calculator here, it's going to be outputting the results based on some of these things, even if there is misspellings and other things in the input. We have this optional context argument that the LLM or the planner can provide that allows it to pass on additional context from the user input that won't be available just by being passed in through the args or the outputs of other tools. The second one is the planner. This is mostly interesting relative to the other agents that we saw today in that the format permits streaming and it really focuses on that. So it'll output the things that look like the Python function invocation where it has the name of the tool and then parentheses and the arguments there. I added in the keyword-based arguments here so it's easier to parse into a dictionary and use that within our state. I created this additional planner which has this branch that decides whether it should replan or whether it should not. And if it's replanning, it'll be doing some updates to the state just to make sure it's formatted correctly for the agent. So again, this single function handles both the planner and the replaner from above. Find that. All right, so next up is the task fetching unit. And this I think is the most interesting part of this paper relative to the other papers we were reviewing today. It takes in a stream of dictionaries where each dictionary contains the tool that we're gonna be invoking, as well as a list of dependencies. Each dependency is represented by a number, which is that index in the stream of tasks that you have emitted from the LLM. Importantly, the index is continued to be incremented across replans. So you can continue to reference some of the original tasks that were executed even after you've replanned so you don't have to do redundant work in theory. I'll jump into the main function here. So this scheduled tasks component, again, you take this stream of tasks, so this is a generator of dictionaries, as well as the state of messages. So if we're thinking about the state graph, the line graph state graph, it's tracking all the information about an execution within this list of messages for the chatbot. We get the observations from the messages because we are then formatting all of the tool responses as function messages. So if this is a replanning step, then you can get all of the observations for those previous ones, format it in a nice dictionary so we can be looking up and substituting that here. After that, we're going to be starting this execution step. We want to be able to schedule this in a separate thread so we can continue to stream these inputs from the LLM in without impacting the overall runtime of the tools. We check if there's dependencies of the task and that all these dependencies are not yet met and if that is the case then we'll schedule it as a pending task and we'll go over what this means later. Otherwise we can schedule it immediately. So we'll just schedule this directly. We'll call the tool. And then once that is actually ready, once that's ready, the results will be populated into the observations again, and the scheduled task can then check and then eventually be executed once they're ready. The switchable pending task here is basically a loop that checks for the dependencies of the task and will just sleep so it'll pull to see whether it can actually be executed yet. The regular scheduled task, again, invokes the tool. And once it is actually invoked, then we will return that as an observation. If there's an exception in calling it, then we're going gonna return the exception here within the list of observations. When you're executing the task itself, we try to resolve the arguments. So in doing that, we look for this syntax with the dollar sign for variable substitution. There's some cases where it will use more of a code-based dot output or something in it, so we're handling an additional case there. And then we will look at the index that it represents from that list of observations, we'll look up the observation there or the dictionary observations, we'll get that from the index and then we'll return that there. So then all the arguments for the tool can be multiple arguments to the tool will be resolved. And so yeah, this is the way that we use variable substitution to let it actually generate the full DAG without having to do just incremental steps and recall planning there. So I'll run this. And the final, we'll see if it runs first. All right, so it ran. And you can actually check Langsmith to see what the results look like. We'll wait for that until the final step just to get on with it. So the joiner is the replanner here. This is a little less interesting. It's kind of the same as the previous steps. It takes in the previous responses, and then we'll decide whether to replan or whether to output a final response. We're using function calling here just because it's a reliable way to generate structured output from unstructured input using LLMs. The joiner output, we will then format either into a system message if we wanna be replanning, or we'll respond with a final AI message here. So format that to both. So you can see what it outputs like here. And the final step is actually to orchestrate this with line graph. Cool. So you see the results and it decides that it's actually going to finish and set the temperature of San Francisco as mentioned, etc. The graph is created with just these two nodes, really. It's a pretty simple one. Since planet schedule is all contained within a single vulnerable graph, vulnerable DAG, do you have the planet schedule node and the join node? And the, anytime the planet schedule is called, it will automatically go to the joiner, since that decides whether the loop should end or whether it should continue. And then finally, we will add this conditional edge from join and decide whether we should end or whether we should go back to the plan and schedule node. So again, the conditional edge has a function and then the function outputs the strings. This is just a variable for the double underscore end string. And these then will correspond to different nodes in the graph or the finishing condition. So we'll compile it and then ask it a simple question. So like, what's the GDP of New York? And now that it's running, I'll ask it some multi hop and multi step math. When it's running, you can see in Langsmith, you can jump over and look at the results here. So you can, I ran this just a second ago. You can see what's the GDP of New York, and you can see the series of operations here. So we have the planner, it outputs. So this is again, the first result there. So it was totally search, and then it has this value, and then calls again, looking for a replanting attempt to look at GDP of New York in 2023 because in the original one, it didn't get the most specific results that it wanted. And then it finally goes and it says, it says it's provided in the search results, the New York has this. And so it says the GDP of New York in 2022 was 2.053 trillion. Again, we could provide additional information if we wanted to be more specific about which year or something like that. The next question we had for it was a multi-hop question. Let's go back here. So what's the oldest parrot alive? You can see the output of the plan. It has 0 and 2. It has the search for the oldest parrots alive. And then it has average life status and a parrot. And then it has the end. There's no variable substitution here. It's able to just do parallel function calling, basically. But you can see both of these are called pretty quickly. And then after that, it calls the planner and says that it found the age of the oldest parrot, who's 83 years old, quite old. But it doesn't have the average lifespan of parrots. It was only able to get some other specific information. So it tries again. So you can see the replanner. It then continues from where it left off in the task list and says average lifespan of a parrot. You look it up, and then the replanner chooses that it's found the oldest one there. It says that they don't have the specific lifespan there, but it's implied that large pirates may live longer than smaller species. So basically it's saying the search results weren't good enough. Both from the output here and from debugging with the trace, you can see the search engine actually should probably be tuned here for this type of question if we wanted to be able to respond well to this type of question. And that's really the way we'd want to be improving things, because we want to improve the overall quality of this agent. Anyways, we'll jump back to the notebook. You can see that it's completed here. You can see it also did a math problem, which we can jump to here as well. It did, again, this math chain a few times in sequence there. So again, that's about it for the LLM compiler notebook. I think the key takeaway from this one is that whenever you're doing planning and you have variable substitution, if you do track the dependencies there, then you can actually start executing the tools before the agent ends. And so this can overall optimize your runtime, save on tokens relative to the React agent style, and improve the overall performance of your model. I think all of these agents in summary are a step towards a more robust and more effective agent design if you wanna be using LLMs to make decisions about how to execute tasks. And I think implementing them in LaneGraph is pretty straightforward. I would actually recommend doing so yourself and maybe combining some of these different approaches like taking maybe the Rewoo or Planet Execute style output from the planner and streaming it and using something like an LLM compiler executor to be able to do it more equally and stream mixing it with maybe other design patterns that we've showed throughout the LENGRAPH flow as well. So that's about it. Again, thanks again for watching this video. Feel free to leave feedback on the video and like or subscribe to share and follow other types of content like this so that you can learn more about how to create different agents both for production and for experimental use. And finally, if you want to sign up for LangSmith, feel free to go do so at smith.langchain.com. We no longer have a wait list, so you can go ahead and get started debugging your LLM workflows immediately. And thank you all and hope to see you again.


video_id: v5ymBTXNqtk
Hi all, this is Will from Langchain. Today I'm going to be talking about reflection. If you've ever been building an agent and found that it gets caught repeating itself or isn't making strategic decisions, then this video is for you. Reflection is a common technique used to improve the quality and success rate of agents and similar AI systems. It involves prompting an LLM to critique and improve its past actions, sometimes based on additional external information such as tool observations or other observations from the environment. While it typically adds a bit of additional execution time, it can boost overall performance, so if your application permits, I would recommend you use it. If your application does not permit, you can use this technique to generate additional fine tuning data and shift the computation left to improve your performance without impacting the inference. All right. So for our first example, we'll do a super simple reflection graph here. We basically will prompt an LLM to generate an output, then we'll take that output and then prompt another LLM to role play as a teacher and provide reflections and criticisms so that the model can improve the output. It repeats it a fixed number of times and then returns the final result to the user. Hopefully, based on this series of improvements, it will actually generate a better artifact at the end than the original shot there. So we first will set up our connection to the LLM with these API keys and we'll set up tracing so that we can reproduce a number of these real results later if we want to be improving the prompts. We can see exact sequence of steps that were made in Langsmith and then without further ado we'll define our generators. The generator here we just take a simple prompt template and then we have this LLM in this case case, it's an open source Mixed Roll model developed by Mistral. We'll create that and then we'll ask it to generate an essay about the little prints. So we can see it generates a number of paragraphs here. It follows the style. It seems pretty good as a first site. I'm not gonna read into it too closely because we don't have time. So then we take that output here and we can see that we have the original request and we have the critique which we're going to format as a human message because again we're asking this reflection step to be role playing as a teacher and so it generates a new one and so then it says we have the or it takes it out and then it says we're gonna have an improved essay outline here and then you can repeat. So this basic step is pretty simple. The next bit shows how to wire this up in line graph. So we're gonna use our basic message graph which is line graph primitive that stores state across nodes as a list of chat messages. We'll wrap each of our functions above in these nodes that we'll put them in as the messages key so whenever it's called in the prompt template is formatted correctly. And then here we include a little bit of a translation from the AI messages to human messages and back so that the role-playing model doesn't get too confused about who's saying what. The overall graph is simple. We have two nodes, generate and reflect, and we continue to loop around based on this conditional edge here. If once it's reached a certain point it's going to end. So we create that and then we'll see how it does. So you'll see over time, it'll start to generate, reflect, this is the first pass. You can see it has an output there. This one says you've demonstrated a strong understanding, but maybe include some more quotes and citations. We can see over here, it continues to chug along and includes some improvements to different paragraphs. We're going to print it out at the end. You can see these are starting to get quite long and then we'll generate this one out here. So the last result here we can include some references. Of course, this looks like it was probably hallucinated, but you can see it responding to the overall criticism of the reflection step by incorporating that later. So it's trying to be doing citations and everything and appear more informative and et cetera. Before we move on to the next one, I'll note that this is extremely simple. It is a single cycle, but since we're not grounding the reflection step in anything, it isn't guaranteed to actually improve performance. This often still does improve because you're teaching the LLM to role play and you're encouraging it to incrementally improve. Sometimes it commits to a specific path in the token generation trajectory that isn't ideal. And so then you give it a chance to use some more compute improve. But again, overall, doing this without grounding in anything can see some limited but not extremely wonderful gain. The next design we'll be going over is Reflexion by Shin et al. It's an architecture designed to learn through verbal feedback and self-reflection. It extends the reflection example we have below, before by incorporating both tool observations as well as more explicit prompting around how to ground or how to project the reflections. So as you can see in this graph, the user request is first fed to a responder which generates the initial response. So it tries to answer, it tries to generate its own self-critique, and then generate some search terms that it could use to extend its answer and improve it there. All of the tools are executed and it's fed into the revision step which then re-answers, so it does an improvement based on the critique and search and other observations there, as well as adding some citations that seeks to address the original critique based on the retrieved outputs. It repeats this a number of times, so continuing to reflect and then search for things in order to improve based on that criticism until it reaches desired state and then returns the final response to the user. As in last time, we have some initial prerequisites to set up. I already installed these beforehand, but it depends on Langchain, and then we're using a search engine called Tavoli in order to treat this as a research agent. We're connecting to OpenAI this time, so we'll need that API key. And then we'll also set up Langsmith tracing, just so we can reproduce and see the exact trajectory of this agent as it gets a little bit more complex than the last one. We'll start by defining the tools. Again, these tools ground the agent a lot. So we have a little bit of boilerplate here just to execute them in parallel and do some niceties there. And then we'll get into the meat of it. So here we have the original actor prompt template. So this is again the responder, and we just call it an expert researcher and we give it the instructions and then say they reflect and criticize and recommend search queries in order to improve your answer based on what it already knows. And again, this lets it use its parametric knowledge, so what the model has already been trained on to give an initial guess, but then we'll use external knowledge and self-reflection in order to improve that through a series of loops. We're using function calling just so that it's easy to parse out. We'll see that we explicitly prompt the criticism to generate both missing and superfluous aspects of its response, Again, so it has that balance and response type. And then we also generate these search queries that will be passed on to the search engine afterwards. These can be executed in parallel so it's not too long. And the answer we're telling it should be around a 250-word detailed answer. Again, you can control all of this stuff from the prompts itself. We'll be doing some retries in case it doesn't validate well. And here is an example. So we'll see the answer. We're asking about reflection, so it talks about, oh, it refers to artificial systems and really analyze and improve. This seems pretty good. It says that it's quite focused, but there is not superfluous information, so it's actually pretty good there. It could expand on some specific techniques or algorithms. So then it's looking for techniques for this, real world examples, impact and reflection. Maybe in the future, we'll be able to find this video itself. So once that comes out, we'll set up the revised one. This prompt is very, pretty much the similar. The only difference being we're going to inject these revised instructions back here in the template here is the first step. And these specifically will instruct it on how to use numeric citations for that output here. So we're going to be adding references. And this is again, a technique both as useful so that it can be citing the search results that it's being used and then it also can help steer the LLM to generate more grounded output. We'll go through that again and then get to the point where we're all waiting for, we can get constructing the graph. We'll be using the message graph again which treats the state as just a list of chat messages. We'll have three nodes, the original draft, we have the tool executor node and the revised node. Every time that it hits the draft, it'll go to execute tools. And anytime the execute tools node completes, it goes to revise. The looping logic is defined here. So we have the conditional edges here. And then once it reaches the maximum number of iterations, it'll end. So we're doing an explicit loop. So let's see a very easy question. How should we handle the climate crisis? And so you can see I ran this before. So it looks like the execution is completed. And we can print out all of the examples here. You can see that it both has a number of points here and then a lot of references that try to cite each of these examples and it's done this over a series of steps. You can look at the whole trajectory by going to Langsmith. So you can go in this reflection one and you can see the series of steps. So you can see that it generates, it decides the original answer, executes the tools, um, revises, and then does that a number of times. So you can actually see the final output. And then for each of these, you could try and go in and rerun some of these outputs just to see how it performs if you want to be making any changes to the prompt template. So our final example is a little bit more complicated. how it perform if you want to be making any changes to the prompt template. So our final example is a little bit more complicated. It's called language agent tree search by Zhou et al. It combines reflection and other reward modeling in an overall tree search process to give you the best possible trajectory at the outcome. The technique looks like the graph below. Basically, you first will generate a candidate and then you'll reflect based on the outputs of the candidate and the tool actions. The reflection assigns a score and then in this step you can also perform additional external checks. So if you're generating code you might be running unit tests and using that to update the score. Based on that score you then pick a node, in this case it's just the first one, to generate a number of new candidates and then you repeat the process. You reflect on them, possibly run some additional checks outside, use that to create a score, and that score is back propagated up to the parent node so that it reflects the overall how promising this particular branch is in the search, and then you repeat again. So you pick this one, in this case it's this branch, and then generate candidates from that, and then you go and reflect and update and continue to go until you've solved the problem or until you've reached a maximum tree depth. So this is interesting because it uses both a metric from reflection and from external observations. It can help balance the search process and hopefully improve over some naive depth-first search or other search algorithms. And it's overall a pretty fun one to implement. We'll get more in detail about each component below. I apologize, it's a lot of code, but hopefully you get the sense of how this operates based on the diagram here. So again, we have some prerequisites. We'll set this up here, and then we'll define the graph state. So I think this is the most interesting part, to be honest. It's a node that stores the messages in that node that are meant to be appendage to the trajectory, the parent and its children, and then it has some values. Again, basically, every time you unroll the tree, you're searching for some candidates, and then you score the candidates, and then you use this to be updating the overall branch. You might have some questions, like how do you pick the best candidate? So you could do the naive way of just picking whichever one has the highest value. This Monte Carlo tree search wants to be balancing that with the ability to explore other branches that it hasn't explored as much yet. And so it does this by using an upper confidence bound or specific upper confidence bound applied to trees, so UCT. And it's defined right here. It basically has this, like an average reward term and an exploration term that it combines here with a specific weighting term to balance exploration exploitation. Again, this is all a hyperparameter of the system that lets you balance the ability to pick the best path with the inevitability that sometimes it'll get caught in a local optimus. You want it to be able to search other ones and get itself out. I'll skip over some of the other specifics there but if you check out the code you can try to really implement it yourself. The tree state itself, well I need to run this first, the tree state itself then just has this tree represented by the root and then the original input. Now that we have the tree state, we can define the language agent, which is a little bit less complicated, honestly. We're just going to use TrapDBT turbo, and we'll be using again a search engine here. Most of the things that I've implemented uses function calling as well. So reflection, you generate a string based on the reflections on whether the output is sufficient or superfluous and those types of things. We'll assign a score, in this case on a scale from 0 to 10, on the quality of the output. And then we'll use this to determine whether it's found a solution. So we can do this. And then the next step would be generating the initial response. So whenever, this is a very simple prompt. We're just saying yeah, it's this thing. We give it a couple tools and then it can pick a path there. So example here, if we're asking it to write a research report on lithium pollution, it'll decide to query the search engine with the impulsion Research Report. Not the most creative, I'll admit, but it gets the job done. We wrap this in a node itself. So all this does is it takes the generator, it executes the tools if they are there, calls the reflection agent, and then uses that to create a new node in the tree. And this route is then added here since the first step, then added to the graph state. The next step is candidate generation. It's the same as the previous step but unrolled a little bit more. So here we're going to be using the LM. generate button or method on the length chain LLMs just to generate end candidates. This is all configurable and each of these candidates will be treated independently as a new node in the tree. We'll do some deduplication later. So here's an example. We're asking it the same question. It generates five outputs. They're all pretty much the same, but with different query terms. Once you do that, we will put this in this big, scary-looking node here, which, again, does the same as above, but since we're doing a bunch of candidate generations and I wanted to do it all in parallel, there's a little execute, or a little additional code here. So we get the best candidate here with this get trajectory, or we get the best child method here from the root. We generate, then we get the messages from it. We expand to generate five new candidates. We'll then parse these in as like tool invocations. We run all of the search queries in parallel. And then we'll be prompting the reflection change and then score all those outputs. And then we'll take all those outputs. So the original selection of tools or final response and the reflection messages and putting them in new nodes and adding them all back into the tree as children of the best candidate. So we can run this. Finally, the moment you've been waiting for, the actual graph is quite simple here. We have two nodes, start and expand. Start in the start node, and then at each of these points, we'll check whether we should loop. So we can finish if the root is solved, if the reflection step says, hey, you've answered the question directly. And we also finish if the root's height is greater than 5. So we want to make sure that the tree doesn't get too big. And then you can try calling it. Well, I need to run this first. You can see I ran a couple of these before, and it was pretty good at these outputs. We'll see how well it does this time. So I'm printing out the steps here. You can also use some other extreme events methods to get more incremental information. I think this is enough for me. While it's proceeding, we can actually go check out LangSmith. And let's see, I'm going to a new project here. And you can see it's streaming here. So you can see there's a lot of calls right now, since we're doing a lot of parallel execution. We generate the initial candidate. And it says we're going to talk about, so we're asking it to generate parallel execution, we generate the initial candidate and it says we're going to talk about, so we're asking it to generate the table of average size and weight, and so it needs these parallel processes. Again, all this is all considered a single node and then we reflect the reflection method outputs a score of nine, so pretty good. It says, again, if this were a code generation task or another task where you have external feedback on it, you can get a little bit more of a nuance score here than just the self-reflection. Next, we go into this expand step. So you generate five more candidates, and then you generate all these tool calls from all of the different candidates. We do some deduplication in between, but there's probably a lot of repetitiveness here. And then the agent's still going. So as you can see, it goes out, and then there's some reflection. And then it looks like it found the solution here in a number of the branches. And it's good. So I'll go back to this output. So it seems that it is asking for additional information. We can probably go back up a step and generate a summary of the outputs there based on it. And then here, we're asking it to generate a series of moves based on the Magnus Carlsen's chess game against Alariza Therosa. Sorry for butchering the name. And it's saying, ta-da-da. So it's looking at some of the outputs here based on the research there. So these outputs weren't the best here. I think that it would probably be a better example if we included some code execution if for the reflection steps of the scores are more useful. But overall, I think it's a promising technique. Again, the overall approach is that you have some tree roll out, you score each step, you then pick the best node based on those scores and then you continue to update and refine. This isn't super feasible if you want an immediate snappy application experience for your chatbot. It is good if you want to have more complicated reasoning type tasks and if it's embedded in an overall system. And this is also a great way to generate better fine tuning data since if you just automatically generating from a single pass, the quality won't be as good. Typically when you want to be having fine-tuning data, quality is way more important than quantity there. So I guess with that, I'll sign off here. Thanks again for listening. This is another fun video. Hope that you enjoyed. We talked about reflection in this video, and specifically reflection as a means of improving agent performance. We went over a couple of videos, examples, one was a basic reflection example, one was reflexion, which adds additional tool observations and specific prompting into the loop there to improve the overall output. And then the final is language agent tree search, which incorporates reflection as well as other external feedback components in a tree search, which can hopefully get the best trajectory overall by doing a bunch of parallel operations. So anyways, thanks for watching. If you want to sign up for LangSmith, you can do so for free at smith.langchain.com and check out some of our other videos here on YouTube. So, thanks again!


video_id: 1uUORSZwTz4
All right, this is Lance for Langchain. I've been talking about a recent paper called Storm, which brings together three pretty interesting themes. The first theme, just to set the stage here, is on AI-mediated assistance. There's been quite a few of these in recent months. We've seen a lot of work on code assistance, GPT-Engineer being one particularly popular one. There's also a lot of work on code assistance, GPT engine being one particularly popular one. There's also a lot of work on web research assistance, like GPT researcher and others. So being one is this idea of assistance for certain tasks that are a little bit more targeted to code generation or web research. The second is using RAG to assist with that generation process. So there's kind of this nice review tweet and paper on retrieval augmented generation for content. It's pretty interesting. It goes through lots of the themes you might use RAG, like the code gen, image gen, audio, and so forth. But it's an interesting general theme that RAG is kind of the centerpiece for some of the generative workflows that could involve code, research, or other things. And the third is this theme of personas. So to solve certain problems or to perform certain simulations, AI can take on different personas, and they can have debates, and they can kind of bounce ideas. One really interesting work on all these lines is called the Generative Agents Paper out of ideas. One really interesting work on the longest lines is called the Generative Agents paper out of Stanford. And this was one of the first works that used generative agents in a simulation to simulate human behavior. It was a fun game simulation that you could play. So those three ideas come together in this newer work called STORM, which is also out of Stanford. Here's the paper. So it stands for, you know, the paper title is assisting in writing Wikipedia-like articles from scratch using large language models. And STORM is basically short for Synthesis of Topic Outlines through Retrieval and Multiperspective Question Answers. That's kind of a mouthful. But the main tuition is that you're bringing basically short for synthesis of topic outlines through retrieval and multi-perspective question answers. That's kind of a mouthful. But the main tuition is that you're bringing together these three ideas to write Wikipedia articles. That's the highlight of the point. So here's kind of a cartoon, kind of the way, at least, I think about really what's going on here. And we'll actually walk through the code and we'll kind of build this from scratch. So it looks like the, this is slightly... Here we go. You kind of see it now. So what's happening is this. You're starting with the topic and there's like kind of two branches here. Let's just go through the bottom branch. The bottom branch is an LLM's taking that topic and fanning it out to a few related topics. And it's actually retrieving 4-H related topic from Wikipedia, Wikipedia articles that serve as like kind of templates or example articles that can be written on similar topics. Now, what happens here is those articles and questions are used to create some Wiki editor personas, which are basically like kind of AI personas that are going to be the editors asking questions about these topics. And we're also going to have experts. So experts are experts on this topic and they're going to be using retrieval, web retrieval using different tools. We'll use Tabularly for this one, but these other things as well to answer questions. So what's happening here is we're going to take our initial topic, we're going to break it out into related topics, we're going to fish out Wikipedia articles for those topics and use those to instantiate some editors. Okay, so that's like phase one, we're going to build some editors. We're then also going to independently build kind of an expert module that answers questions. And here's what we're going to do. We're going to have the editors and the expert kind of debate, question, answer, question, answer, or we can actually use LandGraph to implement that. This is going to take a little kind of like debate loop, which you'll see is kind of cool, that's going to accumulate a discussion about the usual related topics. And then pretty simply, we have that discussion in hand, you can kind of see over here, what we're going to do is use it for two things. One, we're going to use is use it for two things. One, we're going to use it to help us build a refined outline of our final article. See, this is a parallel branch up here. We take the topic, we build a general outline, and then we refine that outline based on the discussion. Then what we're also going to do is we're going to take the discussion and put it in a vector store so it's easily retrievable. And we're going to have a section writer that's responsible for taking every section in that refined outline and fleshing it out in detail. And then the final thing we're going to have is just a Wiki formatter that's going to build our final article. So that's really the overall flow. We're going to take a topic, we're going to get some related topics. We're going to build a Wiki editor and an extra persona we're going to get some related topics. We're going to build a Wiki editor and an expert persona that are going to debate those related topics. We're going to then capture that discussion in a vector store such that it can be retrieved. And we're going to use that discussion also to refine an outline of a Wiki article. And then we're going to do more fine gray fleshing out of that article using the vector store, which contains the granular store, which contains the granular discussions to write every section and then will format as a wiki. So that's really it. That's the flow and we'll kind of reference back to it because there's a little bit of code involved here. So we'll kind of go back and forth from that outline. But first I want to kind of like kick things off here in terms of coding. So I have an empty notebook here, a few PIP installs. What I'm going to do first is simply define what's a topic I want to study. Now I've been doing a lot of work recently along context LLMs. You see our past video, I actually talked a lot about this. So I'm going to make this an example topic. What is the impact of million plus token LLMs on RAG? Okay, so that's like my high level topic. Now here's where we're gonna copy over just some code to do this first stage of topic expansion. So that's this step here, right? We're gonna run this. So all this is doing is we're defining our two LLMs. That's fine. We're gonna have a prompt here that you're writing a Wikipedia article, recommend some pages. And okay, so basically the related subjects we find are impact of Mennonconus LNs and RAG. So not the most helpful set of related topics here, but it'll work. So then what we're going to do is we can combine a few different steps here together. So we'll just go ahead and kick this off and I'll kind of walk through the code here. So again, where we are in the flow is this related topic generation, retrieval of wiki pages and creation of our editors. And we can kind of see here, here's the editors that we created. Now, how did that work? If we just look at the code, we can see this was the chain that we defined above to expand the topics into these related subjects. So did that first. And then it retrieved wiki articles relevant to that, to each of our subjects. That's fine. So we'll treat them. It does a lot of formatting. And then it generates these perspectives, or different personas, so to speak, for our editors. And we can actually go back and look at that chain. Here is our Gen Perspectives chain. And here's our prompt. You need to select a diverse and distinct set of Wikipedia editors, work together. And that's basically what you do. I will make the interesting note here that we're using structured outputs quite a bit in this code. And of course, you should look at the notebook that's going to be linked to this video to walk through it all in detail. But this is very useful and important here and more generally. So in particular, what we're doing here is with structured outputs being bound to RLM, and it's using this perspectives thing. So what is that? Well, that's a pedantic object that we define right here that basically defines what an editor is. And so basically, this class editor defines a bunch of attributes that every editor has, which we then capture in this perspectives of an entity object. And the key point is this. Every one of our editors has a very nicely formatted kind of description, so to speak. So here's our list of editors. Here's each one, the affiliation, the name, Dr. Researcher, the rules researcher. The researcher will focus on analyzing the impact of context, the long context rag. And anyway, you kind of build this set of editors that are going to be responsible for different questions basically in this discussion we're going to create shortly. So that's kind of cool. Now what we're going to do is we're going to give these loop. So that's kind of cool. Now what we're going to do is we're going to give these editors the ability to ask questions. So that's kind of the next thing we want to do. Let's copy this over, and we'll kind of kick this off. And so that already ran. It's very quick. So all that's happening here is we've defined this generate question chaining. And really, all that's happening here is we basically are defining the editor there we want to work with. And then we have this gen question prompt, which is right here. So you're an experienced Wikipedia writer. You want to ask a question. When you have no more questions, say thanks so much for your help to end the conversation. So that's like kind of a we'll see this as you later to kind of terminate the discussion. And again, we have a bunch of we define a number of objects here that will help mediate the discussion, as we're going to see shortly. And again, there's like kind of a lot of code here. So I encourage you to actually look up in notebook. This just kind of gives you in broad strokes the main pieces that are involved. So basically what happened here is we invoked this generate question chain with our editor. And this messages you'll see is basically a question. So you said you're writing an article on the impact of million plus tokens on RAG. And what we say, basically, that's how we initiate this question answer chain with an introduction. And this was the answer. This is the output. So you give it the editor persona, an input like prompt, and then it performs this question generation. You can see the questions generated is, yes, that's correct, responding to, so you were writing an article about this main topic, right? And yes, that's correct. I'm focusing on analyzing the impact of million plus token, conness windows on RAD. This includes looking into how these library models improve generation, blah, blah, blah. Is there any specific you would like to know about this topic? So that's how you set up question generation that the editor will use in this discussion with the expert. So now what we can do is here we're setting up the expert. OK, so now what we can do is here we're setting up the expert. So actually, I'm going to delineate this. So we'll call this the expert. OK, and then this is the editor. So here's the editor. Cool. Great, now here's the expert. Now the first thing that the expert's going to do, so the expert is going to be more of a search tool. So the expert's going to do is it's going to take the input question that the editor provided and it's going to break that out into a few sub questions, try to answer some to some questions. So this is kind of the first part where it took that input question about the impact of million plus token on rag, and it fans it down to a few kind of related sub questions. This is kind of just helps in like defining a better answer. So again, we're inside the expert. The first part of the expert is like taking what the editor asks and trying to fan it out and just set up some questions. So this is then where we're going to define the full expert answer chain. So this is basically a decent amount of code. Again, I encourage you to actually look at the notebook to kind of crock all the details that are going on here. But this is the high level idea. Basically, we have this gen answer function. And you can see it already ran. It's pretty nice. Where we're taking in the current state of the interview. We'll see why that's relevant in a bit. And basically, the name is Sub-Gift Matter Expert, as the default name. And what's happening is you're basically setting the role to be the expert. You're generating those sub-questions that we just showed above. And now what you're doing is you're calling the search engine. Using a search tool, just a search for things relating to the topic that you were asked or actually to each sub question that you generated. You perform a search and don't worry about too much about these details. You're basically getting the site and URLs and you're also calling this generate answers chain which is going to be, let's see where that is. Yep, right here. So you can answer prompt, you're an expert, you can use information effectively, you're chatting with your Wikipedia writers who want to write it with your Wikipedia page. And that's really it. And you're basically gonna pass in information from search to provide the final answer. And here we go. You can see this final generation here. So there's a generation, here's a set of URLs, and we kind of format the final message. You get our answer and we can look at it here. So million plus token windows have significant impact on RAD, largely goes like send context, will allow for accomplished prompt execution. So, you know, and basically the point is this, you take the input question from the editor, you fatted out with some sub questions, for each sub question, you kick off like a web search, you get search result and the URL back, you consolidate that into a spinal answer with all your citations. That's really all going on here. So if we zoom all the way back up to the overall flow here, we have taken our topic, broken down the subtopics, defined a bunch of editor personas, defined our expert personas, given the editor's ability to ask questions, given the expert's ability to answer questions using web search. That's really it. So we set all that up. Now what's pretty cool is we have defined a few functions to enable all this. Now what we can do is we can set up that interview graph. You can see the code here and boom, here we go. Actually, this is pretty nice. We can actually render the graph visually, so we can look at it. So it's started the graph, we'll ask a question, we'll answer it, we'll route back to ask another question. Unless we had a stopping condition or remember, in the editor prompt, there was basically this specification, say thank you for your help if you're done, if you're satisfied. That's what we basically do in this answer question route messages thing. You can see right here. If the question ends with, thank you so much for your help, you end, or if basically the max number of turns you define is five, so you go five turns or the editor is satisfied and you're done. So that's really it. And we can go ahead and actually go test that. So let's just try to run that. So this will take a little bit to run. But you can see this is just the discussion basically following and we're printing it out at every stage. So you can see it had the entry point here. I mentioned, so the entry point is, so you said you were writing an article on our main topic, which was long context, long context model impact on RAG. And you can see this actually just followed the discussion, which is pretty cool. Yes, that's correct. I'm interested in understanding what's going on with that topic. So this is what the expert says. These $1 million tokens have a major impact. Thank you for such insight. So actions is kind of neat. Usually I grab to mediate a discussion between editors that we've created and experts on this topic. And so you bounce that back and forth, and now this is capturing the spinal graph state. So you can see we've just meted in this graph. It's running not too long, maybe like a minute or so. Now what we're going to do here is we're just going to go ahead and store that in a retriever. And there we go. So we can actually just run retrieve invoke, give it a question, and we can retrieve information from this conversation we've had. So let's just pull back and think about what we've done here. So I think it's pretty cool, actually. I get it. It's a lot of code. So you have to look at the notebook to really grok this. But at least I want to walk through the high level ideas. Again, we've taken a topic, we've broken it up as some topics, we've used those to build a number of editors. We've also defined an expert. We've let the editors and expert have a basically a discussion mediated by web search to build like an understanding of this question. And we put that in a vector store. That's all we've done. But it's pretty cool because we use Landgraf to mediate this interaction between simulated editors and simulated experts, which are effectively just a search tool. That's nice. And we've done that. And we've built out from that a vector store that we can read things from accordingly. So that's really it. Now to some of the kind of a little bit more straightforward pieces here, because I know that's kind of a mouthful to digest. We're going to now take our input topic again. Remember this example, topics are input. We're going to build like an outline of our blog, which we talked about kind of up here at the top. Now we're entering the top branch here. We're generating a high level outline of our topic. So we should have that done. So this is pretty cool. We have a high level outlier in now. It walks through like an introduction, rag frame, and so forth. And now, following the flow we outlined, what we're going to do is refine that outline a little bit based on the discussion we've had. And so this is going to run. You can see we set up a prompts here, your Wikipedia writing, gathered information from experts and search engines. Now you're refining your outline and that's really it. So this is like kind of a refinement stage. Here's the old outline. You're refining it based on discussion with the expert. And so, okay, cool. There we have it. So this is like a slightly refined outline. Again, initially the outline did not use our conversation at all. This is just the LLN reasoning about our topic, defining a sane outline. Now we refined it a little bit based upon the discussion that we had. So that's pretty neat. And now what we're going to do is actually build a more refined outline. Or actually, what we're going to do is flesh out each section in our outline in detail. So if you go back to the top here, what's happening is we've had our discussion. We've had our discussion. we've had our discussion, we've also saved it in the vector store, we've built our outline, we've refined our outline, and now where we are is we're in section writer stage. So the section writer is basically going through that outline and actually writing the sections accordingly, referencing our vector stores storage contains the expert editor discussion. So that's really what's happening now. We're fletching out the outline in detail based upon the discussion between the experts. Here we go. This is pretty cool. So you can see it's streaming. Pretty nice. It's actually writing it out for us right now. Look at that. Not bad. writing it out for us right now. Look at that. Not bad. Yeah, it's kind of fun to see that. And see, it's still going. Cool. It's writing out the references for us. So there we go. We're actually getting a little close here. Now what I'm actually going to do is I'm just going to capture all these in one final graph. So here we go. Cool. Look at this. So this basically is our whole workflow. This is just wrapping what we go. Cool. Look at this. So this basically is our whole workflow. This is just wrapping what we've already done, so we can run it all end to end, which is pretty cool. And what we're going to call now is let's go ahead and try to run all of that. Boom. So this is going to maybe take a little bit. And we're going to ask a new topic now. We're going to say, topic, Grok and video at LamaCVP in the future of inference. So that's pretty cool. We're starting from scratch. New topic. You can see we kicked off that conversation. In fact, let me pull up next to this while this is running our diagram so we can kind of like follow along. So here's our diet room right here. Nice. And we can see that where we are currently in the flow is that we're in the wiki editor persona, an extra persona discussion about inference and we can see that we kind of started it with the overall topic, Grok and Video Alarmist EP and the future of inference and then we've initiated the research. be in the future of inference. And then we've initiated the research. So there is our overall topic. We've defined an outline. And so right now, it's probably in the discussion stage, effectively here, which is the most time-consuming stage of the overall flow. We actually can go over Linesmith, and we can see what's actually running at any point in time. So I just open up Langsmith. This is pretty neat. So I can look at all these pending things in Langsmith, tell me what's actually running right now. So okay, this is kind of nice. We can see kind of like where we are in the flow. Here's a various subject matter experts, for example. Here was a kind of our experts, for example. Here was our introductory question. This is actually showing the discussion that's ongoing between our subject matter experts and our, this is pretty cool, our various editors and the experts. So this is actually what's happening under the hood as this is running right now. We can investigate some of the other ongoing things. I see, so this is what's happening right now. We can investigate some of the other kind of ongoing things. Right, I see. So this is what's happening right now. Basically, a number of these discussions have been kicked off between different subject matter experts and different editors. So this is pretty neat. Let's actually look at this in detail because this is kind of fun. Let's actually look at this in detail, because this is kind of fun. So here's the subject matter expert. So you said you were writing an article on Grok, NVIDIA, Lambda, CQP, and the future of Al-Anf, right? So this is our editor. Yes, that's correct. I see you're interested in machine learning and tools optimization. You're finding with more information about the specific contributions of Grok and NVIDIA and the sale of machine learning. So look at all these references that pulls up. This is pretty good. So it gives a nice overview here. Grok, a company focusing on creating ships for running AM models efficiently, developed the LPU, which is accurate. So this seems to be sane. As of right now, this seems correct. They've demonstrated superior performance in LM inference compared to GPUs. So this all looks pretty sane to me. NVIDIA, obviously, yeah, it talks about NVIDIA and NVIDIA's role in inference. Provides lots of citations. It's pretty nice. So then this goes back to our editor, asks a follow-up question, subject matter experts, and answers in some detail. A few more references. That's great information. And I believe that is really cool. And then we get an output. So this is the output of that discussion. Pretty cool. And we can see we kicked off a bunch of discussions along these lines. And it looks like, ah, look at this. Here's our article. It's all written. So this is kind of like some of the discussions that were logged. And look at this, it's pretty neat. Here's our article. Large-language model inference advances in future directions. Here's an intro. Ah, look at this, not bad. It's pretty detailed. Talks about graph, talks about LQUs, benchmarking, impact on AI models and videos contributions Harbor innovations frameworks on the CBP Not terrible the final set of references So, you know that took maybe five minutes it's a little bit long I understand here sitting in as the video was going but you know This actually compiled a fair amount of information, it's structured pretty nicely, gave us references. It's pretty cool. And again, we can use likes with a look under the hood and see kind of what's going on in all these discussions. Yeah, you can really audit like we did previously the discussions between the editor and the expert. And again, I maybe just want to zoom back up and show you the overall flow. What we did here is we showed how to use a few different ideas, retrieval, personas, to basically perform wiki writing from a given topic. And we took the topic, we built a general outline in parallel with that, we took the topic, we built some personas, we built experts, we had them discuss, we then captured the discussion, we use that to refine our outline, we basically capture that in a vector store, we use that to write the individual sections of our outline. We then produce a final wiki, format it correctly, and boom, there we go. On a topic of interest to us from scratch, took about five minutes on Grok and Vidya LNZP and the Future of Inference. So anyway, I just said there's a lot of code. You're not going to be able to grow all the code just by listening to me. You should absolutely look on our notebook because it's pretty cool. And definitely recommend playing with this. I think ability to use these ideas, different personas and assistance, whether it be for code or heat generation here are really powerful. And these things will probably all be highly refined and use very frequently in our workflows and plenty of months and years. So thanks a lot.

